Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@inproceedings{Mori2004,
author = {Mori, G and Ren, X and Efros, A.-A. and Malik, J},
booktitle = {cvpr},
title = {{Recovering human body configurations: combining segmentation and recognition}},
year = {2004}
}
@inproceedings{EmentFarabet2013,
abstract = {Scene labeling consists of labeling each pixel in an image with the category of the object it belongs to. We propose a method that uses a multiscale convolutional network trained from raw pixels to extract dense feature vectors that encode regions of multiple sizes centered on each pixel. The method alleviates the need for engineered features, and produces a powerful representation that captures texture, shape, and contextual information. We report results using multiple postprocessing methods to produce the final labeling. Among those, we propose a technique to automatically retrieve, from a pool of segmentation components, an optimal set of components that best explain the scene; these components are arbitrary, for example, they can be taken from a segmentation tree or from any family of oversegmentations. The system yields record accuracies on the SIFT Flow dataset (33 classes) and the Barcelona dataset (170 classes) and near-record accuracy on Stanford background dataset (eight classes), while being an order of magnitude faster than competing approaches, producing a 320{\&}{\#}x00D7;240 image labeling in less than a second, including feature extraction.},
author = {{Ement Farabet}, C{\'{i}} and Couprie, Camille and Najman, Laurent and Lecun, Yann and Farabet, Clement and Couprie, Camille and Najman, Laurent and Lecun, Yann},
booktitle = {tpami},
doi = {10.1109/TPAMI.2012.231},
file = {:home/hale/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ement Farabet et al. - Unknown - Learning Hierarchical Features for Scene Labeling.pdf:pdf},
isbn = {0162-8828 VO - 35},
issn = {01628828},
keywords = {(),2D,Convolutional networks,deep learning,image classification,image segmentation,scene labelling,scene parsing},
mendeley-tags = {2D,deep learning,scene labelling},
number = {8},
pages = {1915--1929},
pmid = {23787344},
title = {{Learning hierarchical features for scene labeling}},
url = {http://yann.lecun.com/exdb/publis/pdf/farabet-pami-13.pdf},
volume = {35},
year = {2013}
}
@inproceedings{ma,
author = {Ma, Chao and Huang, Jia-Bin and Yang, Xiaokang and Yang, Ming-Hsuan},
booktitle = {iccv},
title = {{Hierarchical Convolutional Features for Visual Tracking}},
year = {2015}
}
@article{ssim,
author = {{Zhou Wang} and Bovik, A C and Sheikh, H R and Simoncelli, E P},
doi = {10.1109/TIP.2003.819861},
journal = {tip},
keywords = {Algorithms,Automated,Computer-Assisted,Data Interpretation,Data mining,Degradation,Humans,Hypermedia,Image Enhancement,Image Interpretation,Image quality,Indexes,Information Storage and Retrieval,JPEG,JPEG2000,Layout,Models,Pattern Recognition,Quality Control,Quality assessment,Reproducibility of Results,Sensitivity and Specificity,Signal Processing,Statistical,Subtraction Technique,Transform coding,Visual perception,Visual system,data compression,distorted image,error sensitivity,error visibility,human visual perception,human visual system,image coding,image compression,image database,perceptual image quality assessment,reference image,structural information,structural similarity index,visual perception},
month = {apr},
number = {4},
pages = {600--612},
title = {{Image quality assessment: from error visibility to structural similarity}},
volume = {13},
year = {2004}
}
@inproceedings{Alexa2000,
address = {New York, NY, USA},
author = {Alexa, Marc and Cohen-Or, Daniel and Levin, David},
booktitle = {Proceedings of the 27th Annual Conference on Computer Graphics and Interactive Techniques},
doi = {10.1145/344779.344859},
isbn = {1-58113-208-5},
keywords = {compatible triangulation,shape blending,vertex path problem},
pages = {157--164},
publisher = {ACM Press/Addison-Wesley Publishing Co.},
series = {SIGGRAPH '00},
title = {{As-rigid-as-possible Shape Interpolation}},
url = {http://dx.doi.org/10.1145/344779.344859},
year = {2000}
}
@inproceedings{Lucas1981,
address = {San Francisco, CA, USA},
author = {Lucas, Bruce D and Kanade, Takeo},
booktitle = {Proceedings of the 7th International Joint Conference on Artificial Intelligence - Volume 2},
keywords = {optical flow,pioneer},
mendeley-tags = {optical flow,pioneer},
pages = {674--679},
publisher = {Morgan Kaufmann Publishers Inc.},
series = {IJCAI'81},
title = {{An Iterative Image Registration Technique with an Application to Stereo Vision}},
url = {http://dl.acm.org/citation.cfm?id=1623264.1623280},
year = {1981}
}
@inproceedings{Zhu2018,
author = {Zhu, Xinge and Yin, Zhichao and Shi, Jianping and Li, Hongsheng and Lin, Dahua},
booktitle = {Proceedings of the International Conference on 3D Vision (3DV)},
keywords = {cross-view,generative},
mendeley-tags = {cross-view,generative},
organization = {IEEE},
pages = {454--463},
title = {{Generative Adversarial Frontal View to Bird View Synthesis}},
year = {2018}
}
@incollection{Chen2018,
author = {Chen, Liang-Chieh and Zhu, Yukun and Papandreou, George and Schroff, Florian and Adam, Hartwig},
booktitle = {eccv},
doi = {10.1007/978-3-030-01234-2_49},
pages = {833--851},
publisher = {Springer International Publishing},
title = {{Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation}},
url = {https://doi.org/10.1007/978-3-030-01234-2{\_}49},
year = {2018}
}
@inproceedings{Taylor2007,
author = {Taylor, G R and Chosak, A J and Brewer, P C},
booktitle = {cvpr},
keywords = {simulator},
mendeley-tags = {simulator},
pages = {1--8},
title = {{OVVV: Using Virtual Worlds to Design and Evaluate Surveillance Systems}},
year = {2007}
}
@inproceedings{Sykora2009,
abstract = {We present a new approach to deformable image registration suit- able for articulated images such as hand-drawn cartoon characters and human postures. For such type of data state-of-the-art tech- niques typically yield undesirable results. We propose a novel ge- ometrically motivated iterative scheme where point movements are decoupled from shape consistency. By combining locally optimal block matching with as-rigid-as-possible shape regularization, our algorithm allows us to register images undergoing large free-form deformations and appearance variations. We demonstrate its prac- tical usability in various challenging tasks performed in the cartoon animation production pipeline including unsupervised inbetween- ing, example-based shape deformation, auto-painting, editing, and motion retargeting.},
author = {S{\'{y}}kora, Daniel and Dingliana, John and Collins, Steven},
booktitle = {Proceedings of the 7th International Symposium on Non-Photorealistic Animation and Rendering - NPAR '09},
doi = {10.1145/1572614.1572619},
file = {:home/hale/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/S{\'{y}}kora, Dingliana, Collins - 2009 - As-rigid-as-possible image registration for hand-drawn cartoon animations.pdf:pdf},
isbn = {9781605586045},
issn = {1605586048},
keywords = {ARAP,deformation,graphics,non-dl},
mendeley-tags = {ARAP,deformation,graphics,non-dl},
pages = {25},
title = {{As-rigid-as-possible image registration for hand-drawn cartoon animations}},
url = {http://dcgi.felk.cvut.cz/home/sykorad/Sykora09-NPAR.pdf http://portal.acm.org/citation.cfm?doid=1572614.1572619},
year = {2009}
}
@inproceedings{hui2018,
abstract = {FlowNet2 [14], the state-of-the-art convolutional neural network (CNN) for optical flow estimation, requires over 160M parameters to achieve accurate flow estimation. In this paper we present an alternative network that attains performance on par with FlowNet2 on the challenging Sin-tel final pass and KITTI benchmarks, while being 30 times smaller in the model size and 1.36 times faster in the running speed. This is made possible by drilling down to architectural details that might have been missed in the current frameworks: (1) We present a more effective flow inference approach at each pyramid level through a lightweight cascaded network. It not only improves flow estimation accuracy through early correction, but also permits seamless incorporation of descriptor matching in our network. (2) We present a novel flow regularization layer to ameliorate the issue of outliers and vague flow boundaries by using a feature-driven local convolution. (3) Our network owns an effective structure for pyramidal feature extraction and embraces feature warping rather than image warping as practiced in FlowNet2. Our code and trained models are available at github.com/twhui/LiteFlowNet.},
author = {Hui, Tak-Wai and Tang, Xiaoou and Loy, Chen Change},
booktitle = {cvpr},
file = {:home/hale/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hui, Tang, Loy - Unknown - LiteFlowNet A Lightweight Convolutional Neural Network for Optical Flow Estimation.pdf:pdf},
keywords = {deep learning,optical flow},
mendeley-tags = {deep learning,optical flow},
title = {{LiteFlowNet: A Lightweight Convolutional Neural Network for Optical Flow Estimation}},
url = {http://openaccess.thecvf.com/content{\_}cvpr{\_}2018/papers/Hui{\_}LiteFlowNet{\_}A{\_}Lightweight{\_}CVPR{\_}2018{\_}paper.pdf http://mmlab.ie.cuhk.edu.hk/projects/LiteFlowNet/},
year = {2018}
}
@article{geusebroek2005,
abstract = {We present the ALOI collection of 1,000 objects recorded under various imaging circumstances. In order to capture the sensory variation in object recordings, we systematically varied viewing angle, illumination angle, and illumination color for each object, and additionally captured wide-baseline stereo images. We recorded over a hundred images of each object, yielding a total of 110,250 images for the collection. These images are made publicly available for scientific research purposes.},
author = {Geusebroek, Jan-Mark and Burghouts, Gertjan J and Smeulders, Arnold W.M.},
doi = {10.1023/B:VISI.0000042993.50813.60},
issn = {1573-1405},
journal = {ijcv},
month = {jan},
number = {1},
pages = {103--112},
title = {{The Amsterdam Library of Object Images}},
url = {https://doi.org/10.1023/B:VISI.0000042993.50813.60},
volume = {61},
year = {2005}
}
@inproceedings{Song2015,
author = {Song, Shuran and Lichtenberg, Samuel P and Xiao, Jianxiong},
booktitle = {cvpr},
file = {:home/hale/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Song, Lichtenberg, Xiao - 2015 - Sun RGB-D A RGB-D scene understanding benchmark suite.pdf:pdf},
keywords = {dataset,indoor},
mendeley-tags = {dataset,indoor},
title = {{Sun RGB-D: A RGB-D scene understanding benchmark suite}},
year = {2015}
}
@inproceedings{camvid2008,
author = {Brostow, Gabriel J and Shotton, Jamie and Fauqueur, Julien and Cipolla, Roberto},
booktitle = {eccv},
pages = {44--57},
title = {{Segmentation and Recognition Using Structure from Motion Point Clouds}},
year = {2008}
}
@inproceedings{Liu2019SelFlow,
author = {Liu, Pengpeng and Lyu, Michael R and King, Irwin and Xu, Jia},
booktitle = {cvpr},
title = {{SelFlow: Self-Supervised Learning of Optical Flow}},
year = {2019}
}
@article{saw2017,
author = {Kovacs, Balazs and Bell, Sean and Snavely, Noah and Bala, Kavita},
journal = {cvpr},
keywords = {IID,dataset,real},
mendeley-tags = {IID,dataset,real},
title = {{Shading Annotations in the Wild}},
year = {2017}
}
@inproceedings{Yu2016,
abstract = {State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classification. However, dense prediction and image classification are structurally different. In this work, we develop a new convolutional network module that is specifically designed for dense prediction. The presented module uses dilated convolutions to systematically aggregate multi-scale contextual information without losing resolution. The architecture is based on the fact that dilated convolutions support exponential expansion of the receptive field without loss of resolution or coverage. We show that the presented context module increases the accuracy of state-of-the-art semantic segmentation systems. In addition, we examine the adaptation of image classification networks to dense prediction and show that simplifying the adapted network can increase accuracy.},
archivePrefix = {arXiv},
arxivId = {1511.07122},
author = {Yu, Fisher and Koltun, Vladlen},
booktitle = {International Conference on Learning Representations (ICLR)},
doi = {10.16373/j.cnki.ahr.150049},
eprint = {1511.07122},
isbn = {0894-0282},
issn = {00237205},
pmid = {22352717},
title = {{Multi-Scale Context Aggregation by Dilated Convolutions}},
url = {https://arxiv.org/pdf/1511.07122.pdf http://arxiv.org/abs/1511.07122},
year = {2016}
}
@inproceedings{fgfa_iccv17,
abstract = {Extending state-of-the-art object detectors from image to video is challenging. The accuracy of detection suffers from degenerated object appearances in videos, e.g., motion blur, video defocus, rare poses, etc. Existing work attempts to exploit temporal information on box level, but such methods are not trained end-to-end. We present flow-guided feature aggregation, an accurate and end-to-end learning framework for video object detection. It leverages temporal coherence on feature level instead. It improves the per-frame features by aggregation of nearby features along the motion paths, and thus improves the video recognition accuracy. Our method significantly improves upon strong single-frame baselines in ImageNet VID, especially for more challenging fast moving objects. Our framework is principled, and on par with the best engineered systems winning the ImageNet VID challenges 2016, without additional bells-and-whistles. The code would be released.},
archivePrefix = {arXiv},
arxivId = {1703.10025},
author = {Zhu, Xizhou and Wang, Yujie and Dai, Jifeng and Yuan, Lu and Wei, Yichen},
booktitle = {iccv},
eprint = {1703.10025},
file = {:home/hale/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhu et al. - Unknown - Flow-Guided Feature Aggregation for Video Object Detection.pdf:pdf},
title = {{Flow-Guided Feature Aggregation for Video Object Detection}},
url = {http://arxiv.org/abs/1703.10025 https://arxiv.org/pdf/1703.10025.pdf},
year = {2017}
}
@article{Comaniciu2012,
author = {Comaniciu, D and Meer, P},
journal = {tpami},
month = {may},
number = {5},
pages = {603--619},
title = {{Mean Shift: A Robust Approach Toward Feature Space Analysis}},
volume = {24},
year = {2002}
}
@inproceedings{Zou2018,
author = {Zou, Yuliang and Luo, Zelun and Huang, Jia-Bin},
booktitle = {eccv},
title = {{DF-Net: Unsupervised Joint Learning of Depth and Flow using Cross-Task Consistency}},
year = {2018}
}
@inproceedings{mscoco,
abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
address = {Cham},
author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Dollar, Piotr and Zitnick, C Lawrence},
booktitle = {eccv},
editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
isbn = {978-3-319-10602-1},
keywords = {dataset,real},
mendeley-tags = {dataset,real},
pages = {740--755},
publisher = {Springer International Publishing},
title = {{Microsoft COCO: Common Objects in Context}},
year = {2014}
}
@inproceedings{Maturana2015,
abstract = {Robust object recognition is a crucial skill for robots operating autonomously in real world environments. Range sensors such as LiDAR and RGBD cameras are in- creasingly found in modern robotic systems, providing a rich source of 3D information that can aid in this task. However, many current systems do not fully utilize this information and have trouble efficiently dealing with large amounts of point cloud data. In this paper, we propose VoxNet, an architecture to tackle this problem by integrating a volumetric Occupancy Grid representation with a supervised 3D Convolutional Neural Network (3D CNN). We evaluate our approach on publicly available benchmarks using LiDAR, RGBD, and CAD data. VoxNet achieves accuracy beyond the state of the art while labeling hundreds of instances per second.},
author = {Maturana, Daniel and Scherer, Sebastian},
booktitle = {Iros},
doi = {10.1109/IROS.2015.7353481},
isbn = {9781479999934},
issn = {21530866},
keywords = {3d,3dcnn,point cloud,theano},
mendeley-tags = {3d,3dcnn,point cloud,theano},
pages = {922--928},
title = {{VoxNet: A 3D Convolutional Neural Network for Real-Time Object Recognition}},
year = {2015}
}
@inproceedings{Girshick2015,
abstract = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.},
archivePrefix = {arXiv},
arxivId = {1504.08083},
author = {Girshick, Ross},
booktitle = {iccv},
doi = {10.1109/ICCV.2015.169},
eprint = {1504.08083},
file = {:home/hale/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ros Girshick - 2015 - Fast R-CNN.pdf:pdf},
isbn = {978-1-4673-8391-2},
issn = {978-1-4673-8391-2},
keywords = {CNN,RCNN,caffe,deep learning,region proposal,region-base},
mendeley-tags = {CNN,RCNN,caffe,deep learning,region proposal,region-base},
pages = {1440--1448},
title = {{Fast R-CNN}},
url = {http://arxiv.org/abs/1504.08083},
year = {2015}
}
@article{Xu2012,
author = {Xu, L and Jia, J. and Matsushita, Y},
journal = {tpami},
number = {9},
pages = {1744--1757},
title = {{Motion Detail Preserving Optical Flow Estimation}},
volume = {34},
year = {2012}
}
@misc{Socher2012,
abstract = {Recent advances in 3D sensing technologies make it possible to easily record color and depth images which together can improve object recognition. Most current methods rely on very well-designed features for this new 3D modality. We in- troduce a model based on a combination of convolutional and recursive neural networks (CNN and RNN) for learning features and classifying RGB-D images. The CNN layer learns low-level translationally invariant features which are then given as inputs to multiple, fixed-tree RNNs in order to compose higher order fea- tures. RNNs can be seen as combining convolution and pooling into one efficient, hierarchical operation. Our main result is that even RNNs with random weights compose powerful features. Our model obtains state of the art performance on a standard RGB-D object dataset while being more accurate and faster during train- ing and testing than comparable architectures such as two-layer CNNs.},
author = {Socher, Richard and Huval, Brody and Bhat, Bharath and Manning, Christopher D and Ng, Andrew Y},
booktitle = {nips},
file = {:home/hale/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Socher et al. - Unknown - Convolutional-Recursive Deep Learning for 3D Object Classification.pdf:pdf},
keywords = {3D,3D object,3d,Deep learning,classification,cnn,deep learning,object detection,rnn},
mendeley-tags = {3D,3D object,3d,Deep learning,classification,cnn,deep learning,object detection,rnn},
number = {i},
pages = {665--673},
title = {{Convolutional-Recursive Deep Learning for 3D Object Classification}},
url = {http://nlp.stanford.edu/pubs/SocherHuvalBhatManningNg{\_}NIPS2012.pdf},
urldate = {2016-04-26},
year = {2012}
}
@inproceedings{Mueller2016,
abstract = {In this paper, we propose a new aerial video dataset and benchmark for low altitude UAV target tracking, as well as, a photo-realistic UAV simulator that can be coupled with tracking methods. Our benchmark provides the first evaluation of many state-of-the-art and popular trackers on 123 new and fully annotated HD video sequences captured from a low-altitude aerial perspective. Among the compared trackers, we determine which ones are the most suitable for UAV tracking both in terms of tracking accuracy and run-time. The simulator can be used to evaluate tracking algorithms in real-time scenarios before they are deployed on a UAV ``in the field'', as well as, generate synthetic but photo-realistic tracking datasets with automatic ground truth annotations to easily extend existing real-world datasets. Both the benchmark and simulator are made publicly available to the vision community on our websiteto further research in the area of object tracking from UAVs. (https://ivul.kaust.edu.sa/Pages/pub-benchmark-simulator-uav.aspx.).},
address = {Cham},
author = {Mueller, Matthias and Smith, Neil and Ghanem, Bernard},
booktitle = {eccv},
editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
isbn = {978-3-319-46448-0},
keywords = {simulator},
mendeley-tags = {simulator},
pages = {445--461},
publisher = {Springer International Publishing},
title = {{A Benchmark and Simulator for UAV Tracking}},
year = {2016}
}
@techreport{Dorta,
abstract = {(a) Input image (previously unseen) (b) User requested edit: "big nose" (c) User requested edit: "narrowed eyes" Figure 1: An illustrative example of semantic image editing at high resolution (3456Ã—5184). The user only requests a change in a semantic binary attribute and the input image (a) is automatically transformed into, e.g., an image with a "big nose" (b) or "narrowed eyes" (c). The identity and high resolution detail of the original input is preserved. Image courtesy of flickr user Kaue Lima. Abstract Deep neural networks have recently been used to edit images with great success. However, they are often limited by only being able to work at a restricted range of resolutions. They are also so flexible that semantic face edits can often result in an unwanted loss of identity. This work proposes a model that learns how to perform semantic image edits through the application of smooth warp fields. This warp field can be efficiently predicted at a reasonably low resolution and then resampled and applied at arbitrary resolutions. Previous approaches that attempted to use warping for semantic edits required paired data, that is example images of the same object with different semantic characteristics. In contrast, we employ recent advances in Generative Adversarial Networks that allow our model to be effectively trained with unpaired data. We demonstrate the efficacy of our method for editing face images at very high resolutions (4k images) with an efficient single forward pass of a deep network at a lower resolution. We illustrate how the extent of our edits can be trivially reduced or exaggerated by scaling the predicted warp field, and we also show that our edits are substantially better at maintaining the subject's identity.},
archivePrefix = {arXiv},
arxivId = {1811.12784v1},
author = {Dorta, Garoe and Vicente, Sara and Campbell, Neill D F and Simpson, Ivor},
eprint = {1811.12784v1},
keywords = {face,gan,semantic,warp},
mendeley-tags = {face,gan,semantic,warp},
title = {{The GAN that Warped: Semantic Attribute Editing with Unpaired Data}},
url = {https://arxiv.org/pdf/1811.12784.pdf}
}
@article{Godard2019,
author = {Godard, Cl{\'{e}}ment and {Mac Aodha}, Oisin and Firman, Michael and Brostow, Gabriel J},
journal = {iccv},
month = {oct},
title = {{Digging into Self-Supervised Monocular Depth Prediction}},
url = {https://arxiv.org/pdf/1806.01260.pdf},
year = {2019}
}
@inproceedings{Park2017,
abstract = {We present a transformation-grounded image generation network for novel 3D view synthesis from a single image. Our approach first explicitly infers the parts of the geometry visible both in the input and novel views and then casts the remaining synthesis problem as image completion. Specifically, we both predict a flow to move the pixels from the input to the novel view along with a novel visibility map that helps deal with occulsion/disocculsion. Next, conditioned on those intermediate results, we hallucinate (infer) parts of the object invisible in the input image. In addition to the new network structure, training with a combination of adversarial and perceptual loss results in a reduction in common artifacts of novel view synthesis such as distortions and holes, while successfully generating high frequency details and preserving visual aspects of the input image. We evaluate our approach on a wide range of synthetic and real examples. Both qualitative and quantitative results show our method achieves significantly better results compared to existing methods.},
archivePrefix = {arXiv},
arxivId = {1703.02921},
author = {Park, Eunbyung and Yang, Jimei and Yumer, Ersin and Ceylan, Duygu and Berg, Alexander C},
booktitle = {cvpr},
doi = {10.1109/CVPR.2017.82},
eprint = {1703.02921},
file = {:home/hale/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Park et al. - 2017 - Transformation-grounded image generation network for novel 3D view synthesis.pdf:pdf},
isbn = {9781538604571},
pages = {702--711},
title = {{Transformation-grounded image generation network for novel 3D view synthesis}},
url = {http://www.cs.unc.edu/},
year = {2017}
}
@inproceedings{icp_2014,
abstract = {Geometric alignment of 3D pointclouds, obtained using a depth sensor such as a time-of-flight camera, is a challenging task with important applications in robotics and computer vision. Due to the recent advent of cheap depth sensing devices, many different 3D registration algorithms have been proposed in literature, focussing on different domains such as localization and mapping or image registration. In this survey paper, we review the state-of-the-art registration algorithms and discuss their common mathematical foundation. Starting from simple deterministic methods, such as Principal Component Analysis (PCA) and Singular Value Decomposition SVD), more recently introduced approaches such as Iterative Closest Point (ICP) and its variants, are analyzed and compared. The main contribution of this paper therefore consists of an overview of registration algorithms that are of interest in the field of computer vision and robotics, for example simultaneous Localization and Mapping.},
author = {Bellekens, Ben and Spruyt, Vincent and {Maarten Weyn}, Rafael Berkvens},
booktitle = {Fourth International Conference on Ambient Computing, Applications, Services and Technologies, Proceedings},
file = {:home/hale/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bellekens, Spruyt, Maarten Weyn - 2014 - A survey of rigid 3D pointcloud registration algorithms.pdf:pdf},
isbn = {9781612083568},
issn = {2326-9324},
keywords = {3D pointcloud,3D registration,PCL,icp,pc matching,pca,point cloud,rigid transformation,survey,survey paper,svd},
mendeley-tags = {icp,pc matching,pca,point cloud,survey,svd},
pages = {8--13},
publisher = {IARA},
title = {{A survey of rigid 3D pointcloud registration algorithms}},
url = {http://www.thinkmind.org/download.php?articleid=ambient{\_}2014{\_}1{\_}20{\_}40015 https://doc.anet.be/docman/docman.phtml?file=.irua.1ab789.4a6a3c9a.pdf},
year = {2014}
}
@inproceedings{Johnston2019,
author = {Johnston, A and Carneiro, G},
booktitle = {Proceedings of the Digital Image Computing: Techniques and Applications (DICTA)},
doi = {10.1109/DICTA47822.2019.8945841},
issn = {null},
keywords = {3D Reconstruction,3D ground truth training data,3D point cloud estimation,3D textured point cloud reconstruction,Deep Generative Modelling,Deep Learning,Depth Estimation,Self-Supervised Learning,deep generative modelling,depth sensors,estimation theory,image reconstruction,image texture,self-supervised depth estimation method,self-supervised learning,single input imaging,single view 3D point cloud reconstruction,structure from motion techniques,supervised learning,unsupervised learning,unsupervised method},
month = {dec},
pages = {1--8},
title = {{Single View 3D Point Cloud Reconstruction using Novel View Synthesis and Self-Supervised Depth Estimation}},
year = {2019}
}
@inproceedings{Lee,
author = {Lee, Kyong Joon and Zhao, Qi and Tong, Xin and Gong, Minmin and Izadi, Shahram and Lee, Sang Uk and Tan, Ping and Lin, Stephen},
booktitle = {eccv},
title = {{Estimation of intrinsic image sequences from image+depth video}},
year = {2012}
}
@inproceedings{narihia0,
author = {Narihira, T and Maire, M and Yu, S {\~{}}X.},
booktitle = {cvpr},
title = {{Learning Lightness from Human Judgement on Relative Reflectance}},
year = {2015}
}
@inproceedings{cityscapes2016,
abstract = {Semantic understanding of urban street scenes through visual perception has been widely studied due to many pos-sible practical applications. Key challenges arise from the high visual complexity of such scenes. In this paper, we present ongoing work on a new large-scale dataset for (1) assessing the performance of vision algorithms for differ-ent tasks of semantic urban scene understanding, including scene labeling, instance-level scene labeling, and object de-tection; (2) supporting research that aims to exploit large volumes of (weakly) annotated data, e.g. for training deep neural networks. We aim to provide a large and diverse set of stereo video sequences recorded in street scenes from 50 different cities, with high quality pixel-level annotations of 5000 frames in addition to a larger set of weakly annotated frames. The dataset is thus an order of magnitude larger than similar previous attempts. Several aspects are still up for discussion, and timely feedback from the community would be greatly appreciated. Details on annotated classes and examples will be avail-able at www.cityscapes-dataset.net. Moreover, we will use this website to collect remarks and suggestions.},
archivePrefix = {arXiv},
arxivId = {1604.01685},
author = {Cordts, Marius and Omran, Mohamed and Ramos, Sebastian and Scharw{\"{a}}chter, Timo and Enzweiler, Markus and Benenson, Rodrigo and Franke, Uwe and Roth, Stefan and Schiele, Bernt},
booktitle = {cvprw},
eprint = {1604.01685},
file = {:home/hale/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cordts et al. - 2016 - The Cityscapes Dataset.pdf:pdf},
keywords = {city,dataset,driving,large,urban},
mendeley-tags = {city,dataset,driving,large,urban},
title = {{The Cityscapes Dataset}},
url = {www.cityscapes-dataset.net http://www.visinf.tu-darmstadt.de/media/visinf/vi{\_}papers/2015/cordts-cvprws.pdf},
volume = {3},
year = {2016}
}
@article{Schwalbe2014,
author = {Schwalbe, Margot A B and Webb, Jacqueline F},
doi = {10.1016/j.zool.2013.09.003},
journal = {Zoology},
month = {apr},
number = {2},
pages = {112--121},
publisher = {Elsevier {\{}BV{\}}},
title = {{Sensory basis for detection of benthic prey in two Lake Malawi cichlids}},
url = {https://doi.org/10.1016/j.zool.2013.09.003},
volume = {117},
year = {2014}
}
@inproceedings{kondermann2016hci,
author = {Kondermann, Daniel and Nair, Rahul and Honauer, Katrin and Krispin, Karsten and Andrulis, Jonas and Brock, Alexander and Gussefeld, Burkhard and Rahimimoghaddam, Mohsen and Hofmann, Sabine and Brenner, Claus and Others},
booktitle = {cvprw},
keywords = {dataset,optical flow,real},
mendeley-tags = {dataset,optical flow,real},
pages = {19--28},
title = {{The HCI Benchmark Suite: Stereo and Flow Ground Truth With Uncertainties for Urban Autonomous Driving}},
year = {2016}
}
@inproceedings{Yin2019,
author = {Yin, Wei and Liu, Yifan and Shen, Chunhua and Yan, Youliang},
booktitle = {iccv},
title = {{Enforcing geometric constraints of virtual normal for depth prediction}},
year = {2019}
}
@inproceedings{Tatarchenko2016,
author = {Tatarchenko, M and Dosovitskiy, A and Brox, T},
booktitle = {eccv},
title = {{Multi-view 3D Models from Single Images with a Convolutional Network}},
year = {2016}
}
@article{Couprie2014,
abstract = {This work addresses multi-class segmentation of indoor scenes with RGB-D inputs. While this area of research has gained much attention recently, most works still rely on hand-crafted features. In contrast, we apply a multiscale convolutional network to learn features directly from the images and the depth information. Using a frame by frame labeling, we obtain nearly state-of-the-art performance on the NYU-v2 depth dataset with an accuracy of 64.5{\%}. We then show that the labeling can be further improved by exploiting the temporal consistency in the video sequence of the scene. To that goal, we present a method producing temporally consistent superpixels from a streaming video. Among the different methods producing superpixel segmentations of an image, the graph-based approach of Felzenszwalb and Huttenlocher is broadly employed. One of its interesting properties is that the regions are computed in a greedy manner in quasi-linear time by using a minimum spanning tree. In a framework exploiting minimum spanning trees all along, we propose an efficient video segmentation approach that computes temporally consistent pixels in a causal manner, filling the need for causal and real-time applications. We illustrate the labeling of indoor scenes in video sequences that could be processed in real-time using appropriate hardware such as an FPGA.},
author = {Couprie, Camille and Najman, Laurent and Najman, Laurent and Lecun, Yann},
file = {:home/hale/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Couprie et al. - 2014 - Toward Real-time Indoor Semantic Segmentation Using Depth Information.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {cnn,deep,indoor,realtime,region proposal,rgbd,segmentation,semantic segmentation},
mendeley-tags = {cnn,deep,indoor,realtime,region proposal,rgbd,segmentation,semantic segmentation},
pages = {1--48},
title = {{Toward Real-time Indoor Semantic Segmentation Using Depth Information}},
volume = {1},
year = {2014}
}
@inproceedings{gadde_2017,
author = {Gadde, Raghudeep and Jampani, Varun and Gehler, Peter V},
booktitle = {iccv},
publisher = {IEEE},
title = {{Semantic Video CNNs through Representation Warping}},
year = {2017}
}
@inproceedings{synthia2016,
author = {Ros, G and Sellart, L and Materzynska, J and Vazquez, D and Lopez, A. M. and {German Ros;} and {Laura Sellart;} and {Joanna Materzynska;} and {David Vazquez;} and {Antonio M. Lopez;}},
booktitle = {cvpr},
keywords = {city,dataset,segmentation,semantic,synthetic,urban},
mendeley-tags = {city,dataset,segmentation,semantic,synthetic,urban},
title = {{The SYNTHIA dataset: A large collection of synthetic images for semantic segmentation of urban scenes}},
url = {http://synthia-dataset.net/wp-content/uploads/2016/06/gros{\_}cvpr16-1.pdf},
year = {2016}
}
@inproceedings{Noh,
abstract = {We propose a novel semantic segmentation algorithm by learning a deconvolution network. We learn the network on top of the convolutional layers adopted from VGG 16-layer net. The deconvolution network is composed of deconvolution and unpooling layers, which identify pixel-wise class labels and predict segmentation masks. We apply the trained network to each proposal in an input image, and construct the final semantic segmentation map by combining the results from all proposals in a simple manner. The proposed algorithm mitigates the limitations of the existing methods based on fully convolutional networks by integrating deep deconvolution network and proposal-wise prediction; our segmentation method typically identifies detailed structures and handles objects in multiple scales naturally. Our network demonstrates outstanding performance in PASCAL VOC 2012 dataset, and we achieve the best accuracy (72.5{\%}) among the methods trained with no external data through ensemble with the fully convolutional network.},
archivePrefix = {arXiv},
arxivId = {1505.04366},
author = {Noh, Hyeonwoo and Hong, Seunghoon and Han, Bohyung},
booktitle = {iccv},
doi = {10.1109/ICCV.2015.178},
eprint = {1505.04366},
file = {:home/hale/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Noh, Hong, Han - Unknown - Learning Deconvolution Network for Semantic Segmentation.pdf:pdf},
isbn = {978-1-4673-8391-2},
issn = {15505499},
keywords = {cnn,deconv,deconvolution,deep,region proposal,segmentation,semantic segmentation},
mendeley-tags = {cnn,deconv,deconvolution,deep,region proposal,segmentation,semantic segmentation},
pages = {1520--1528},
title = {{Learning Deconvolution Network for Semantic Segmentation}},
url = {http://arxiv.org/abs/1505.04366},
volume = {1},
year = {2015}
}
@inproceedings{Nguyen2018,
author = {Nguyen-Phuoc, Thu and Li, Chuan and Balaban, Stephen and Yang, Yong-Liang},
booktitle = {nips},
title = {{RenderNet: A deep convolutional network for differentiable rendering from 3D shapes}},
year = {2018}
}
@inproceedings{Kang_2017_CVPR,
abstract = {Object detection in videos has drawn increasing attention recently with the introduction of the large-scale ImageNet VID dataset. Different from object detection in static images, temporal information in videos provides vital information for object detection. To fully utilize temporal information, state-of-the-art methods are therefore based on spatiotemporal tubelets, which are essentially sequences of associated bounding boxes across time. However, the existing methods have major limitations in generating tubelets in terms of quality and efficiency. Motion-based methods are able to obtain dense tubelets, but the lengths are generally only several frames, which is not optimal to incorporate long-term temporal information. Appearance-based methods, usually involving generic object tracking, could generate long tubelets, but are usually computational expensive. In this work, we propose a framework for object detection in videos, which consists of a novel tubelet proposal network to efficiently generate spatiotemporal proposals, and a Long Short-term Memory (LSTM) network that incorporates temporal information from tubelet proposals for achieving high object detection accuracy in videos. The experiments on the large-scale ImageNet VID dataset demonstrate the effectiveness of the proposed framework for object detection in videos.},
archivePrefix = {arXiv},
arxivId = {1604.04053},
author = {Kang, Kai and Li, Hongsheng and Xiao, Tong and Ouyang, Wanli and Yan, Junjie and Liu, Xihui and Wang, Xiaogang and Li, Hongsheng and Wang, Xiaogang and Xiao, Tong and Ouyang, Wanli and Yan, Junjie and Liu, Xihui and Wang, Xiaogang},
booktitle = {cvpr},
doi = {10.1109/CVPR.2016.95},
eprint = {1604.04053},
file = {:home/hale/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kang et al. - 2016 - Object Detection from Video Tubelets with Convolutional Neural Networks.pdf:pdf},
month = {jul},
pages = {727--735},
title = {{Object Detection in Videos With Tubelet Proposal Networks}},
url = {http://arxiv.org/abs/1604.04053 http://dx.doi.org/10.1109/CVPR.2016.95 http://arxiv.org/abs/1702.06355},
year = {2017}
}
@article{Visin2016,
archivePrefix = {arXiv},
arxivId = {1511.07053},
author = {Visin, Francesco and Ciccone, Marco and Romero, Adriana and Kastner, Kyle and Kyunghyun, Cho and Bengio, Yoshua and Matteucci, Matteo and Courville, Aaron and Visi, F and M.Ciccone and Romero, Adriana and Kastner, Kyle and Cho, K and Bengio, Yoshua and Matteucci, Matteo and Courville, Aaron},
doi = {10.1109/CVPRW.2016.60},
eprint = {1511.07053},
isbn = {9781509014378},
journal = {cvprw},
title = {{ReSeg : A Recurrent Neural Network-based Model for Semantic Segmentation}},
year = {2016}
}
@article{PascalVOC,
author = {Everingham, M and Van{\~{}}Gool, L and Williams, C K I and Winn, J and Zisserman, A},
journal = {ijcv},
month = {jun},
number = {2},
pages = {303--338},
title = {{The Pascal Visual Object Classes (VOC) Challenge}},
volume = {88},
year = {2010}
}
@inproceedings{Qi2016,
abstract = {3D shape models are becoming widely available and easier to capture, making available 3D information crucial for progress in object classification. Current state-of-the-art methods rely on CNNs to address this problem. Recently, we witness two types of CNNs being developed: CNNs based upon volumetric representations versus CNNs based upon multi-view representations. Empirical results from these two types of CNNs exhibit a large gap, indicating that existing volumetric CNN architectures and approaches are unable to fully exploit the power of 3D representations. In this paper, we aim to improve both volumetric CNNs and multi-view CNNs according to extensive analysis of existing approaches. To this end, we introduce two distinct network architectures of volumetric CNNs. In addition, we examine multi-view CNNs, where we introduce multi-resolution filtering in 3D. Overall, we are able to outperform current state-of-the-art methods for both volumetric CNNs and multi-view CNNs. We provide extensive experiments designed to evaluate underlying design choices, thus providing a better understanding of the space of methods available for object classification on 3D data.},
archivePrefix = {arXiv},
arxivId = {1604.03265},
author = {Qi, Charles R and Su, Hao and Niessner, Matthias and Dai, Angela and Yan, Mengyuan and Guibas, Leonidas J},
booktitle = {cvpr},
doi = {10.1109/CVPR.2016.609},
eprint = {1604.03265},
file = {:home/hale/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Qi et al. - 2016 - Volumetric and Multi-View CNNs for Object Classification on 3D Data.pdf:pdf},
keywords = {3d,3dcnn,classification,volumetric},
mendeley-tags = {3d,3dcnn,classification,volumetric},
title = {{Volumetric and Multi-View CNNs for Object Classification on 3D Data}},
url = {http://arxiv.org/abs/1604.03265},
year = {2016}
}
@article{Chen2013,
address = {New York, NY, USA},
author = {Chen, Tao and Zhu, Zhe and Shamir, Ariel and Hu, Shi-Min and Cohen-Or, Daniel},
doi = {10.1145/2508363.2508378},
issn = {0730-0301},
journal = {tog},
keywords = {3d,interactive,interactive modeling,models,nvs,photo manipulation},
mendeley-tags = {3d,interactive,models,nvs},
month = {nov},
number = {6},
publisher = {Association for Computing Machinery},
title = {{3-Sweep: Extracting Editable Objects from a Single Photo}},
url = {https://doi.org/10.1145/2508363.2508378},
volume = {32},
year = {2013}
}
@inproceedings{DAVIS_CVPR16,
author = {Perazzi, Federico and Pont-Tuset, Jordi and McWilliams, Brian and Gool, Luc Van and Gross, Markus and Sorkine-Hornung, Alexander},
booktitle = {cvpr},
title = {{A Benchmark Dataset and Evaluation Methodology for Video Object Segmentation}},
year = {2016}
}
@article{Csurka2011,
author = {Csurka, G and Perronnin, F},
journal = {ijcv},
pages = {198--212},
title = {{An Efficient Approach to Semantic Segmentation}},
year = {2011}
}
@book{Julesz1971,
author = {Julesz, B},
title = {{Foundations of Cyclopean Perception}},
year = {1971}
}
@inproceedings{Baraldi1989,
abstract = {Passive navigation of mobile robots is one of the chal-lenging goals of machine vision. This note demonstrates the use of optical flow, which encodes the visual infor-mation in a sequence of time varying images [1], for the recovery of motion and the understanding of the three dimensional structure of the viewed scene. By using a modified version of an algorithm, which has recently been proposed to compute optical flow, it is possible to obtain dense and accurate estimates of the true ID motion field. Then these estimates are used to recover the angular ve-locity of the viewed rigid objects. Finally it is shown that, when the camera translation is known, a coarse depth map of the scene can be extracted from the optical flow of real time varying images. The navigation of a robot in any environment requires the knowledge of the motion of the robot relative to the environment, the three dimensional structure of the scene and the motion parameters of the object moving in the scene. These informations can be obtained by us-ing active sensors and/or by passive vision, provided by cameras mounted on the robot. In this note it is shown how passive vision can be used to recover depth when the camera on the robot is trans-lating and angular velocity when the camera looks at rotating objects. The proposed technique first computes the optical from a sequence of time varying images by using a modification of the algorithm recently proposed [2,3]. The angular velocity can be obtained by exploit-ing mathematical properties of the 2D motion field [5]. Depth is obtained from the computed optical flow, using an equation already proposed by many authors (Horn 1987 , Tommasi personal communication).},
author = {Baraldi, P. and Micheli, E. D. and Uras, S.},
booktitle = {Procedings of the Alvey Vision Conference 1989},
doi = {10.5244/C.3.35},
keywords = {depth,joint,optical flow},
mendeley-tags = {depth,joint,optical flow},
pages = {35.1--35.4},
title = {{Motion and Depth from Optical Flow}},
url = {http://www.bmva.org/bmvc/1989/avc-89-035.html},
year = {1989}
}
@inproceedings{cd_emd,
abstract = {Generation of 3D data by deep neural networks has been attracting increasing attention in the research community. The majority of extant works resort to regular representations such as volumetric grids or collections of images; however, these representations obscure the natural invariance of 3D shapes under geometric transformations, and also suffer from a number of other issues. In this paper we address the problem of 3D reconstruction from a single image, generating a straight-forward form of output - point cloud coordinates. Along with this problem arises a unique and interesting issue, that the groundtruth shape for an input image may be ambiguous. Driven by this unorthodox output form and the inherent ambiguity in groundtruth, we design architecture, loss function and learning paradigm that are novel and effective. Our final solution is a conditional shape sampler, capable of predicting multiple plausible 3D point clouds from an input image. In experiments not only can our system outperform state-of-the-art methods on single image based 3d reconstruction benchmarks; but it also shows strong performance for 3D shape completion and promising ability in making multiple plausible predictions.},
archivePrefix = {arXiv},
arxivId = {1612.00603},
author = {Fan, Haoqiang and Su, Hao and Guibas, Leonidas},
booktitle = {cvpr},
doi = {10.1109/CVPR.2017.264},
eprint = {1612.00603},
file = {:home/hale/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fan, Su, Guibas - 2017 - A point set generation network for 3D object reconstruction from a single image.pdf:pdf},
isbn = {9781538604571},
keywords = {point cloud,reconstruction,single image},
mendeley-tags = {point cloud,reconstruction,single image},
pages = {2463--2471},
title = {{A point set generation network for 3D object reconstruction from a single image}},
volume = {2017-Janua},
year = {2017}
}
@article{Perlin1985,
address = {New York, NY, USA},
author = {Perlin, Ken},
doi = {10.1145/325334.325247},
isbn = {0897911660},
issn = {0097-8930},
journal = {tog},
keywords = {algorithm development,fire,functional composition,interactive,pixel stream editor,solid texture,space function,stochastic modelling,turbulence,waves},
month = {jul},
number = {3},
pages = {287--296},
publisher = {Association for Computing Machinery},
series = {SIGGRAPH '85},
title = {{An Image Synthesizer}},
url = {https://doi.org/10.1145/325334.325247 https://doi.org/10.1145/325165.325247},
volume = {19},
year = {1985}
}
@article{Zheng2012,
address = {New York, NY, USA},
author = {Zheng, Youyi and Chen, Xiang and Cheng, Ming-Ming and Zhou, Kun and Hu, Shi-Min and Mitra, Niloy J},
doi = {10.1145/2185520.2185595},
issn = {0730-0301},
journal = {tog},
keywords = {coupled optimization,cuboid proxies,image manipulation,interactive,non-local relations,scene understanding},
mendeley-tags = {interactive},
month = {jul},
number = {4},
publisher = {Association for Computing Machinery},
title = {{Interactive Images: Cuboid Proxies for Smart Image Manipulation}},
url = {https://doi.org/10.1145/2185520.2185595},
volume = {31},
year = {2012}
}
@inproceedings{dai16rfcn,
abstract = {We present region-based, fully convolutional networks for accurate and efficient object detection. In contrast to previous region-based detectors such as Fast/Faster R-CNN that apply a costly per-region subnetwork hundreds of times, our region-based detector is fully convolutional with almost all computation shared on the entire image. To achieve this goal, we propose position-sensitive score maps to address a dilemma between translation-invariance in image classification and translation-variance in object detection. Our method can thus naturally adopt fully convolutional image classifier backbones, such as the latest Residual Networks (ResNets), for object detection. We show competitive results on the PASCAL VOC datasets (e.g., 83.6{\%} mAP on the 2007 set) with the 101-layer ResNet. Meanwhile, our result is achieved at a test-time speed of 170ms per image, 2.5-20x faster than the Faster R-CNN counterpart. Code is made publicly available at: https://github.com/daijifeng001/r-fcn},
archivePrefix = {arXiv},
arxivId = {1605.06409},
author = {{Jifeng Dai Yi Li}, Kaiming He Jian Sun and Dai, Jifeng and Li, Yi and He, Kaiming and Sun, Jian},
booktitle = {nips},
eprint = {1605.06409},
file = {:home/hale/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dai et al. - Unknown - R-FCN Object Detection via Region-based Fully Convolutional Networks.pdf:pdf},
issn = {10495258},
keywords = {detection},
mendeley-tags = {detection},
title = {{R-FCN: Object Detection via Region-based Fully Convolutional Networks}},
url = {https://arxiv.org/pdf/1605.06409.pdf http://arxiv.org/abs/1605.06409},
year = {2016}
}
@inproceedings{kitti2012,
abstract = {Today, visual recognition systems are still rarely employed in robotics applications. Perhaps one of the main reasons for this is the lack of demanding benchmarks that mimic such scenarios. In this paper, we take advantage of our autonomous driving platform to develop novel challenging benchmarks for the tasks of stereo, optical flow, visual odometry/SLAM and 3D object detection. Our recording platform is equipped with four high resolution video cameras, a Velodyne laser scanner and a state-of-the-art localization system. Our benchmarks comprise 389 stereo and optical flow image pairs, stereo visual odometry sequences of 39.2 km length, and more than 200k 3D object annotations captured in cluttered scenarios (up to 15 cars and 30 pedestrians are visible per image). Results from state-of-the-art algorithms reveal that methods ranking high on established datasets such as Middlebury perform below average when being moved outside the laboratory to the real world. Our goal is to reduce this bias by providing challenging benchmarks with novel difficulties to the computer vision community. Our benchmarks are available online at: www.cvlibs.net/datasets/kitti.},
archivePrefix = {arXiv},
arxivId = {1612.07695},
author = {Geiger, Andreas and Lenz, Philip and Urtasun, Raquel},
booktitle = {cvpr},
doi = {10.1109/CVPR.2012.6248074},
eprint = {1612.07695},
file = {:home/hale/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Geiger, Lenz, Urtasun - 2012 - Are we ready for autonomous driving The KITTI vision benchmark suite.pdf:pdf},
isbn = {9781467312264},
issn = {10636919},
keywords = {dataset,detection,flow,odometry,real,stereo,tracking},
mendeley-tags = {dataset,detection,flow,odometry,real,stereo,tracking},
pages = {3354--3361},
title = {{Are we ready for autonomous driving? The KITTI vision benchmark suite}},
url = {http://www.cvlibs.net/publications/Geiger2012CVPR.pdf},
year = {2012}
}
@inproceedings{Ranjan2018,
abstract = {The optical flow of humans is well known to be useful for the analysis of human action. Given this, we devise an optical flow algorithm specifically for human motion and show that it is superior to generic flow methods. Designing a method by hand is impractical, so we develop a new training database of image sequences with ground truth optical flow. For this we use a 3D model of the human body and motion capture data to synthesize realistic flow fields. We then train a convolutional neural network to estimate human flow fields from pairs of images. Since many applications in human motion analysis depend on speed, and we anticipate mobile applications, we base our method on SpyNet with several modifications. We demonstrate that our trained network is more accurate than a wide range of top methods on held-out test data and that it generalizes well to real image sequences. When combined with a person detector/tracker, the approach provides a full solution to the problem of 2D human flow estimation. Both the code and the dataset are available for research.},
archivePrefix = {arXiv},
arxivId = {1806.05666},
author = {Ranjan, Anurag and Romero, Javier and Black, Michael J.},
booktitle = {bmvc},
eprint = {1806.05666},
keywords = {dataset,synthetic},
mendeley-tags = {dataset,synthetic},
title = {{Learning Human Optical Flow}},
url = {http://arxiv.org/abs/1806.05666},
year = {2018}
}
@inproceedings{Bansal2016,
abstract = {We introduce an approach that leverages surface normal predictions, along with appearance cues, to retrieve 3D models for objects depicted in 2D still images from a large CAD object library. Critical to the success of our approach is the ability to recover accurate surface normals for objects in the depicted scene. We introduce a skip-network model built on the pre-trained Oxford VGG convolutional neural network (CNN) for surface normal prediction. Our model achieves state-of-the-art accuracy on the NYUv2 RGB-D dataset for surface normal prediction, and recovers fine object detail compared to previous methods. Furthermore, we develop a two-stream network over the input image and predicted surface normals that jointly learns pose and style for CAD model retrieval. When using the predicted surface normals, our two-stream network matches prior work using surface normals computed from RGB-D images on the task of pose prediction, and achieves state of the art when using RGB-D input. Finally, our two-stream network allows us to retrieve CAD models that better match the style and pose of a depicted object compared with baseline approaches.},
archivePrefix = {arXiv},
arxivId = {1604.01347},
author = {Bansal, Aayush and Russell, Bryan and Gupta, Abhinav},
booktitle = {cvpr},
doi = {10.1109/CVPR.2016.642},
eprint = {1604.01347},
file = {:home/hale/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bansal, Russell, Gupta - 2016 - Marr Revisited 2D-3D Alignment via Surface Normal Prediction.pdf:pdf},
isbn = {978-1-4673-8851-1},
issn = {10636919},
keywords = {CNN,surface normals},
mendeley-tags = {CNN,surface normals},
pages = {5965----5974},
title = {{Marr Revisited: 2D-3D Alignment via Surface Normal Prediction}},
url = {http://www.cs.cmu.edu http://arxiv.org/abs/1604.01347},
year = {2016}
}
@inproceedings{tappen,
author = {Tappen, Marshall F. and Freeman, William T. and Adelson, Edward H.},
booktitle = {nips},
title = {{Recovering Intrinsic Images from a Single Image}},
year = {2003}
}
@inproceedings{vgg,
author = {Simonyan, Karen and Zisserman, Andrew},
booktitle = {International Conference on Learning Representations},
title = {{Very Deep Convolutional Networks for Large-Scale Image Recognition}},
year = {2015}
}
@article{Farabet2010,
abstract = {In this paper we present a scalable hardware architecture to implement large-scale convolutional neural networks and state-of-the-art multi-layered artificial vision systems. This system is fully digital and is a modular vision engine with the goal of performing real-time detection, recognition and segmentation of mega-pixel images. We present a performance comparison between a software, FPGA and ASIC implementation that shows a speed up in custom hardware implementations.},
author = {Farabet, Cl{\'{e}}ment and Martini, Berin and Akselrod, Polina and Talay, Sel{\c{c}}uk and LeCun, Yann and Culurciello, Eugenio},
doi = {10.1109/ISCAS.2010.5537908},
file = {:home/hale/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Farabet et al. - 2010 - Hardware Accelerated Convolutional Neural Networks for Synthetic Vision Systems.pdf:pdf},
isbn = {9781424453092},
issn = {10450823},
journal = {Proc. International Symposium on Circuits and Systems (ISCAS'10)},
keywords = {CNN,architecture},
mendeley-tags = {CNN,architecture},
pages = {257--260},
pmid = {18255614},
title = {{Hardware Accelerated Convolutional Neural Networks for Synthetic Vision Systems}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5537908},
year = {2010}
}
@inproceedings{Satkin2012,
author = {Satkin, Scott and Lin, Jason and Hebert, Martial},
booktitle = {bmvc},
title = {{Data-Driven Scene Understanding from 3{\{}D{\}} Models}},
year = {2012}
}
@inproceedings{Laffont1,
author = {Laffont, Pierre-Yves and Bazin, Jean-Charles},
booktitle = {iccv},
title = {{Intrinsic decomposition of image sequences from local temporal variations}},
year = {2015}
}
@article{Mayer2018,
abstract = {The finding that very large networks can be trained efficiently and reliably has led to a paradigm shift in computer vision from engineered solutions to learning formulations. As a result, the research challenge shifts from devising algorithms to creating suitable and abundant training data for supervised learning. How to efficiently create such training data? The dominant data acquisition method in visual recognition is based on web data and manual annotation. Yet, for many computer vision problems, such as stereo or optical flow estimation, this approach is not feasible because humans cannot manually enter a pixel-accurate flow field. In this paper, we promote the use of synthetically generated data for the purpose of training deep networks on such tasks. We suggest multiple ways to generate such data and evaluate the influence of dataset properties on the performance and generalization properties of the resulting networks. We also demonstrate the benefit of learning schedules that use different types of data at selected stages of the training process.},
author = {Mayer, Nikolaus and Ilg, Eddy and Fischer, Philipp and Hazirbas, Caner and Cremers, Daniel and Dosovitskiy, Alexey and Brox, Thomas},
doi = {10.1007/s11263-018-1082-6},
file = {:home/hale/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mayer et al. - 2018 - What Makes Good Synthetic Training Data for Learning Disparity and Optical Flow Estimation.pdf:pdf},
issn = {1573-1405},
journal = {ijcv},
keywords = {Data generation,Deep learning,DispNet,FlowNet,Synthetic ground truth},
month = {sep},
number = {9},
pages = {942--960},
title = {{What Makes Good Synthetic Training Data for Learning Disparity and Optical Flow Estimation?}},
url = {https://doi.org/10.1007/s11263-018-1082-6},
volume = {126},
year = {2018}
}
@book{Charisopoulos2017,
abstract = {Neural networks have traditionally relied on mostly linear models, such as the multiply-accumulate architecture of a linear percep-tron that remains the dominant paradigm of neuronal computation. How-ever, from a biological standpoint, neuron activity may as well involve inherently nonlinear and competitive operations. Mathematical morphol-ogy and minimax algebra provide the necessary background in the study of neural networks made up from these kinds of nonlinear units. This paper deals with such a model, called the morphological perceptron. We study some of its geometrical properties and introduce a training algo-rithm for binary classification. We point out the relationship between morphological classifiers and the recent field of tropical geometry, which enables us to obtain a precise bound on the number of linear regions of the maxout unit, a popular choice for deep neural networks introduced recently. Finally, we present some relevant numerical results.},
author = {Charisopoulos, Vasileios and Maragos, Petros},
doi = {10.1007/978-3-319-57240-6},
file = {:home/hale/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Charisopoulos, Maragos - 2017 - Mathematical Morphology and Its Applications to Signal and Image Processing.pdf:pdf},
isbn = {978-3-319-57239-0},
keywords = {math{\_}morph},
mendeley-tags = {math{\_}morph},
title = {{Mathematical Morphology and Its Applications to Signal and Image Processing}},
url = {https://link.springer.com/content/pdf/10.1007{\%}2F978-3-319-57240-6{\_}1.pdf http://link.springer.com/10.1007/978-3-319-57240-6},
volume = {10225},
year = {2017}
}
@inproceedings{Gehler,
author = {Gehler, P V and Rother, C and Kiefel, M and Zhang, L and Sch{\"{o}}lkopf, B},
booktitle = {nips},
title = {{Recovering intrinsic images with a global sparsity prior on reflectance}},
year = {2011}
}
@article{trimbot,
abstract = {Robots are increasingly present in modern industry and also in everyday life. Their applications range from health-related situations, for assistance to elderly people or in surgical operations, to automatic and driver-less vehicles (on wheels or flying) or for driving assistance. Recently, an interest towards robotics applied in agriculture and gardening has arisen, with applications to automatic seeding and cropping or to plant disease control, etc. Autonomous lawn mowers are succesful market applications of gardening robotics. In this paper, we present a novel robot that is developed within the TrimBot2020 project, funded by the EU H2020 program. The project aims at prototyping the first outdoor robot for automatic bush trimming and rose pruning.},
author = {Strisciuglio, Nicola and Tylecek, Radim and Petkov, Nicolai and Biber, Peter and Hemming, Jochen and {Van Henten}, Eldert and Sattler, Torsten and Pollefeys, Marc and Gevers, Theo and Brox, Thomas and Fisher, Robert B},
file = {:home/hale/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Strisciuglio et al. - Unknown - TrimBot2020 an outdoor robot for automatic gardening.pdf:pdf},
keywords = {trimbot},
mendeley-tags = {trimbot},
title = {{TrimBot2020: an outdoor robot for automatic gardening}},
url = {https://arxiv.org/pdf/1804.01792.pdf}
}
@article{Aslani2013,
author = {Aslani, S and Mahdavi-Nasab, H},
journal = {International Journal of Electrical, Computer, Energetic, Electronic and Communication Engineering},
pages = {1252--1256},
title = {{Optical Flow Based Moving Object Detection and Tracking for Traffic Surveillance}},
year = {2013}
}
@inproceedings{Ilg2016,
abstract = {The FlowNet demonstrated that optical flow estimation can be cast as a learning problem. However, the state of the art with regard to the quality of the flow has still been defined by traditional methods. Particularly on small displacements and real-world data, FlowNet cannot compete with variational methods. In this paper, we advance the concept of end-to-end learning of optical flow and make it work really well. The large improvements in quality and speed are caused by three major contributions: first, we focus on the training data and show that the schedule of presenting data during training is very important. Second, we develop a stacked architecture that includes warping of the second image with intermediate optical flow. Third, we elaborate on small displacements by introducing a sub-network specializing on small motions. FlowNet 2.0 is only marginally slower than the original FlowNet but decreases the estimation error by more than 50{\%}. It performs on par with state-of-the-art methods, while running at interactive frame rates. Moreover, we present faster variants that allow optical flow computation at up to 140fps with accuracy matching the original FlowNet.},
archivePrefix = {arXiv},
arxivId = {1612.01925},
author = {Ilg, Eddy and Mayer, Nikolaus and Saikia, Tonmoy and Keuper, Margret and Dosovitskiy, Alexey and Brox, Thomas},
booktitle = {cvpr},
eprint = {1612.01925},
file = {:home/hale/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ilg et al. - 2017 - FlowNet 2.0 Evolution of Optical Flow Estimation with Deep Networks.pdf:pdf},
title = {{FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks}},
url = {http://arxiv.org/abs/1612.01925 http://lmb.informatik.uni-freiburg.de//Publications/2017/IMKDB17},
year = {2017}
}
@article{Weber1995,
abstract = {Recent advances in computer graphics have produced images approaching the elusive goal of photorealism. Since many natural objects are so complex and detailed, they are often not rendered with convincing fidelity due to the difficulties in succinctly defining and efficiently rendering their geometry. With the increased demand of future simulation and virtual reality applications, the production of realistic natural-looking background objects will become increasingly more important. We present a model to create and render trees. Our emphasis is on the overall geometrical structure of the tree and not a strict adherence to botanical principles. Since the model must be utilized by general users, it does not require any knowledge beyond the principles of basic geometry. We also explain a method to seamlessly degrade the tree geometry at long ranges to optimize the drawing of large quantities of trees in forested areas.},
author = {Weber, Jason and Penn, Joseph},
doi = {10.1145/218380.218427},
isbn = {0897917014},
issn = {0097-8930},
journal = {tog},
pages = {119--128},
title = {{Creation and rendering of realistic trees}},
year = {1995}
}
@inproceedings{Lee2016,
abstract = {This paper presents a robust multi-class multi-object tracking (MCMOT) formulated by a Bayesian filtering framework. Multi-object tracking for unlimited object classes is conducted by combining detection responses and changing point detection (CPD) algorithm. The CPD model is used to observe abrupt or abnormal changes due to a drift and an occlusion based spatiotemporal characteristics of track states. The ensemble of convolutional neural network (CNN) based object detector and Lucas-Kanede Tracker (KLT) based motion detector is employed to compute the likelihoods of foreground regions as the detection responses of different object classes. Extensive experiments are performed using lately introduced challenging benchmark videos; ImageNet VID and MOT benchmark dataset. The comparison to state-of-the-art video tracking techniques shows very encouraging results.},
archivePrefix = {arXiv},
arxivId = {1608.08434},
author = {Lee, Byungjae and Erdenee, Enkhbayar and Jin, Songguo and Nam, Mi Young and Jung, Young Giu and Rhee, Phill Kyu},
booktitle = {eccvw},
doi = {10.1007/978-3-319-48881-3_6},
eprint = {1608.08434},
isbn = {9783319488806},
issn = {16113349},
keywords = {Changing point detection,Convolutional neural network,Entity transition,Multi-class and multi-object tracking,Object detection from video},
month = {aug},
pages = {68--83},
title = {{Multi-class multi-object tracking using changing point detection}},
url = {http://arxiv.org/abs/1608.08434},
volume = {9914 LNCS},
year = {2016}
}
@inproceedings{pascalContext,
author = {Mottaghi, Roozbeh and Chen, Xianjie and Liu, Xiaobai and Cho, Nam-Gyu and Lee, Seong-Whan and Fidler, Sanja and Urtasun, Raquel and Yuille, Alan and Urtasun, Raquel and Yuille, Alan},
booktitle = {cvpr},
doi = {10.13140/2.1.2577.6000},
file = {:home/hale/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mottaghi et al. - 2014 - The Role of Context for Object Detection and Semantic Segmentation in the Wild.pdf:pdf},
keywords = {dataset,rgb,semantic},
mendeley-tags = {dataset,rgb,semantic},
pages = {891--898},
title = {{The Role of Context for Object Detection and Semantic Segmentation in the Wild}},
year = {2014}
}
@inproceedings{Newell2016,
abstract = {This work introduces a novel convolutional network architecture for the task of human pose estimation. Features are processed across all scales and consolidated to best capture the various spatial relationships associated with the body. We show how repeated bottom-up, top-down processing used in conjunction with intermediate supervision is critical to improving the performance of the network. We refer to the architecture as a â€œstacked hourglassâ€ network based on the successive steps of pooling and upsampling that are done to produce a final set of predictions. State-of-the-art results are achieved on the FLIC and MPII benchmarks outcompeting all recent methods.},
author = {Newell, Alejandro and Yang, Kaiyu and Deng, Jia},
booktitle = {eccv},
keywords = {Human pose estimation},
title = {{Stacked hourglass networks for human pose estimation}},
year = {2016}
}
@inproceedings{scannet2017,
author = {Dai, Angela and Chang, Angel X and Savva, Manolis and Halber, Maciej and Funkhouser, Thomas and Nie{\ss}ner, Matthias},
booktitle = {cvpr},
keywords = {dataset,scanned},
mendeley-tags = {dataset,scanned},
title = {{ScanNet: Richly-annotated 3D Reconstructions of Indoor Scenes}},
year = {2017}
}
@article{Black1996CVIU,
author = {Black, Michael J. and Anandan, P},
journal = {Computer Vision and Image Understanding (CVIU)},
number = {1},
pages = {75--104},
title = {{Nonmetric calibration of wide-angle lenses and polycameras}},
volume = {63},
year = {1996}
}
@article{Liscum2014,
author = {Liscum, Emmanuel and Askinosie, Scott K and Leuchtman, Daniel L and Morrow, Johanna and Willenburg, Kyle T and Coats, Diana Roberts},
doi = {10.1105/tpc.113.119727},
journal = {The Plant Cell},
month = {jan},
number = {1},
pages = {38--55},
publisher = {American Society of Plant Biologists ({\{}ASPB{\}})},
title = {{Phototropism: Growing towards an Understanding of Plant Movement}},
url = {https://doi.org/10.1105/tpc.113.119727},
volume = {26},
year = {2014}
}
@article{Zhao,
author = {Zhao, Q and Tan, P and Dai, Q and Shen, L and Wu, E and Lin, S},
journal = {tpami},
pages = {1437--1444},
title = {{A closed-form solution to retinex with nonlocal texture constraints}},
year = {2012}
}
@inproceedings{Jiang,
author = {Jiang, X and Schofield, A {\~{}}J. and Wyatt, J {\~{}}L.},
booktitle = {eccv},
title = {{Correlation-Based intrinsic image extraction from a single image}},
year = {2010}
}
@inproceedings{Liu2015,
abstract = {Nowadays, detecting objects in 3D scenes like point clouds has become an emerging challenge with various applications. However, it retains as an open problem due to the deficiency of labeling 3D training data. To deploy an accurate detection algorithm typically resorts to investigating both RGB and depth modalities, which have distinct statistics while correlated with each other. Previous research mainly focus on detecting objects using only one modality, which ignores exploiting the cross-modality cues. In this work, we propose a cross-modality deep learning framework based on deep Boltzmann Machines for 3D Scenes object detection. In particular, we demonstrate that by learning cross-modality feature from RGBD data, it is possible to capture their joint information to reinforce detector trainings in individual modalities. In particular, we slide a 3D detection window in the 3D point cloud to match the exemplar shape, which the lack of training data in 3D domain is conquered via (1) We collect 3D CAD models and 2D positive samples from Internet. (2) adopt pretrained R-CNNs [2] to extract raw feature from both RGB and Depth domains. Experiments on RMRC dataset demonstrate that the bimodal based deep feature learning framework helps 3D scene object detection.},
author = {Liu, Wei and Ji, Rongrong and Li, Shaozi},
booktitle = {cvpr},
doi = {10.1109/CVPR.2015.7298920},
isbn = {VO -},
issn = {10636919},
keywords = {3D Objects},
mendeley-tags = {3D Objects},
pages = {402},
title = {{Towards 3D Object Detection with Bimodal Deep Boltzmann Machines over RGBD Imagery}},
year = {2015}
}
@inproceedings{Beigpour,
author = {Beigpour, S and Weijer, J {\~{}}van {\~{}}de},
booktitle = {iccv},
title = {{Object recoloring based on intrinsic image decomposition}},
year = {2011}
}
@inproceedings{Jafari2017,
abstract = {This paper addresses the task of designing a modular neural network architecture that jointly solves different tasks. As an example we use the tasks of depth estimation and semantic segmentation given a single RGB image. The main focus of this work is to analyze the cross-modality influence between depth and semantic prediction maps on their joint refinement. While most previous works solely focus on measuring improvements in accuracy, we propose a way to quantify the cross-modality influence. We show that there is a relationship between final accuracy and cross-modality influence, although not a simple linear one. Hence a larger cross-modality influence does not necessarily translate into an improved accuracy. We find that a beneficial balance between the cross-modality influences can be achieved by network architecture and conjecture that this relationship can be utilized to understand different network design choices. Towards this end we propose a Convolutional Neural Network (CNN) architecture that fuses the state of the state-of-the-art results for depth estimation and semantic labeling. By balancing the cross-modality influences between depth and semantic prediction, we achieve improved results for both tasks using the NYU-Depth v2 benchmark.},
archivePrefix = {arXiv},
arxivId = {1702.08009},
author = {Jafari, Omid Hosseini and Groth, Oliver and Kirillov, Alexander and Yang, Michael Ying and Rother, Carsten},
booktitle = {icra},
doi = {10.1109/ICRA.2017.7989537},
eprint = {1702.08009},
isbn = {9781509046331},
issn = {10504729},
keywords = {depth,joint estimation,refinement,semantic segmentation},
mendeley-tags = {depth,joint estimation,refinement,semantic segmentation},
pages = {4620--4627},
title = {{Analyzing modular CNN architectures for joint depth prediction and semantic segmentation}},
url = {https://arxiv.org/pdf/1702.08009.pdf},
year = {2017}
}
@misc{nyu2012,
abstract = {We present an approach to interpret the major surfaces, objects, and support relations of an indoor scene from an RGBD image. Most existing work ignores physical interactions or is applied only to tidy rooms and hallways. Our goal is to parse typical, often messy, indoor scenes into floor, walls, supporting surfaces, and object regions, and to recover support relationships. One of our main interests is to better understand how 3D cues can best inform a structured 3D interpretation. We also contribute a novel integer programming formulation to infer physical support relations. We offer a new dataset of 1449 RGBD images, capturing 464 diverse indoor scenes, with detailed annotations. Our experiments demonstrate our ability to infer support relations in complex scenes and verify that our 3D scene cues and inferred support lead to better object segmentation.},
archivePrefix = {arXiv},
arxivId = {arXiv:1301.3572v2},
author = {Silberman, Nathan and Hoiem, Derek and Kohli, Pushmeet and Fergus, Rob},
booktitle = {eccv},
doi = {10.1007/978-3-642-33715-4_54},
eprint = {arXiv:1301.3572v2},
file = {:home/hale/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Silberman et al. - 2012 - Indoor segmentation and support inference from RGBD images.pdf:pdf},
isbn = {9783642337147},
issn = {03029743},
keywords = {3D,3D scene,dataset,indoor,nyud2,rgbd,segmentation},
mendeley-tags = {3D,3D scene,dataset,indoor,nyud2,rgbd,segmentation},
number = {PART 5},
pages = {746--760},
title = {{Indoor Segmentation and Support Inference from RGBD Images}},
url = {http://cs.nyu.edu/{~}silberman/papers/indoor{\_}seg{\_}support.pdf},
urldate = {2016-04-26},
volume = {7576 LNCS},
year = {2012}
}
@inproceedings{Johnson2016,
author = {Johnson, Justin and Alahi, Alexandre and Fei-Fei, Li},
booktitle = {eccv},
title = {{Perceptual losses for real-time style transfer and super-resolution}},
year = {2016}
}
@inproceedings{Ikehata2015,
author = {Ikehata, Satoshi and Yan, Hang and Furukawa, Yasutaka},
booktitle = {iccv},
doi = {10.1109/ICCV.2015.156},
file = {:home/hale/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ikehata, Yan, Furukawa - 2015 - Structured Indoor Modeling.pdf:pdf},
isbn = {978-1-4673-8391-2},
pages = {1323--1331},
title = {{Structured Indoor Modeling}},
year = {2015}
}
@article{Siam_17,
author = {Siam, Mennatullah and Mahgoub, Heba and Zahran, Mohamed and Yogamani, Senthil and Jagersand, Martin and El-Sallab, Ahmad},
title = {{MODNet: Moving Object Detection Network with Motion and Appearance for Autonomous Driving}},
year = {2017}
}
@inproceedings{Chen2014,
abstract = {Deep Convolutional Neural Networks (DCNNs) have recently shown state of the art performance in high level vision tasks, such as image classification and object detection. This work brings together methods from DCNNs and probabilistic graphical models for addressing the task of pixel-level classification (also called "semantic image segmentation"). We show that responses at the final layer of DCNNs are not sufficiently localized for accurate object segmentation. This is due to the very invariance properties that make DCNNs good for high level tasks. We overcome this poor localization property of deep networks by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF). Qualitatively, our "DeepLab" system is able to localize segment boundaries at a level of accuracy which is beyond previous methods. Quantitatively, our method sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 71.6{\%} IOU accuracy in the test set. We show how these results can be obtained efficiently: Careful network re-purposing and a novel application of the 'hole' algorithm from the wavelet community allow dense computation of neural net responses at 8 frames per second on a modern GPU.},
archivePrefix = {arXiv},
arxivId = {1412.7062},
author = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L.},
eprint = {1412.7062},
file = {:home/hale/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen et al. - Unknown - DeepLab Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRF.pdf:pdf},
isbn = {9783901608353},
keywords = {Atrous Convolution,Conditional Random Fields,Index Termsâ€”Convolutional Neural Networks,Semantic Segmentation},
pages = {1--14},
title = {{DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRF}},
url = {http://arxiv.org/abs/1412.7062}
}
@article{Felzenszwalb2004,
abstract = {This paper addresses the problem of segmenting an image into regions. We define a predicate for measuring the evidence for a boundary between two regions using a graph-based representation of the image. We then develop an efficient segmentation algorithm based on this predicate, and show that although this algorithm makes greedy decisions it produces segmentations that satisfy global properties. We apply the al-gorithm to image segmentation using two different kinds of local neighborhoods in constructing the graph, and illustrate the results with both real and synthetic images. The algorithm runs in time nearly linear in the number of graph edges and is also fast in practice. An important characteristic of the method is its ability to preserve detail in low-variability image regions while ignoring detail in high-variability regions.},
annote = {http://cs.brown.edu/{\~{}}pff/segment/},
author = {Felzenszwalb, Pedro F and Huttenlocher, Daniel P},
file = {:home/hale/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Felzenszwalb, Huttenlocher - 2004 - Efficient Graph-Based Image Segmentation.pdf:pdf},
journal = {ijcv},
keywords = {clustering,graph algorithm,graph-based,image segmentation,perceptual organization,segmentation},
mendeley-tags = {graph-based,segmentation},
number = {2},
pages = {1--26},
title = {{Efficient Graph-Based Image Segmentation}},
url = {papers3://publication/uuid/D1250C05-2FC7-4954-A734-E33EBBEECB95},
volume = {59},
year = {2004}
}
@inproceedings{He2017,
author = {He, Kaiming and Gkioxari, Georgia and Dollar, Piotr and Girshick, Ross},
booktitle = {iccv},
month = {oct},
title = {{Mask R-CNN}},
year = {2017}
}
@inproceedings{Handaa,
abstract = {We introduce gvnn, a neural network library in Torch aimed towards bridging the gap between classic geometric computer vision and deep learning. Inspired by the recent success of Spatial Transformer Networks, we propose several new layers which are often used as parametric transformations on the data in geometric computer vision. These layers can be inserted within a neural network much in the spirit of the original spatial transformers and allow backpropagation to enable endtoend learning of a network involving any domain knowledge in geometric computer vision. This opens up applications in learning invariance to 3D geometric transformation for place recognition, endtoend visual odometry, depth estimation and unsupervised learning through warping with a parametric transformation for image reconstruction error.},
author = {Handa, Ankur and Bloesch, Michael and {PË˜ AtrË˜ Aucean}, Viorica and Stent, Simon and Mccormac, John and Davison, Andrew},
file = {:home/hale/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Handa et al. - Unknown - gvnn Neural Network Library for Geometric Computer Vision.pdf:pdf},
title = {{gvnn: Neural Network Library for Geometric Computer Vision}}
}
@inproceedings{imagenet,
author = {Deng, J and Dong, W and Socher, R and Li, L.-J. and Li, K and Fei-Fei, L},
booktitle = {cvpr},
keywords = {dataset,real},
mendeley-tags = {dataset,real},
title = {{ImageNet: A Large-Scale Hierarchical Image Database}},
year = {2009}
}
@article{middlebury2011,
abstract = {The quantitative evaluation of optical flow algorithms by Barron et al. (1994) led to significant advances in performance. The challenges for optical flow algorithms today go beyond the datasets and evaluation methods proposed in that paper. Instead, they center on problems associated with complex natural scenes, including nonrigid motion, real sensor noise, and motion discontinuities. We propose a new set of benchmarks and evaluation methods for the next generation of optical flow algorithms. To that end, we contribute four types of data to test different aspects of optical flow algorithms: (1) sequences with nonrigid motion where the ground-truth flow is determined by tracking hidden fluorescent texture, (2) realistic synthetic sequences, (3) high frame-rate video used to study interpolation error, and (4) modified stereo sequences of static scenes. In addition to the average angular error used by Barron et al., we compute the absolute flow endpoint error, measures for frame interpolation error, improved statistics, and results at motion discontinuities and in textureless regions. In October 2007, we published the performance of several well-known methods on a preliminary version of our data to establish the current state of the art. We also made the data freely available on the web at http://vision.middlebury.edu/flow/ . Subsequently a number of researchers have uploaded their results to our website and published papers using the data. A significant improvement in performance has already been achieved. In this paper we analyze the results obtained to date and draw a large number of conclusions from them.},
author = {Baker, Simon and Scharstein, Daniel and Lewis, J P and Roth, Stefan and Black, Michael J. and Szeliski, Richard},
doi = {10.1007/s11263-010-0390-2},
issn = {1573-1405},
journal = {ijcv},
month = {mar},
number = {1},
pages = {1--31},
title = {{A Database and Evaluation Methodology for Optical Flow}},
url = {https://doi.org/10.1007/s11263-010-0390-2},
volume = {92},
year = {2011}
}
@inproceedings{Papandreou2015,
abstract = {Deep convolutional neural networks (DCNNs) trained on a large number of images with strong pixel-level annotations have recently significantly pushed the state-of-art in semantic image segmentation. We study the more challenging problem of learning DCNNs for semantic image segmentation from either (1) weakly annotated training data such as bounding boxes or image-level labels or (2) a combination of few strongly labeled and many weakly labeled images, sourced from one or multiple datasets. We develop methods for semantic image segmentation model training under these weakly supervised and semi-supervised settings. Extensive experimental evaluation shows that the proposed techniques can learn models delivering state-of-art results on the challenging PASCAL VOC 2012 image segmentation benchmark, while requiring significantly less annotation effort.},
annote = {http://liangchiehchen.com/projects/DeepLab.html},
archivePrefix = {arXiv},
arxivId = {1502.02734},
author = {Papandreou, George and Chen, Liang-Chieh and Murphy, Kevin and Yuille, Alan L},
booktitle = {iccv},
doi = {10.1109/ICCV.2015.203},
eprint = {1502.02734},
file = {:home/hale/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Papandreou et al. - 2015 - Weakly- and Semi-Supervised Learning of a DCNN for Semantic Image Segmentation.pdf:pdf},
isbn = {978-1-4673-8391-2},
keywords = {cnn,semantic segmentation,weak},
mendeley-tags = {cnn,semantic segmentation,weak},
pages = {10},
title = {{Weakly- and Semi-Supervised Learning of a DCNN for Semantic Image Segmentation}},
url = {http://arxiv.org/abs/1502.02734},
year = {2015}
}
@inproceedings{ChenKoltun2013,
author = {Chen, Q and Koltun, V},
booktitle = {iccv},
title = {{A simple model for intrinsic image decomposition with depth cues}},
year = {2013}
}
@inproceedings{McCormac2017,
abstract = {We introduce SceneNet RGB-D, a dataset providing pixel-perfect ground truth for scene understanding prob-lems such as semantic segmentation, instance segmenta-tion, and object detection. It also provides perfect camera poses and depth data, allowing investigation into geomet-ric computer vision problems such as optical flow, cam-era pose estimation, and 3D scene labelling tasks. Ran-dom sampling permits virtually unlimited scene configu-rations, and here we provide 5M rendered RGB-D im-ages from 16K randomly generated 3D trajectories in syn-thetic layouts, with random but physically simulated ob-ject configurations. We compare the semantic segmenta-tion performance of network weights produced from pre-training on RGB images from our dataset against generic VGG-16 ImageNet weights. After fine-tuning on the SUN RGB-D and NYUv2 real-world datasets we find in both cases that the synthetically pre-trained network outper-forms the VGG-16 weights. When synthetic pre-training includes a depth channel (something ImageNet cannot na-tively provide) the performance is greater still. This sug-gests that large-scale high-quality synthetic RGB datasets with task-specific labels can be more useful for pre-training than real-world generic pre-training such as Im-ageNet. We host the dataset at http://robotvault. bitbucket.io/scenenet-rgbd.html.},
author = {Mccormac, John and Handa, Ankur and Leutenegger, Stefan and Davison, Andrew J and J.Davison, Andrew},
booktitle = {iccv},
keywords = {dataset},
mendeley-tags = {dataset},
title = {{SceneNet RGB-D: Can 5M Synthetic Images Beat Generic ImageNet Pre-training on Indoor Segmentation?}},
url = {https://www.doc.ic.ac.uk/{~}sleutene/publications/scenenet{\_}rgbd{\_}post{\_}iccv{\_}submission.pdf},
year = {2017}
}
@article{Mller2009,
author = {M{\"{u}}ller, Brigitte and Gl{\"{o}}smann, Martin and Peichl, Leo and Knop, Gabriel C and Hagemann, Cornelia and Ammerm{\"{u}}ller, Josef},
doi = {10.1371/journal.pone.0006390},
editor = {Leal, Walter S},
journal = {{\{}PLoS{\}} {\{}ONE{\}}},
month = {jul},
number = {7},
pages = {e6390},
publisher = {Public Library of Science ({\{}PLoS{\}})},
title = {{Bat Eyes Have Ultraviolet-Sensitive Cone Photoreceptors}},
url = {https://doi.org/10.1371/journal.pone.0006390},
volume = {4},
year = {2009}
}
@article{KinectNoise2,
author = {Iversen, T M and Kraft, D},
doi = {10.1049/el.2017.0392},
issn = {0013-5194},
journal = {Electronics Letters},
keywords = {Kinect v1 depth images,computer vision,computer vision algorithms,empirical noise model,image sensors,synthetic Kinect depth images},
number = {13},
pages = {856--858},
title = {{Generation of synthetic Kinect depth images based on empirical noise model}},
volume = {53},
year = {2017}
}
@article{Nawrot2014,
author = {Nawrot, Mark and Ratzlaff, Michael and Leonard, Zachary and Stroyan, Keith},
doi = {10.3389/fpsyg.2014.01103},
journal = {Frontiers in Psychology},
month = {oct},
publisher = {Frontiers Media {\{}SA{\}}},
title = {{Modeling depth from motion parallax with the motion/pursuit ratio}},
url = {https://doi.org/10.3389/fpsyg.2014.01103},
volume = {5},
year = {2014}
}
@inproceedings{Kicanaoglu2018,
author = {Kicanaoglu, B and Tao, R and Smeulders, A W M},
booktitle = {bmvc},
title = {{Estimating small differences in car-pose from orbits}},
year = {2018}
}
@inproceedings{Matsushita,
author = {Matsushita, Y and Lin, S and Kang, S. B. and Shum, H. Y.},
booktitle = {eccv},
title = {{Estimating Intrinsic Images from Image Sequences with Biased Illumination}},
year = {2004}
}
@inproceedings{ade20k,
author = {Zhou, Bolei and Zhao, Hang and Puig, Xavier and Fidler, Sanja and Barriuso, Adela and Torralba, Antonio},
booktitle = {cvpr},
title = {{Scene Parsing through ADE20K Dataset}},
year = {2017}
}
@inproceedings{Su2015,
author = {Su, H and Wang, F and Yi, E and Guibas, L},
booktitle = {iccv},
pages = {2677--2685},
title = {{3D-Assisted Feature Synthesis for Novel Views of an Object}},
year = {2015}
}
@inproceedings{Bai2016,
abstract = {We tackle the problem of estimating optical flow from a monocular camera in the context of autonomous driving. We build on the observation that the scene is typically composed of a static background, as well as a relatively small number of traffic participants which move rigidly in 3D. We propose to estimate the traffic participants using instance-level segmentation. For each traffic participant, we use the epipolar constraints that govern each independent motion for faster and more accurate estimation. Our second contribution is a new convolutional net that learns to perform flow matching, and is able to estimate the uncertainty of its matches. This is a core element of our flow estimation pipeline. We demonstrate the effectiveness of our approach in the challenging KITTI 2015 flow benchmark, and show that our approach outperforms published approaches by a large margin.},
archivePrefix = {arXiv},
arxivId = {1311.2901},
author = {Bai, Min and Luo, Wenjie and Kundu, Kaustav and Urtasun, Raquel},
booktitle = {Advances in Artificial Intelligence},
doi = {10.1007/978-3-319-46466-4_10},
eprint = {1311.2901},
isbn = {9783319464657},
issn = {16113349},
keywords = {Autonomous driving,Deep learning,Low-level vision,Optical flow,driving,optical flow,semantic segmentation},
mendeley-tags = {driving,optical flow,semantic segmentation},
organization = {Springer},
pages = {154--170},
pmid = {4520227},
title = {{Exploiting semantic information and deep matching for optical flow}},
volume = {9910 LNCS},
year = {2016}
}
@inproceedings{Rusu2009,
author = {Rusu, R B and Blodow, N and Beetz, M},
booktitle = {icra},
doi = {10.1109/ROBOT.2009.5152473},
keywords = {Clouds,Computational complexity,Convergence,Histograms,Intelligent systems,Iterative closest point algorithm,Optimization methods,Performance analysis,Robotics and automation,Robustness},
month = {may},
pages = {3212--3217},
title = {{Fast Point Feature Histograms (FPFH) for 3D registration}},
year = {2009}
}
@inproceedings{vkitti2016,
author = {Gaidon, A and Wang, Q and Cabon, Y and Vig, E},
booktitle = {cvpr},
keywords = {dataset,synthetic},
mendeley-tags = {dataset,synthetic},
title = {{Virtual Worlds as Proxy for Multi-Object Tracking Analysis}},
year = {2016}
}
@article{fusionseg,
author = {Jain, Suyog and Xiong, Bo and Grauman, Kristen},
journal = {iccv},
title = {{FusionSeg: Learning to combine motion and appearance for fully automatic segmention of generic objects in videos}},
year = {2017}
}
@inproceedings{Zitnick,
abstract = {The use of object proposals is an effective recent approach for increasing the computational efficiency of object detection. We propose a novel method for generating object bounding box proposals using edges. Edges provide a sparse yet informative representation of an image. Our main observation is that the number of contours that are wholly contained in a bounding box is indicative of the likelihood of the box containing an object. We propose a simple box objectness score that measures the number of edges that exist in the box minus those that are members of contours that overlap the box's boundary. Using efficient data structures, millions of candidate boxes can be evaluated in a fraction of a second, returning a ranked set of a few thousand top-scoring proposals. Using standard metrics, we show results that are significantly more accurate than the current state-of-the-art while being faster to compute. In particular, given just 1000 proposals we achieve over 96{\%} object recall at overlap threshold of 0.5 and over 75{\%} recall at the more challenging overlap of 0.7. Our approach runs in 0.25 seconds and we additionally demonstrate a near real-time variant with only minor loss in accuracy.},
archivePrefix = {arXiv},
arxivId = {1411.4038},
author = {Zitnick, C Lawrence and Doll{\'{a}}r, Piotr},
booktitle = {eccv},
doi = {10.1007/978-3-319-10602-1_26},
eprint = {1411.4038},
file = {:home/hale/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zitnick, Doll{\'{a}}r - 2014 - Edge boxes Locating object proposals from edges.pdf:pdf},
isbn = {978-3-319-10601-4},
issn = {16113349},
keywords = {edge detection,object detection,object proposals},
number = {PART 5},
pages = {391--405},
title = {{Edge boxes: Locating object proposals from edges}},
volume = {8693 LNCS},
year = {2014}
}
@inproceedings{Weinzaepfel2013,
address = {Washington, DC, USA},
author = {Weinzaepfel, Philippe and Revaud, Jerome and Harchaoui, Zaid and Schmid, Cordelia},
booktitle = {iccv},
doi = {10.1109/ICCV.2013.175},
isbn = {978-1-4799-2840-8},
pages = {1385--1392},
publisher = {IEEE Computer Society},
series = {ICCV '13},
title = {{DeepFlow: Large Displacement Optical Flow with Deep Matching}},
url = {http://dx.doi.org/10.1109/ICCV.2013.175},
year = {2013}
}
@article{xu2017,
author = {Xu, Jia and Ranftl, Ren{\'{e}} and Koltun, Vladlen},
journal = {arXiv preprint arXiv:1704.07325},
keywords = {optical flow},
mendeley-tags = {optical flow},
title = {{Accurate optical flow via direct cost volume processing}},
year = {2017}
}
@inproceedings{Simonyan2014,
author = {Simonyan, K and Zisserman, A},
booktitle = {nips},
title = {{Two-Stream Convolutional Networks for Action Recognition in Videos}},
year = {2014}
}
@inproceedings{Shotton2013,
abstract = {We address the problem of inferring the pose of an RGB-D camera relative to a known 3D scene, given only a single acquired image. Our approach employs a regression forest that is capable of inferring an estimate of each pixel's correspondence to 3D points in the scene's world coordinate frame. The forest uses only simple depth and RGB pixel comparison features, and does not require the computation of feature descriptors. The forest is trained to be capable of predicting correspondences at any pixel, so no interest point detectors are required. The camera pose is inferred using a robust optimization scheme. This starts with an initial set of hypothesized camera poses, constructed by applying the forest at a small fraction of image pixels. Preemptive RANSAC then iterates sampling more pixels at which to evaluate the forest, counting inliers, and refining the hypothesized poses. We evaluate on several varied scenes captured with an RGB-D camera and observe that the proposed technique achieves highly accurate relocalization and substantially out-performs two state of the art baselines.},
author = {Shotton, Jamie and Glocker, Ben and Zach, Christopher and Izadi, Shahram and Criminisi, Antonio and Fitzgibbon, Andrew},
booktitle = {cvpr},
doi = {10.1109/CVPR.2013.377},
file = {:home/hale/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shotton et al. - 2013 - Scene coordinate regression forests for camera relocalization in RGB-D images.pdf:pdf},
isbn = {978-0-7695-4989-7},
issn = {10636919},
keywords = {localization},
mendeley-tags = {localization},
month = {jun},
pages = {2930--2937},
publisher = {IEEE},
title = {{Scene Coordinate Regression Forests for Camera Relocalization in RGB-D Images}},
url = {https://www.microsoft.com/en-us/research/publication/scene-coordinate-regression-forests-for-camera-relocalization-in-rgb-d-images/},
year = {2013}
}
@article{Aggarwal1988,
author = {Aggarwal, J K and Nandhakumar, N},
journal = {Proceedings of the IEEE},
keywords = {dataset,optical flow,review},
mendeley-tags = {dataset,optical flow,review},
number = {8},
pages = {917--935},
title = {{On the computation of motion from sequences of images-A review}},
volume = {76},
year = {1988}
}
@inproceedings{Gupta2013,
abstract = {Extracting high-quality dynamic foreground layers from a video sequence is a challenging problem due to the coupling of color, motion, and occlusion. Many approaches assume that the background scene is static or undergoes the planar perspective transformation. In this paper, we relax these restrictions and present a comprehensive system for accurately computing object motion, layer, and depth information. A novel algorithm that combines different clues to extract the foreground layer is proposed, where a voting-like scheme robust to outliers is employed in optimization. The system is capable of handling difficult examples in which the background is nonplanar and the camera freely moves during video capturing. Our work finds several applications, such as high-quality view interpolation and video editing.},
author = {Gupta, Saurabh and Arbelaez, Pablo and Malik, Jitendra},
booktitle = {cvpr},
doi = {10.1109/CVPR.2013.79},
file = {:home/hale/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gupta, Arbelaez, Malik - 2013 - Perceptual organization and recognition of indoor scenes from RGB-D images.pdf:pdf},
isbn = {978-1-4673-0063-6},
issn = {10636919},
keywords = {RGBD Recognition,RGBD Segmentation,nyud2,rgbd},
mendeley-tags = {nyud2,rgbd},
pages = {564--571},
pmid = {20530810},
title = {{Perceptual organization and recognition of indoor scenes from RGB-D images}},
year = {2013}
}
@inproceedings{deeptrack,
author = {{H. Li}, Y Li and Porikli, F},
booktitle = {bmvc},
title = {{DeepTrack: Learning Discriminative Feature Representations by Convolutional Neural Networks for Visual Tracking}},
year = {2014}
}
@article{SegNet,
abstract = {We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network [1]. The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable filters to produce dense feature maps.We compare our proposed architecture with the fully convolutional network [2] architecture and its variants. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance. The design of SegNet was primarily motivated by road scene understanding applications. Hence, it is efficient both in terms of memory and computational time during inference. It is also significantly smaller in the number of trainable parameters than competing architectures and can be trained end-to-end using stochastic gradient descent without complex training protocols.We also benchmark the performance of SegNet on Pascal VOC12 salient object segmentation and the recent SUN RGB-D indoor scene understanding challenge. These quantitative assessments show that SegNet provides competitive performance although it is significantly faster than other architectures.We also provide a Caffe implementation of SegNet and a web demo at http://mi.eng.cam.ac.uk/projects/segnet/. Index},
archivePrefix = {arXiv},
arxivId = {1505.0729},
author = {Badrinarayanan, Vijay and Kendall, Alex and Cipolla, Roberto},
doi = {10.1103/PhysRevX.5.041024},
eprint = {1505.0729},
file = {:home/hale/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Badrinarayanan, Kendall, Cipolla - 2017 - SegNet A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation.pdf:pdf},
issn = {21603308},
journal = {tpami},
keywords = {cnn,deep,fcn,segmentation,semantic segmentation},
mendeley-tags = {cnn,deep,fcn,segmentation,semantic segmentation},
title = {{SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation}},
url = {https://arxiv.org/abs/1511.00561},
year = {2017}
}
@inproceedings{genericmodel,
author = {Fan, Q and Yang, J and Hua, G and Chen, B and Wipf, D},
booktitle = {iccv},
title = {{A Generic Deep Architecture for Single Image Reflection Removal and Image Smoothing}},
year = {2017}
}
@inproceedings{Funt,
author = {Funt, Brian V. and Drew, M S and Brockington, M},
booktitle = {eccv},
title = {{Recovering shading from color images}},
year = {1992}
}
@inproceedings{Zeiler2014,
abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky $\backslash$etal on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
archivePrefix = {arXiv},
arxivId = {1311.2901},
author = {Zeiler, Matthew D and Fergus, Rob},
booktitle = {eccv},
doi = {10.1007/978-3-319-10590-1_53},
eprint = {1311.2901},
file = {:home/hale/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zeiler, Fergus - 2014 - Visualizing and understanding convolutional networks.pdf:pdf},
isbn = {9783319105895},
issn = {16113349},
keywords = {CNN,basic,deep learning},
mendeley-tags = {CNN,basic,deep learning},
number = {PART 1},
pages = {818--833},
pmid = {26353135},
title = {{Visualizing and understanding convolutional networks}},
url = {http://link.springer.com/10.1007/978-3-319-10590-1{\_}53{\%}5Cnhttp://arxiv.org/abs/1311.2901{\%}5Cnpapers3://publication/uuid/44feb4b1-873a-4443-8baa-1730ecd16291},
volume = {8689 LNCS},
year = {2014}
}
@misc{stanford23d2017arxiv,
archivePrefix = {arXiv},
arxivId = {cs.CV/1702.01105},
author = {Armeni, I and Sax, A and Zamir, A.{\~{}}R. and Savarese, S},
booktitle = {ArXiv e-prints},
eprint = {1702.01105},
keywords = {Computer Science - Computer Vision and Pattern Rec,Computer Science - Robotics},
month = {feb},
primaryClass = {cs.CV},
title = {{Joint 2D-3D-Semantic Data for Indoor Scene Understanding}},
year = {2017}
}
@inproceedings{Mahjourian2018,
abstract = {We present a novel approach for unsupervised learning of depth and ego-motion from monocular video. Unsupervised learning removes the need for separate supervisory signals (depth or ego-motion ground truth, or multi-view video). Prior work in unsupervised depth learning uses pixel-wise or gradient-based losses, which only consider pixels in small local neighborhoods. Our main contribution is to explicitly consider the inferred 3D geometry of the whole scene, and enforce consistency of the estimated 3D point clouds and ego-motion across consecutive frames. This is a challenging task and is solved by a novel (approximate) backpropagation algorithm for aligning 3D structures. We combine this novel 3D-based loss with 2D losses based on photometric quality of frame reconstructions using estimated depth and ego-motion from adjacent frames. We also incorporate validity masks to avoid penalizing areas in which no useful information exists. We test our algorithm on the KITTI dataset and on a video dataset captured on an uncalibrated mobile phone camera. Our proposed approach consistently improves depth estimates on both datasets, and outperforms the state-of-the-art for both depth and ego-motion. Because we only require a simple video, learning depth and ego-motion on large and varied datasets becomes possible. We demonstrate this by training on the low quality uncalibrated video dataset and evaluating on KITTI, ranking among top performing prior methods which are trained on KITTI itself.1},
archivePrefix = {arXiv},
arxivId = {1802.05522},
author = {Mahjourian, Reza and Wicke, Martin and Angelova, Anelia},
booktitle = {cvpr},
doi = {10.1109/CVPR.2018.00594},
eprint = {1802.05522},
file = {:home/hale/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mahjourian, Wicke, Angelova - 2018 - Unsupervised Learning of Depth and Ego-Motion from Monocular Video Using 3D Geometric Constraints.pdf:pdf},
isbn = {9781538664209},
issn = {10636919},
pages = {5667--5675},
title = {{Unsupervised Learning of Depth and Ego-Motion from Monocular Video Using 3D Geometric Constraints}},
url = {http://sites.google.com/view/vid2depth http://openaccess.thecvf.com/content{\_}cvpr{\_}2018/papers/Mahjourian{\_}Unsupervised{\_}Learning{\_}of{\_}CVPR{\_}2018{\_}paper.pdf},
year = {2018}
}
@inproceedings{Zheng2015,
abstract = {Pixel-level labelling tasks, such as semantic segmentation, play a central role in image understanding. Recent approaches have attempted to harness the capabilities of deep learning techniques for image recognition to tackle pixel-level labelling tasks. One central issue in this methodology is the limited capacity of deep learning techniques to delineate visual objects. To solve this problem, we introduce a new form of convolutional neural network that combines the strengths of Convolutional Neural Networks (CNNs) and Conditional Random Fields (CRFs)-based probabilistic graphical modelling. To this end, we formulate Conditional Random Fields as Recurrent Neural Networks. This network, called CRF-RNN, is then plugged in as a part of a CNN to obtain a deep network that has desirable properties of both CNNs and CRFs. Importantly, our system fully integrates CRF modelling with CNNs, making it possible to train the whole deep network end-to-end with the usual back-propagation algorithm, avoiding offline post-processing methods for object delineation. We apply the proposed method to the problem of semantic image segmentation, obtaining top results on the challenging Pascal VOC 2012 segmentation benchmark.},
archivePrefix = {arXiv},
arxivId = {1502.03240},
author = {Zheng, Shuai and Jayasumana, Sadeep and Romera-Paredes, Bernardino and Vineet, Vibhav and Su, Zhizhong and Du, Dalong and Huang, Chang and Torr, Philip H S},
booktitle = {iccv},
doi = {10.1109/ICCV.2015.179},
eprint = {1502.03240},
file = {:home/hale/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zheng et al. - 2015 - Conditional Random Fields as Recurrent Neural Networks.pdf:pdf},
isbn = {978-1-4673-8391-2},
keywords = {semantic segmentation},
mendeley-tags = {semantic segmentation},
pages = {16},
title = {{Conditional Random Fields as Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1502.03240},
year = {2015}
}
@inproceedings{Zitnick2004,
address = {New York, NY, USA},
author = {Zitnick, C Lawrence and Kang, Sing Bing and Uyttendaele, Matthew and Winder, Simon and Szeliski, Richard},
booktitle = {tog},
doi = {10.1145/1186562.1015766},
isbn = {9781450378239},
keywords = {Computer Vision,Dynamic Scenes,Image-Based Rendering},
pages = {600--608},
publisher = {Association for Computing Machinery},
series = {SIGGRAPH '04},
title = {{High-Quality Video View Interpolation Using a Layered Representation}},
url = {https://doi.org/10.1145/1186562.1015766},
year = {2004}
}
@inproceedings{KinectNoise1,
author = {Nguyen, C V and Izadi, S and Lovell, D},
booktitle = {2012 Second International Conference on 3D Imaging, Modeling, Processing, Visualization Transmission},
doi = {10.1109/3DIMPVT.2012.84},
issn = {1550-6185},
keywords = {3D reconstruction,3D tracking,Cameras,Image edge detection,Kinect depth maps,Kinect sensor noise modeling,KinectFusion system,Mathematical model,Noise,Noise measurement,Robot sensing systems,Solid modeling,axial noise distributions,computer vision,image fusion,image reconstruction,lateral noise distributions,object tracking,pose estimation,volumetric fusion},
month = {oct},
pages = {524--530},
title = {{Modeling Kinect Sensor Noise for Improved 3D Reconstruction and Tracking}},
year = {2012}
}
@inproceedings{viper2017,
author = {Richter, Stephan R and Hayder, Zeeshan and Koltun, Vladlen},
booktitle = {iccv},
title = {{Playing for Benchmarks}},
year = {2017}
}
@inproceedings{sintel2012,
author = {Butler, Daniel J. and Wulff, Jonas and Stanley, Garrett B. and Black, Michael J.},
booktitle = {eccv},
editor = {{A. Fitzgibbon et al. (Eds.)}},
keywords = {dataset,synthetic},
mendeley-tags = {dataset,synthetic},
month = {oct},
pages = {611--625},
publisher = {Springer-Verlag},
series = {Part IV, LNCS 7577},
title = {{A naturalistic open source movie for optical flow evaluation}},
year = {2012}
}
@incollection{GAN,
author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
booktitle = {nips},
editor = {Ghahramani, Z and Welling, M and Cortes, C and Lawrence, N D and Weinberger, K Q},
pages = {2672--2680},
publisher = {Curran Associates, Inc.},
title = {{Generative Adversarial Nets}},
url = {http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf},
year = {2014}
}
@article{Otte1995,
abstract = {This contribution11Partial results have been presented in a preliminary version at the ECCV '94 [42]. investigates local differential techniques for estimating optical flow and its derivatives based on the brightness change constraint. By using the tensor calculus representation we build the Taylor expansion of the gray-value derivatives as well as of the optical flow in a spatiotemporal neighborhood. Such a formulation provides a unifying framework for all existing local differential approaches and allows to derive new systems of equations for the estimation of the optical flow and of its derivatives. We also tested various optical flow estimation approaches on real image sequences recorded by a calibrated camera which was fixed on the arm of a robot. By moving the arm of the robot along a precisely defined trajectory, we can determine the true displacement rate of scene surface elements projected into the image plane and compare it quantitatively with the results of different optical flow estimators. Since the optical flow estimators are based on gray-value derivatives of up to fourth-order, we were forced to develop modified Gaussian derivative filters to obtain acceptable estimates for the derivatives. Further, we show quantitatively that these filters contribute to a much more robust optical flow estimation. In addition, successive lines of TV-cameras have an offset in time due to the interlace technique. We demonstrate the adaptation of filter kernels for estimating higher-order spatiotemporal derivatives in interlaced image sequences.},
annote = {Special Volume on Computer Vision},
author = {Otte, Michael and Nagel, Hans-Hellmut},
doi = {https://doi.org/10.1016/0004-3702(95)00033-X},
issn = {0004-3702},
journal = {Artificial Intelligence},
keywords = {dataset,optical flow},
mendeley-tags = {dataset,optical flow},
number = {1},
pages = {5--43},
title = {{Estimation of optical flow based on higher-order spatiotemporal derivatives in interlaced and non-interlaced image sequences}},
url = {http://www.sciencedirect.com/science/article/pii/000437029500033X},
volume = {78},
year = {1995}
}
@inproceedings{tappenn,
author = {Tappen, Marshall F. and Adelson, Edward H. and Freeman, William T.},
booktitle = {cvpr},
title = {{Estimating intrinsic component images using non-linear regression}},
year = {2006}
}
@inproceedings{Gao2018,
author = {Gao, Ruohan and Xiong, Bo and Grauman, Kristen},
booktitle = {cvpr},
month = {jun},
title = {{Im2Flow: Motion Hallucination From Static Images for Action Recognition}},
year = {2018}
}
@inproceedings{Barth2017,
abstract = {A bottleneck of state-of-the-art machine learning methods, e.g. deep learning, for plant part image segmentation in agricultural robotics is the requirement of large manually annotated datasets. As a solution, large synthetic datasets including ground truth can be rendered that realistically reflect the empirical situation. However, a dissimilarity gap can remain between synthetic and empirical data by incomplete manual modelling. This paper contributes to closing this gap by optimising the realism of synthetic agricultural images using unsupervised cycle generative adversarial networks, enabling unpaired image-to-image translation from the synthetic to empirical domain and vice versa. For this purpose, the Capsicum annuum (sweet- or bell pepper) dataset was used, containing 10,500 synthetic and 50 empirical annotated images. Additionally, 225 unlabelled empirical images were used. We hypothesised that the similarity of the synthetic images with the empirical images increases qualitatively and quantitively when translated to the empirical domain and investigated the effect of the translation on the factors color, local texture and morphology. Results showed an increased mean class color distribution correlation with the empirical dataset from 0.62 prior and 0.90 post translation of the synthetic dataset. Qualitatively, synthetic images translate very well in local features such as color,illumination scattering and texture. However, global features like plant morphology appeared not to be translatable.},
author = {Barth, R and IJsselmuiden, J M M and Hemming, J and van Henten, E J},
booktitle = {Proceedings of the IEEE IROS workshop on Agricultural Robotics},
editor = {Kounalakis, Tsampikos and van Evert, Frits and Ball, David Michael and Kootstra, Gert and Nalpantidis, Lazaros},
keywords = {dataset,synthetic},
mendeley-tags = {dataset,synthetic},
pages = {18--22},
publisher = {Wageningen University {\&} Research},
title = {{Optimising Realism of Synthetic Agricultural Images using Cycle Generative Adversarial Networks}},
year = {2017}
}
@inproceedings{Choi2018,
abstract = {Recent studies have shown remarkable success in image-to-image translation for two domains. However, existing approaches have limited scalability and robustness in handling more than two domains, since different models should be built independently for every pair of image domains. To address this limitation, we propose StarGAN, a novel and scalable approach that can perform image-to-image translations for multiple domains using only a single model. Such a unified model architecture of StarGAN allows simultaneous training of multiple datasets with different domains within a single network. This leads to StarGAN's superior quality of translated images compared to existing models as well as the novel capability of flexibly translating an input image to any desired target domain. We empirically demonstrate the effectiveness of our approach on a facial attribute transfer and a facial expression synthesis tasks.},
archivePrefix = {arXiv},
arxivId = {1711.09020},
author = {Choi, Yunjey and Choi, Minje and Kim, Munyoung and Ha, Jung-Woo and Kim, Sunghun and Choo, Jaegul},
booktitle = {cvpr},
doi = {10.1016/J.PHYSLETB.2017.12.053},
eprint = {1711.09020},
file = {:home/hale/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Choi et al. - 2018 - StarGAN Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation.pdf:pdf},
isbn = {9781467398947},
issn = {0717-6163},
keywords = {conditional,gan,multi-domain},
mendeley-tags = {conditional,gan,multi-domain},
pmid = {172668},
title = {{StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation}},
url = {https://arxiv.org/pdf/1711.09020.pdf http://arxiv.org/abs/1711.09020},
year = {2018}
}
@inproceedings{Yan,
author = {Yan, X and Shen, J and He, Y and Mao, X},
booktitle = {Proc. DICTA},
title = {{Retexturing by intrinsic video}},
year = {2010}
}
@inproceedings{Tokmakov_2017,
abstract = {The problem of determining whether an object is in motion, irrespective of camera motion, is far from being solved. We address this challenging task by learning motion patterns in videos. The core of our approach is a fully convolutional network, which is learned entirely from synthetic video sequences, and their ground-truth optical flow and motion segmentation. This encoder-decoder style architecture first learns a coarse representation of the optical flow field features, and then refines it iteratively to produce motion labels at the original high-resolution. We further improve this labeling with an objectness map and a conditional random field, to account for errors in optical flow, and also to focus on moving "things" rather than "stuff". The output label of each pixel denotes whether it has undergone independent motion, i.e., irrespective of camera motion. We demonstrate the benefits of this learning framework on the moving object segmentation task, where the goal is to segment all objects in motion. Our approach outperforms the top method on the recently released DAVIS benchmark dataset, comprising real-world sequences, by 5.6{\%}. We also evaluate on the Berkeley motion segmentation database, achieving state-of-the-art results.},
archivePrefix = {arXiv},
arxivId = {1612.07217},
author = {Tokmakov, Pavel and Alahari, Karteek and Schmid, Cordelia},
booktitle = {cvpr},
doi = {10.1109/CVPR.2017.64},
eprint = {1612.07217},
title = {{Learning Motion Patterns in Videos}},
url = {http://arxiv.org/abs/1612.07217},
year = {2017}
}
@article{Brox2011,
author = {Brox, Thomas and Malik, Jitendra},
doi = {10.1109/TPAMI.2010.143},
issn = {0162-8828},
journal = {tpami},
month = {mar},
number = {3},
pages = {500--513},
title = {{Large Displacement Optical Flow: Descriptor Matching in Variational Motion Estimation}},
volume = {33},
year = {2011}
}
@inproceedings{Sevilla-Lara_2016_CVPR,
author = {Sevilla-Lara, Laura and Sun, Deqing and Jampani, Varun and Black, Michael J.},
booktitle = {cvpr},
month = {jun},
title = {{Optical Flow With Semantic Segmentation and Localized Layers}},
year = {2016}
}
@misc{Lai2014,
abstract = {This paper presents an approach for labeling objects in 3D scenes. We introduce HMP3D, a hierarchical sparse coding technique for learning features from 3D point cloud data. HMP3D classifiers are trained using a synthetic dataset of virtual scenes generated using CAD models from an online database. Our scene labeling system combines features learned from raw RGB-D images and 3D point clouds directly, without any hand-designed features, to assign an object label to every 3D point in the scene. Experiments on the RGB-D Scenes Dataset v.2 demonstrate that the proposed approach can be used to label indoor scenes containing both small tabletop objects and large furniture pieces.},
author = {Lai, Kevin and Bo, Liefeng and Fox, Dieter},
booktitle = {icra},
doi = {10.1109/ICRA.2014.6907298},
file = {:home/hale/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lai, Bo, Fox - Unknown - Unsupervised Feature Learning for 3D Scene Labeling.pdf:pdf},
isbn = {978-1-4799-3685-4},
issn = {10504729},
keywords = {3D scene,3D scene labelling,labelling,unsupervised},
mendeley-tags = {3D scene,3D scene labelling,labelling,unsupervised},
pages = {3050--3057},
title = {{Unsupervised Feature Learning for 3D Scene Labeling}},
url = {http://homes.cs.washington.edu/{~}kevinlai/publications/lai{\_}icra14.pdf},
urldate = {2016-04-26},
year = {2014}
}
@inproceedings{kim,
author = {Kim, S and Park, K and Sohn, K and Lin, S},
booktitle = {cvpr},
title = {{Unified Depth Prediction and Intrinsic Image Decomposition from a Single Image via Joint Convolutional Neural Fields}},
year = {2016}
}
@misc{Sarode2019,
abstract = {PointNet has recently emerged as a popular representation for unstructured point cloud data, allowing application of deep learning to tasks such as object detection, segmentation and shape completion. However, recent works in literature have shown the sensitivity of the PointNet representation to pose misalignment. This paper presents a novel framework that uses PointNet encoding to align point clouds and perform registration for applications such as 3D reconstruction, tracking and pose estimation. We develop a framework that compares PointNet features of template and source point clouds to find the transformation that aligns them accurately. In doing so, we avoid computationally expensive correspondence finding steps, that are central to popular registration methods such as ICP and its variants. Depending on the prior information about the shape of the object formed by the point clouds, our framework can produce approaches that are shape specific or general to unseen shapes. Our framework produces approaches that are robust to noise and initial misalignment in data and work robustly with sparse as well as partial point clouds. We perform extensive simulation and real-world experiments to validate the efficacy of our approach and compare the performance with state-of-art approaches. Code is available at https://github.com/vinits5/pointnet-registrationframework.},
archivePrefix = {arXiv},
arxivId = {1912.05766},
author = {Sarode, Vinit and Li, Xueqian and Goforth, Hunter and Aoki, Yasuhiro and Dhagat, Animesh and Srivatsan, Rangaprasad Arun and Lucey, Simon and Choset, Howie},
eprint = {1912.05766},
keywords = {deep learning,pc matching,point cloud,point net},
mendeley-tags = {deep learning,pc matching,point cloud,point net},
month = {dec},
title = {{One Framework to Register Them All: PointNet Encoding for Point Cloud Alignment}},
year = {2019}
}
@article{Chen2016,
author = {Chen, L.-C. and Papandreou, G and Kokkinos, I and Murphy, K and Yuille, A.-L.},
journal = {arXiv preprint arXiv:1606.00915},
title = {{Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs}},
year = {2016}
}
@article{Duchene,
author = {Duch{\^{e}}ne, S and Riant, C and Chaurasia, G and Moreno, J. L. and Laffont, P. Y. and Popov, S and Bousseau, A and Drettakis, G},
journal = {tog},
title = {{Multi-view intrinsic images of outdoors scenes with an application to relighting}},
year = {2015}
}
@article{Meka0,
author = {Meka, A and Zollh{\"{o}}fer, M and Richardt, C and Theobalt, C},
journal = {tog},
title = {{Live intrinsic video}},
year = {2016}
}
@article{Maddern2017,
author = {Maddern, Will and Pascoe, Geoff and Linegar, Chris and Newman, Paul},
doi = {10.1177/0278364916679498},
journal = {The International Journal of Robotics Research (IJRR)},
keywords = {dataset,real},
mendeley-tags = {dataset,real},
number = {1},
pages = {3--15},
title = {{1 Year, 1000km: The Oxford RobotCar Dataset}},
url = {http://dx.doi.org/10.1177/0278364916679498},
volume = {36},
year = {2017}
}
@inproceedings{Su2015,
abstract = {A longstanding question in computer vision concerns the representation of 3D shapes for recognition: should 3D shapes be represented with descriptors operating on their native 3D formats, such as voxel grid or polygon mesh, or can they be effectively represented with view-based descriptors? We address this question in the context of learning to recognize 3D shapes from a collection of their rendered views on 2D images. We first present a standard CNN architecture trained to recognize the shapes' rendered views independently of each other, and show that a 3D shape can be recognized even from a single view at an accuracy far higher than using state-of-the-art 3D shape descriptors. Recognition rates further increase when multiple views of the shapes are provided. In addition, we present a novel CNN architecture that combines information from multiple views of a 3D shape into a single and compact shape descriptor offering even better recognition performance. The same architecture can be applied to accurately recognize human hand-drawn sketches of shapes. We conclude that a collection of 2D views can be highly informative for 3D shape recognition and is amenable to emerging CNN architectures and their derivatives.},
archivePrefix = {arXiv},
arxivId = {arXiv:1505.0088},
author = {Su, Hang and Maji, Subhransu and Kalogerakis, Evangelos and Learned-Miller, Erik},
booktitle = {iccv},
doi = {10.1109/ICCV.2015.114},
eprint = {arXiv:1505.0088},
file = {:home/hale/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Su et al. - Unknown - Multi-view Convolutional Neural Networks for 3D Shape Recognition.pdf:pdf},
isbn = {978-1-4673-8391-2},
issn = {15505499},
keywords = {2D images,3D shape descriptors,3D shape recognition,3D shape representation,Cameras,Computer architecture,Computer vision,Image recognition,Shape,Solid modeling,Three-dimensional displays,computer vision,convolution,hand-drawn shape sketch recognition,image representation,learning,learning (artificial intelligence),mesh generation,multiview convolutional neural networks,neural nets,polygon mesh,rendering (computer graphics),shape recognition,shape rendered views,view-based descriptors,voxel grid},
pages = {945--953},
title = {{Multi-view Convolutional Neural Networks for 3D Shape Recognition}},
url = {http://arxiv.org/abs/1505.00880 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7410471},
year = {2015}
}
@inproceedings{Wang2016,
abstract = {Current generative frameworks use end-to-end learning and generate images by sampling from uniform noise distribution. However, these approaches ignore the most basic principle of image formation: images are product of: (a) Structure: the underlying 3D model; (b) Style: the texture mapped onto structure. In this paper, we factorize the image generation process and propose Style and Structure Generative Adversarial Network (S{\^{}}2-GAN). Our S{\^{}}2-GAN has two components: the Structure-GAN generates a surface normal map; the Style-GAN takes the surface normal map as input and generates the 2D image. Apart from a real vs. generated loss function, we use an additional loss with computed surface normals from generated images. The two GANs are first trained independently, and then merged together via joint learning. We show our S{\^{}}2-GAN model is interpretable, generates more realistic images and can be used to learn unsupervised RGBD representations.},
archivePrefix = {arXiv},
arxivId = {1603.05631},
author = {Wang, Xiaolong and Gupta, Abhinav},
booktitle = {eccv},
doi = {10.1007/978-3-319-46493-0_20},
eprint = {1603.05631},
file = {:home/hale/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang, Gupta - 2016 - Generative image modeling using style and structure adversarial networks.pdf:pdf},
isbn = {9783319464923},
issn = {16113349},
keywords = {conditional,gan,surface normal},
mendeley-tags = {conditional,gan,surface normal},
pmid = {10463930},
title = {{Generative image modeling using style and structure adversarial networks}},
url = {https://arxiv.org/pdf/1603.05631.pdf},
year = {2016}
}
@inproceedings{Ladicky2014,
abstract = {In this work we propose the method for a rather unexplored problem of computer vision - discriminatively trained dense surface normal estimation from a single image. Our method combines contextual and segment-based cues and builds a regressor in a boosting framework by transforming the problem into the regression of coefficients of a local coding. We apply our method to two challenging data sets containing images of man-made environments, the indoor NYU2 data set and the outdoor KITTI data set. Our surface normal predictor achieves results better than initially expected, significantly outperforming state-of-the-art.},
address = {Cham},
author = {Ladick{\'{y}}, L'ubor and Zeisl, Bernhard and Pollefeys, Marc},
booktitle = {eccv},
editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
isbn = {978-3-319-10602-1},
keywords = {context,segmentation,surface normal},
mendeley-tags = {context,segmentation,surface normal},
pages = {468--484},
publisher = {Springer International Publishing},
title = {{Discriminatively Trained Dense Surface Normal Estimation}},
url = {https://www.inf.ethz.ch/personal/ladickyl/normals{\_}eccv14.pdf},
year = {2014}
}
@inproceedings{Regmi2018,
author = {Regmi, Krishna and Borji, Ali},
booktitle = {cvpr},
keywords = {cGAN,cross-view,generative},
mendeley-tags = {cGAN,cross-view,generative},
month = {jun},
title = {{Cross-View Image Synthesis Using Conditional GANs}},
year = {2018}
}
@inproceedings{mit2009,
author = {Grosse, Roger and Johnson, Micah K. and Adelson, Edward H. and Freeman, William T.},
booktitle = {iccv},
title = {{Ground truth dataset and baseline evaluations for intrinsic image algorithms}},
year = {2009}
}
@article{Latecki2015,
abstract = {In this paper, we address the problem of semantic scene segmentation of RGB-D images of indoor scenes. We pro-pose a novel image region labeling method which augments CRF formulation with hard mutual exclusion (mutex) con-straints. This way our approach can make use of rich and accurate 3D geometric structure coming from Kinect in a principled manner. The final labeling result must satisfy all mutex constraints, which allows us to eliminate configu-rations that violate common sense physics laws like plac-ing a floor above a night stand. Three classes of mu-tex constraints are proposed: global object co-occurrence constraint, relative height relationship constraint, and lo-cal support relationship constraint. We evaluate our ap-proach on the NYU-Depth V2 dataset, which consists of 1449 cluttered indoor scenes, and also test generalization of our model trained on NYU-Depth V2 dataset directly on a recent SUN3D dataset without any new training. The ex-perimental results show that we significantly outperform the state-of-the-art methods in scene labeling on both datasets.},
author = {Latecki, Longin Jan},
doi = {10.1109/ICCV.2015.202},
isbn = {978-1-4673-8391-2},
journal = {iccv},
keywords = {rgbd,segmentation},
mendeley-tags = {rgbd,segmentation},
title = {{Semantic Segmentation of RGBD Images with Mutex Constraints}},
year = {2015}
}
@article{Williams2015,
author = {Williams, D L},
doi = {10.1038/eye.2015.220},
journal = {Eye},
month = {nov},
number = {2},
pages = {173--178},
publisher = {Springer Science and Business Media {\{}LLC{\}}},
title = {{Light and the evolution of vision}},
url = {https://doi.org/10.1038/eye.2015.220},
volume = {30},
year = {2015}
}
@inproceedings{Seitz2006,
address = {USA},
author = {Seitz, Steven M and Curless, Brian and Diebel, James and Scharstein, Daniel and Szeliski, Richard},
booktitle = {cvpr},
doi = {10.1109/CVPR.2006.19},
isbn = {0769525970},
pages = {519--528},
publisher = {IEEE Computer Society},
series = {CVPR '06},
title = {{A Comparison and Evaluation of Multi-View Stereo Reconstruction Algorithms}},
url = {https://doi.org/10.1109/CVPR.2006.19},
year = {2006}
}
@inproceedings{Long2015,
abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a novel architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20{\%} relative improvement to 62.2{\%} mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes one third of a second for a typical image.},
archivePrefix = {arXiv},
arxivId = {1411.4038},
author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
booktitle = {cvpr},
doi = {10.1109/CVPR.2015.7298965},
eprint = {1411.4038},
isbn = {9781467369640},
issn = {10636919},
keywords = {caffe,cnn,fcn,rgb,rgbd,semantic segmentation},
mendeley-tags = {caffe,cnn,fcn,rgb,rgbd,semantic segmentation},
pages = {3431--3440},
pmid = {16190471},
title = {{Fully convolutional networks for semantic segmentation}},
volume = {07-12-June},
year = {2015}
}
@inproceedings{gtav2018,
author = {Krahenbuhl, P},
booktitle = {cvpr},
pages = {2955--2964},
title = {{Free Supervision from Video Games}},
year = {2018}
}
@article{Horn1981,
abstract = {Optical flow cannot be computed locally, since only one independent measurement is available from the image sequence at a point, while the flow velocity has two components. A second constraint is needed. A method for finding the optical flow pattern is presented which assumes that the apparent velocity of the brightness pattern varies smoothly almost everywhere in the image. An iterative implementation is shown which successfully computes the optical flow for a number of synthetic image sequences. The algorithm is robust in that it can handle image sequences that are quantized rather coarsely in space and time. It is also insensitive to quantization of brightness levels and additive noise. Examples are included where the assumption of smoothness is violated at singular points or along lines in the image. {\textcopyright} 1981.},
archivePrefix = {arXiv},
arxivId = {1412.0767},
author = {Horn, Berthold K.P. and Schunck, Brian G.},
doi = {10.1016/0004-3702(81)90024-2},
eprint = {1412.0767},
isbn = {0867204524},
issn = {00043702},
journal = {Artificial Intelligence},
keywords = {optical flow,pioneer},
mendeley-tags = {optical flow,pioneer},
month = {aug},
number = {1-3},
pages = {185--203},
pmid = {14765965},
publisher = {Elsevier},
title = {{Determining optical flow}},
url = {https://www.sciencedirect.com/science/article/pii/0004370281900242},
volume = {17},
year = {1981}
}
@article{Douillard2010,
abstract = {This paper presents algorithms for fast segmentation of 3D point clouds and subsequent classification of the obtained 3D segments. The method jointly determines the ground surface and segments individual ob-jects in 3D, including overhanging structures. When compared to six other terrain modelling techniques, this approach has minimal error between the sensed data and the representation; and is fast (processing a Velodyne scan in approximately 2 seconds). Applications include improved alignment of suc-cessive scans by enabling operations in sections (Velodyne scans are aligned 7{\%} sharper compared to an approach using raw points) and more informed decision-making (paths move around overhangs). The use of segmentation to aid classification through 3D features, such as the Spin Image or the Spher-ical Harmonic Descriptor, is discussed and experimentally compared. More-over, the segmentation facilitates a novel approach to 3D classification that bypasses feature extraction and directly compares 3D shapes via the ICP algorithm. This technique is shown to achieve accuracy on par with the best feature based classifier (92.1{\%}) while being significantly faster and allowing a clearer understanding of the classifier's behaviour.},
author = {Douillard, B and Underwood, J and Vlaskine, V and Quadros, A and Singh, S},
file = {:home/hale/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Douillard et al. - 2010 - A pipeline for the segmentation and classification of 3D point clouds.pdf:pdf},
journal = {In ISER},
keywords = {3d,point cloud,segmentation},
mendeley-tags = {3d,point cloud,segmentation},
pages = {1--15},
title = {{A pipeline for the segmentation and classification of 3D point clouds}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-28572-1{\_}40},
year = {2010}
}
@article{Black1996,
abstract = {The modeling of spatial discontinuities for problems such as surface recovery, segmentation, image reconstruction, and optical flow has been intensely studied in computer vision. While ``line-process'' models of discontinuities have received a great deal of attention, there has been recent interest in the use of robust statistical techniques to account for discontinuities. This paper unifies the two approaches. To achieve this we generalize the notion of a ``line process'' to that of an analog ``outlier process'' and show how a problem formulated in terms of outlier processes can be viewed in terms of robust statistics. We also characterize a class of robust statistical problems for which an equivalent outlier-process formulation exists and give a straightforward method for converting a robust estimation problem into an outlier-process formulation. We show how prior assumptions about the spatial structure of outliers can be expressed as constraints on the recovered analog outlier processes and how traditional continuation methods can be extended to the explicit outlier-process formulation. These results indicate that the outlier-process approach provides a general framework which subsumes the traditional line-process approaches as well as a wide class of robust estimation problems. Examples in surface reconstruction, image segmentation, and optical flow are presented to illustrate the use of outlier processes and to show how the relationship between outlier processes and robust statistics can be exploited. An appendix provides a catalog of common robust error norms and their equivalent outlier-process formulations.},
author = {Black, Michael J. and Rangarajan, Anand},
doi = {10.1007/BF00131148},
issn = {1573-1405},
journal = {ijcv},
month = {jul},
number = {1},
pages = {57--91},
title = {{On the unification of line processes, outlier rejection, and robust statistics with applications in early vision}},
url = {https://doi.org/10.1007/BF00131148},
volume = {19},
year = {1996}
}
@inproceedings{mapillary2017,
author = {Neuhold, Gerhard and Ollmann, Tobias and {Rota Bul{\`{o}}}, Samuel and Kontschieder, Peter},
booktitle = {iccv},
keywords = {dataset,real},
mendeley-tags = {dataset,real},
title = {{The Mapillary Vistas Dataset for Semantic Understanding of Street Scenes}},
url = {https://www.mapillary.com/dataset/vistas},
year = {2017}
}
@article{Barron1,
author = {Barron, Jonathan T. and Malik, Jitendra},
journal = {tpami},
pages = {1670--1687},
title = {{Shape, illumination, and reflectance from shading}},
year = {2015}
}
@inproceedings{Boulch2016,
abstract = {Normal estimation in point clouds is a crucial first step for numerous algorithms, from surface reconstruction and scene un-derstanding to rendering. A recurrent issue when estimating normals is to make appropriate decisions close to sharp features, not to smooth edges, or when the sampling density is not uniform, to prevent bias. Rather than resorting to manually-designed geometric priors, we propose to learn how to make these decisions, using ground-truth data made from synthetic scenes. For this, we project a discretized Hough space representing normal directions onto a structure amenable to deep learning. The resulting normal estimation method outperforms most of the time the state of the art regarding robustness to outliers, to noise and to point density variation, in the presence of sharp edges, while remaining fast, scaling up to millions of points.},
author = {Boulch, Alexandre and Marlet, Renaud},
booktitle = {Eurographics Symposium on Geometry Processing},
doi = {10.1111/cgf.12983},
file = {:home/hale/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Boulch, Marlet - 2016 - Deep Learning for Robust Normal Estimation in Unstructured Point Clouds.pdf:pdf},
number = {5},
title = {{Deep Learning for Robust Normal Estimation in Unstructured Point Clouds}},
volume = {35},
year = {2016}
}
@inproceedings{flyingthings3d2016,
abstract = {Recent work has shown that optical flow estimation can be formulated as a supervised learning task and can be successfully solved with convolutional networks. Training of the so-called FlowNet was enabled by a large synthetically generated dataset. The present paper extends the concept of optical flow estimation via convolutional networks to disparity and scene flow estimation. To this end, we propose three synthetic stereo video datasets with sufficient realism, variation, and size to successfully train large networks. Our datasets are the first large-scale datasets to enable training and evaluating scene flow methods. Besides the datasets, we present a convolutional network for real-time disparity estimation that provides state-of-the-art results. By combining a flow and disparity estimation network and training it jointly, we demonstrate the first scene flow estimation with a convolutional network.},
archivePrefix = {arXiv},
arxivId = {1512.02134},
author = {Mayer, Nikolaus and Ilg, Eddy and H{\"{a}}usser, Philip and Fischer, Philipp and Cremers, Daniel and Dosovitskiy, Alexey and Brox, Thomas},
booktitle = {cvpr},
doi = {10.1109/CVPR.2016.438},
eprint = {1512.02134},
isbn = {978-1-4673-8851-1},
issn = {10636919},
keywords = {convolutional neural,dataset,matching cost,scene flow,similarity learning,stereo,supervised learning,synthetic},
mendeley-tags = {dataset,scene flow,synthetic},
pages = {4040--4048},
title = {{A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation}},
url = {http://arxiv.org/abs/1512.02134},
year = {2016}
}
@inproceedings{Dvoroznak2014,
abstract = {This paper focuses on an existing as-rigid-as-possible deformation model that is particularly suitable for manipulating images that capture articulated objects, for example hand-drawn figures. The model can be used for interactive image deformation as well as automatic image registration. We have implemented both applications as tools for free/open-source image editor GIMP. We describe some details of the implementation and demonstrate functional-ity of these new tools on a variety of images. For image registration we compare the results of the method with results produced by two existing deformable image registration tools NiftyReg and Drop.},
author = {Dvoro{\v{z}}Åˆ{\'{a}}k, Marek},
booktitle = {The 18th Central European Seminal on Computer Graphics},
file = {:home/hale/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dvoro{\v{z}}Åˆ{\'{a}}k - 2014 - Interactive As-Rigid-As-Possible Image Deformation and Registration.pdf:pdf},
keywords = {ARAP,as-,deformation,gimp,graphics,image deformation,image registration,non-dl,rigid-as-possible},
mendeley-tags = {ARAP,deformation,graphics,non-dl},
title = {{Interactive As-Rigid-As-Possible Image Deformation and Registration}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.667.118{\&}rep=rep1{\&}type=pdf},
year = {2014}
}
@article{Guo2015,
abstract = {This article presents a novel approach for 3D mesh labeling by using deep Convolutional Neural Networks (CNNs). Many previous methods on 3D mesh labeling achieve impressive performances by using predefined ge-ometric features. However, the generalization abilities of such low-level features, which are heuristically designed to process specific meshes, are often insufficient to handle all types of meshes. To address this problem, we propose to learn a robust mesh representation that can adapt to various 3D meshes by using CNNs. In our approach, CNNs are first trained in a supervised manner by using a large pool of classical geometric features. In the training process, these low-level features are nonlinearly combined and hierarchically compressed to generate a compact and effective repre-sentation for each triangle on the mesh. Based on the trained CNNs and the mesh representations, a label vector is initialized for each triangle to indicate its probabilities of belonging to various object parts. Eventually, a graph-based mesh-labeling algorithm is adopted to optimize the labels of triangles by considering the label consistencies. Experimental results on several public benchmarks show that the proposed approach is robust for various 3D meshes, and outperforms state-of-the-art approaches as well as classic learning algorithms in recognizing mesh labels.},
author = {Guo, Kan and Zou, Dongqing and Chen, Xiaowu},
doi = {10.1145/2835487},
file = {:home/hale/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Guo, Zou, Chen - 2015 - 3D Mesh Labeling via Deep Convolutional Neural Networks.pdf:pdf},
issn = {0730-0301},
journal = {tog},
keywords = {3D mesh labeling,deep convolutional neural networks,geometry features},
number = {1},
pages = {3:1----3:12},
title = {{3D Mesh Labeling via Deep Convolutional Neural Networks}},
url = {http://doi.acm.org/10.1145/2835487},
volume = {35},
year = {2015}
}
@article{Barron1994,
abstract = {While different optical flow techniques continue to appear, there has been a lack of quantitative evaluation of existing methods. For a common set of real and synthetic image sequences, we report the results of a number of regularly cited optical flow techniques, including instances of differential, matching, energy-based, and phase-based methods. Our comparisons are primarily empirical, and concentrate on the accuracy, reliability, and density of the velocity measurements; they show that performance can differ significantly among the techniques we implemented.},
author = {Barron, J L and Fleet, D J and Beauchemin, S S and Burkitt, T A},
doi = {10.1007/BF01420984},
issn = {1573-1405},
journal = {ijcv},
keywords = {dataset,optical flow},
mendeley-tags = {dataset,optical flow},
month = {feb},
number = {1},
pages = {43--77},
title = {{Performance of optical flow techniques}},
url = {https://doi.org/10.1007/BF01420984},
volume = {12},
year = {1994}
}
@article{Baslamisli2019arxiv,
archivePrefix = {arXiv},
arxivId = {cs.CV/1912.04023},
author = {Baslamisli, Anil S and Das, Partha and Le, Hoang-An and Karaoglu, Sezer and Gevers, Theo},
eprint = {1912.04023},
journal = {ArXiv e-prints},
primaryClass = {cs.CV},
title = {{ShadingNet: Image Intrinsics by Fine-Grained Shading Decomposition}},
year = {2019}
}
@inproceedings{3drms2018,
abstract = {This paper discusses a reconstruction challenge held as a part of the second 3D Reconstruction meets Semantics workshop (3DRMS). The challenge goals and datasets are introduced, including both synthetic and real data from outdoor scenes, here represented by gardens with a variety of bushes, trees, other plants and objects. Both qualitative and quantitative evaluation of the challenge participants' submissions is given in categories of geometric and semantic accuracy. Finally, comparison of submitted results with baseline methods is given, showing a modest performance increase in some of the categories.},
address = {Cham},
author = {Tylecek, Radim and Sattler, Torsten and Le, Hoang-An and Brox, Thomas and Pollefeys, Marc and Fisher, Robert B and Gevers, Theo},
booktitle = {eccvw},
doi = {10.1007/978-3-030-11015-4_48},
editor = {Leal-Taix{\'{e}}, Laura and Roth, Stefan},
file = {:home/hale/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tylecek et al. - 2018 - The Second Workshop on 3D Reconstruction Meets Semantics Challenge Results Discussion.pdf:pdf},
isbn = {978-3-030-11015-4},
keywords = {3D reconstruction,Challenge,Dataset,Semantic segmentation},
pages = {631--644},
publisher = {Springer International Publishing},
title = {{The Second Workshop on 3D Reconstruction Meets Semantics: Challenge Results Discussion}},
url = {https://doi.org/10.1007/978-3-030-11015-4{\_}48},
year = {2019}
}
@inproceedings{Lai2012,
abstract = {We propose a view-based approach for labeling objects in 3D scenes reconstructed from RGB-D (color+depth) videos. We utilize sliding window detectors trained from object views to assign class probabilities to pixels in every RGB-D frame. These probabilities are projected into the reconstructed 3D scene and integrated using a voxel representation. We perform efficient inference on a Markov Random Field over the voxels, combining cues from view-based detection and 3D shape, to label the scene. Our detection-based approach produces accurate scene labeling on the RGB-D Scenes Dataset and improves the robustness of object detection.},
author = {Lai, Kevin and Bo, Liefeng and Ren, Xiaofeng and Fox, Dieter},
booktitle = {icra},
doi = {10.1109/ICRA.2012.6225316},
file = {:home/hale/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lai et al. - 2012 - Detection-based object labeling in 3D scenes.pdf:pdf},
isbn = {9781467314039},
issn = {10504729},
keywords = {3D,RGBD,detection,labelling,object detection,sliding windows},
mendeley-tags = {3D,RGBD,detection,labelling,object detection,sliding windows},
pages = {1330--1337},
title = {{Detection-based object labeling in 3D scenes}},
url = {https://homes.cs.washington.edu/{~}xren/publication/lai-icra12-object-labeling.pdf},
year = {2012}
}
@inproceedings{Shen,
author = {Shen, L and Tan, P and Lin, S},
booktitle = {cvpr},
title = {{Intrinsic image decomposition with non-local texture cues}},
year = {2008}
}
@article{Open3D,
author = {Zhou, Qian-Yi and Park, Jaesik and Koltun, Vladlen},
journal = {arXiv:1801.09847},
title = {{{\{}Open3D{\}}: {\{}A{\}} Modern Library for {\{}3D{\}} Data Processing}},
year = {2018}
}
@article{Ren2015,
abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [7] and Fast R-CNN [5] have reduced the running time of these detection networks, exposing region pro-posal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully-convolutional network that simultaneously predicts object bounds and objectness scores at each position. RPNs are trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. With a simple alternating optimization, RPN and Fast R-CNN can be trained to share convolu-tional features. For the very deep VGG-16 model [18], our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007 (73.2{\%} mAP) and 2012 (70.4{\%} mAP) using 300 proposals per image. The code will be released.},
archivePrefix = {arXiv},
arxivId = {arXiv:1506.01497v1},
author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
doi = {10.1016/j.nima.2015.05.028},
eprint = {arXiv:1506.01497v1},
file = {:home/hale/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ren et al. - 2015 - Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks.pdf:pdf},
journal = {nips},
keywords = {2D,CNN,caffe,deep learning,object detection,realtime,region proposal},
mendeley-tags = {2D,CNN,caffe,deep learning,object detection,realtime,region proposal},
pages = {1--10},
title = {{Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks}},
year = {2015}
}
@article{Shotton2009,
author = {Shotton, J and Winn, J and Rother, C and Criminisi, A},
journal = {ijcv},
pages = {2--23},
title = {{TextonBoost for Image Understanding: Multi-Class Object Recognition and Segmentation by Jointly Modeling Texture, Layout, and Context}},
year = {2009}
}
@inproceedings{Pham,
abstract = {â€” Modern SLAM systems with a depth sensor are able to reliably reconstruct dense 3D geometric maps of indoor scenes. Representing these maps in terms of meaningful entities is a step towards building semantic maps for autonomous robots. One approach is to segment the 3D maps into semantic objects using Conditional Random Fields (CRF), which requires large 3D ground truth datasets to train the classification model. Additionally, the CRF inference is often computationally expensive. In this paper, we present an unsupervised geometricbased approach for the segmentation of 3D point clouds into objects and meaningful scene structures. We approximate an input point cloud by an adjacency graph over surface patches, whose edges are then classified as being either on or off. We devise an effective classifier which utilises both global planar surfaces and local surface convexities for edge classification. More importantly, we propose a novel global plane extraction algorithm for robustly discovering the underlying planes in the scene. Our algorithm is able to enforce the extracted planes to be mutually orthogonal or parallel which conforms usually with humanmade indoor environments. We reconstruct 654 3D indoor scenes from NYUv2 sequences to validate the efficiency and effectiveness of our segmentation method.},
author = {Pham, Trung T and Eich, Markus and Reid, Ian and Wyeth, Gordon},
file = {:home/hale/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pham et al. - Unknown - Geometrically Consistent Plane Extraction for Dense Indoor 3D Maps Segmentation.pdf:pdf},
title = {{Geometrically Consistent Plane Extraction for Dense Indoor 3D Maps Segmentation}}
}
@article{zhou2017places,
author = {Zhou, Bolei and Lapedriza, Agata and Khosla, Aditya and Oliva, Aude and Torralba, Antonio},
journal = {tpami},
publisher = {IEEE},
title = {{Places: A 10 million Image Database for Scene Recognition}},
year = {2017}
}
@inproceedings{Richter2016,
abstract = {Recent progress in computer vision has been driven by high-capacity models trained on large datasets. Unfortunately, creating large datasets with pixel-level labels has been extremely costly due to the amount of human effort required. In this paper, we present an approach to rapidly creating pixel-accurate seman-tic label maps for images extracted from modern computer games. Although the source code and the internal operation of commercial games are inaccessible, we show that associations between image patches can be reconstructed from the communication between the game and the graphics hardware. This enables rapid propagation of semantic labels within and across images synthesized by the game, with no access to the source code or the content. We validate the presented ap-proach by producing dense pixel-level semantic annotations for 25 thousand im-ages synthesized by a photorealistic open-world computer game. Experiments on semantic segmentation datasets show that using the acquired data to supplement real-world images significantly increases accuracy and that the acquired data en-ables reducing the amount of hand-labeled real-world data: models trained with game data and just 1 3 of the CamVid training set outperform models trained on the complete CamVid training set.},
archivePrefix = {arXiv},
arxivId = {arXiv:1608.02192v1},
author = {Richter, Stephan R and Vineet, Vibhav and Roth, Stefan and Koltun, Vladlen},
booktitle = {eccv},
eprint = {arXiv:1608.02192v1},
file = {:home/hale/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Richter et al. - Unknown - Playing for Data Ground Truth from Computer Games.pdf:pdf},
keywords = {city,dataset,driving,large,synthetic,urban},
mendeley-tags = {city,dataset,driving,large,synthetic,urban},
title = {{Playing for Data: Ground Truth from Computer Games}},
year = {2016}
}
@inproceedings{mapillary2020,
author = {Warburg, Frederik and Hauberg, Soren and Lopez-Antequera, Manuel and Gargallo, Pau and Kuang, Yubin and Civera, Javier},
booktitle = {cvpr},
month = {jun},
title = {{Mapillary Street-Level Sequences: A Dataset for Lifelong Place Recognition}},
year = {2020}
}
@article{Land1992,
author = {Land, M F and Fernald, R D},
doi = {10.1146/annurev.ne.15.030192.000245},
journal = {Annual Review of Neuroscience},
month = {mar},
number = {1},
pages = {1--29},
publisher = {Annual Reviews},
title = {{The Evolution of Eyes}},
url = {https://doi.org/10.1146/annurev.ne.15.030192.000245},
volume = {15},
year = {1992}
}
@inproceedings{Zhou2016,
author = {Zhou, Tinghui and Tulsiani, Shubham and Sun, Weilun and Malik, Jitendra and Efros, Alexei A},
booktitle = {eccv},
title = {{View Synthesis by Appearance Flow}},
year = {2016}
}
@incollection{places2014,
abstract = {Scene recognition is one of the hallmark tasks of computer vision, allowing definition of a context for object recognition. Whereas the tremendous recent progress in object recognition tasks is due to the availability of large datasets like ImageNet and the rise of Convolutional Neural Networks (CNNs) for learning high-level features, performance at scene recognition has not attained the same level of success. This may be because current deep features trained from ImageNet are not competitive enough for such tasks. Here, we introduce a new scene-centric database called Places with over 7 million labeled pictures of scenes. We propose new methods to compare the density and diversity of image datasets and show that Places is as dense as other scene datasets and has more diversity. Using CNN, we learn deep features for scene recognition tasks, and establish new state-of-the-art results on several scene-centric datasets. A visualization of the CNN layers' responses allows us to show differences in the internal representations of object-centric and scene-centric networks.},
author = {Zhou, Bolei and Lapedriza, Agata and Xiao, Jianxiong and Torralba, Antonio and Oliva, Aude},
booktitle = {nips},
editor = {Ghahramani, Z and Welling, M and Cortes, C and Lawrence, N D and Weinberger, K Q},
file = {:home/hale/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhou et al. - 2014 - Learning Deep Features for Scene Recognition using Places Database(2).pdf:pdf},
issn = {10495258},
keywords = {CNN,places database,scene reccognition},
mendeley-tags = {CNN,places database,scene reccognition},
pages = {487--495},
publisher = {Curran Associates, Inc.},
title = {{Learning Deep Features for Scene Recognition using Places Database}},
url = {http://papers.nips.cc/paper/5349-learning-deep-features-for-scene-recognition-using-places-database.pdf},
year = {2014}
}
@inproceedings{Shen2,
author = {Shen, L and Yeo, C},
booktitle = {cvpr},
title = {{Intrinsic images decomposition using a local and global sparse representation of reflectance}},
year = {2011}
}
@inproceedings{Dosovitskiy2015,
abstract = {Convolutional neural networks (CNNs) have recently been very successful in a variety of computer vision tasks, especially on those linked to recognition. Optical flow estimation has not been among the tasks where CNNs were successful. In this paper we construct appropriate CNNs which are capable of solving the optical flow estimation problem as a supervised learning task. We propose and compare two architectures: a generic architecture and another one including a layer that correlates feature vectors at different image locations. Since existing ground truth data sets are not sufficiently large to train a CNN, we generate a synthetic Flying Chairs dataset. We show that networks trained on this unrealistic data still generalize very well to existing datasets such as Sintel and KITTI, achieving competitive accuracy at frame rates of 5 to 10 fps.},
archivePrefix = {arXiv},
arxivId = {1504.06852},
author = {Dosovitskiy, Alexey and Fischer, P and Ilg, E and H{\"{a}}usser, P and Hazirbacs, C and Golkov, V and v.d. Smagt, P and Cremers, D and Brox, T},
booktitle = {iccv},
doi = {10.1109/ICCV.2015.316},
eprint = {1504.06852},
isbn = {9781467383912},
issn = {15505499},
keywords = {optical flow},
mendeley-tags = {optical flow},
pages = {2758--2766},
title = {{FlowNet: Learning optical flow with convolutional networks}},
url = {http://lmb.informatik.uni-freiburg.de/Publications/2015/DFIB15 https://arxiv.org/pdf/1504.06852.pdf},
volume = {11-18},
year = {2015}
}
@inproceedings{Garg2016,
author = {Garg, Ravi and Kumar, B G Vijay and Carneiro, Gustavo and Reid, Ian},
booktitle = {eccv},
keywords = {depth prediciton,photometric loss,self-supervised,unsupervised},
mendeley-tags = {depth prediciton,photometric loss,self-supervised,unsupervised},
organization = {Springer},
pages = {740--756},
title = {{Unsupervised CNN for single view depth estimation: Geometry to the rescue}},
year = {2016}
}
@article{Shafer1985,
author = {Shafer, S},
journal = {Color research and applications},
pages = {210--218},
title = {{Using color to separate reflection components}},
year = {1985}
}
@inproceedings{yang_cvpr17,
abstract = {We propose a novel superpixel-based multi-view convolutional neural network for semantic image segmentation. The proposed network produces a high quality segmentation of a single image by leveraging information from additional views of the same scene. Particularly in indoor videos such as captured by robotic platforms or handheld and bodyworn RGBD cameras, nearby video frames provide diverse viewpoints and additional context of objects and scenes. To leverage such information, we first compute region correspondences by optical flow and image boundary-based superpixels. Given these region correspondences, we propose a novel spatio-temporal pooling layer to aggregate information over space and time. We evaluate our approach on the NYU--Depth--V2 and the SUN3D datasets and compare it to various state-of-the-art single-view and multi-view approaches. Besides a general improvement over the state-of-the-art, we also show the benefits of making use of unlabeled frames during training for multi-view as well as single-view prediction.},
archivePrefix = {arXiv},
arxivId = {1604.02388},
author = {He, Yang and Chiu, Wei-Chen and Keuper, Margret and Fritz, Mario},
booktitle = {cvpr},
doi = {10.1109/CVPR.2017.757},
eprint = {1604.02388},
title = {{STD2P: RGBD Semantic Segmentation Using Spatio-Temporal Data-Driven Pooling}},
url = {http://arxiv.org/abs/1604.02388 https://arxiv.org/pdf/1604.02388.pdf},
year = {2016}
}
@inproceedings{Zhou2016_FGR,
abstract = {We present an algorithm for fast global registration of partially over-lapping 3D surfaces. The algorithm operates on candidate matches that cover the surfaces. A single objective is optimized to align the surfaces and disable false matches. The objective is defined densely over the surfaces and the optimiza-tion achieves tight alignment with no initialization. No correspondence updates or closest-point queries are performed in the inner loop. An extension of the al-gorithm can perform joint global registration of many partially overlapping sur-faces. Extensive experiments demonstrate that the presented approach matches or exceeds the accuracy of state-of-the-art global registration pipelines, while be-ing at least an order of magnitude faster. Remarkably, the presented approach is also faster than local refinement algorithms such as ICP. It provides the accu-racy achieved by well-initialized local refinement algorithms, without requiring an initialization and at lower computational cost.},
archivePrefix = {arXiv},
arxivId = {1311.2901},
author = {Zhou, Qian-Yi and Park, Jaesik and Koltun, Vladlen},
booktitle = {eccv},
doi = {10.1007/978-3-319-46448-0},
eprint = {1311.2901},
isbn = {978-3-319-46447-3},
issn = {0302-9743},
title = {{Fast Global Registration}},
year = {2016}
}
@article{Brostow2009,
address = {New York, NY, USA},
author = {Brostow, Gabriel J and Fauqueur, Julien and Cipolla, Roberto},
doi = {10.1016/j.patrec.2008.04.005},
issn = {0167-8655},
journal = {Pattern Recognition Letter},
keywords = {42.30.Sy,42.30.Tz,87.57.N-,87.57.nm,Label propagation,Object recognition,Semantic segmentation,Video database,Video understanding},
month = {jan},
number = {2},
pages = {88--97},
publisher = {Elsevier Science Inc.},
title = {{Semantic Object Classes in Video: A High-definition Ground Truth Database}},
url = {http://dx.doi.org/10.1016/j.patrec.2008.04.005},
volume = {30},
year = {2009}
}
@inproceedings{Chen2016a,
abstract = {Deep convolutional neural networks (CNNs) are the backbone of state-of-art semantic image segmentation systems. Recent work has shown that complementing CNNs with fully-connected conditional random fields (CRFs) can significantly enhance their object localization accuracy, yet dense CRF inference is computationally expensive. We propose replacing the fully-connected CRF with domain transform (DT), a modern edge-preserving filtering method in which the amount of smoothing is controlled by a reference edge map. Domain transform filtering is several times faster than dense CRF inference and we show that it yields comparable semantic segmentation results, accurately capturing object boundaries. Importantly, our formulation allows learning the reference edge map from intermediate CNN features instead of using the image gradient magnitude as in standard DT filtering. This produces task-specific edges in an end-to-end trainable system optimizing the target semantic segmentation quality.},
archivePrefix = {arXiv},
arxivId = {1511.03328},
author = {Chen, Liang-Chieh and Barron, Jonathan T and Papandreou, George and Murphy, Kevin and Yuille, Alan L},
booktitle = {cvpr},
doi = {10.1109/CVPR.2016.492},
eprint = {1511.03328},
file = {:home/hale/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen et al. - 2016 - Semantic Image Segmentation with Task-Specific Edge Detection Using CNNs and a Discriminatively Trained Domain Tran.pdf:pdf},
keywords = {cnn,semantic segmentation},
mendeley-tags = {cnn,semantic segmentation},
pages = {12},
title = {{Semantic Image Segmentation with Task-Specific Edge Detection Using CNNs and a Discriminatively Trained Domain Transform}},
url = {http://arxiv.org/abs/1511.03328},
year = {2016}
}
@article{Hoiem2007,
address = {Hingham, MA, USA},
author = {Hoiem, Derek and Efros, Alexei A and Hebert, Martial},
doi = {10.1007/s11263-006-0031-y},
issn = {0920-5691},
journal = {ijcv},
keywords = {context,geometric context,image understanding,model-driven segmentation,multiple segmentations,object detection,object recognition,scene understanding,spatial layout,surface layout},
month = {oct},
number = {1},
pages = {151--172},
publisher = {Kluwer Academic Publishers},
title = {{Recovering Surface Layout from an Image}},
url = {http://dx.doi.org/10.1007/s11263-006-0031-y},
volume = {75},
year = {2007}
}
@article{Heeger87,
abstract = {A model is presented, consonant with current views regarding the neurophysiology and psychophysics of motion perception, that combines the outputs of a set of spatiotemporal motion-energy filters to extract optical flow. The output velocity is encoded as the peak in a distribution of velocity-tuned units that behave much like cells of the middle temporal area of the primate brain. The model appears to deal with the aperture problem as well as the human visual system since it extracts the correct velocity for patterns that have large differences in contrast at different spatial orientations, and it simulates psychophysical data on the coherence of sine-grating plaid patterns.},
author = {Heeger, David J},
doi = {10.1364/JOSAA.4.001455},
journal = {J. Opt. Soc. Am. A},
keywords = {Eye movements,Optical flow,Photoreceptors,Power spectra,Spatial frequency,Visual system,dataset,optical flow},
mendeley-tags = {dataset,optical flow},
month = {aug},
number = {8},
pages = {1455--1471},
publisher = {OSA},
title = {{Model for the extraction of image flow}},
url = {http://josaa.osa.org/abstract.cfm?URI=josaa-4-8-1455},
volume = {4},
year = {1987}
}
@article{Rematas2017,
author = {Rematas, K and Nguyen, C H and Ritschel, T and Fritz, M and Tuytelaars, T},
doi = {10.1109/TPAMI.2016.2601093},
journal = {tpami},
keywords = {2D to 3D alignment,2D-to-3D alignment method,3D model geometry,Automobiles,Geometry,Novel view synthesis,Rendering (computer graphics),Shape,Solid modeling,Three-dimensional displays,Two dimensional displays,correlations,disocclusion,feature extraction,image appearance,image based rendering,image instances,image matching,image object matching,novel-view synthesis,object detection,object detectors,rendering (computer graphics),solid modelling,structural information extraction},
month = {aug},
number = {8},
pages = {1576--1590},
title = {{Novel Views of Objects from a Single Image}},
volume = {39},
year = {2017}
}
@inproceedings{Janai2018ECCV,
author = {Janai, Joel and G{\"{u}}ney, Fatma and Ranjan, Anurag and Black, Michael J. and Geiger, Andreas},
booktitle = {eccv},
month = {sep},
pages = {713--731},
publisher = {Springer, Cham},
title = {{Unsupervised Learning of Multi-Frame Optical Flow with Occlusions}},
volume = {Lecture No},
year = {2018}
}
@article{Carroll,
author = {Carroll, R and Ramamoorthi, R and Agrawala, M},
journal = {tog},
title = {{Illumination decomposition for material recoloring with consistent interreflections}},
year = {2011}
}
@phdthesis{Hewitt2017,
author = {Hewitt, Charlie},
school = {Cambridge Trinity College},
title = {{Procedural Generation of Tree Models for Use in Computer Graphics}},
year = {2017}
}
@inproceedings{Menze2015a,
abstract = {Three-dimensional reconstruction of dynamic scenes is an important prerequisite for applications like mobile robotics or autonomous driving. While much progress has been made in recent years, imaging conditions in natural outdoor environments are still very challenging for current reconstruction and recognition methods. In this paper, we propose a novel unified approach which reasons jointly about 3D scene flow as well as the pose, shape and motion of vehicles in the scene. Towards this goal, we incorporate a deformable CAD model into a slanted-plane conditional random field for scene flow estimation and enforce shape consistency between the rendered 3D models and the parameters of all superpixels in the image. The association of superpixels to objects is established by an index variable which implicitly enables model selection. We evaluate our approach on the challenging KITTI scene flow dataset in terms of object and scene flow estimation. Our results provide a prove of concept and demonstrate the usefulness of our method.},
author = {Menze, Moritz and Heipke, Christian and Geiger, Andreas},
booktitle = {Proc. ISPRS Workshop on Image Sequence Analysis},
keywords = {3D reconstruction,active shape model,motion estimation,object detection,scene flow},
title = {{Joint 3D Estimation of Vehicles and Scene Flow}},
url = {http://www.cvlibs.net/publications/Menze2015ISA.pdf},
year = {2015}
}
@article{iiw2014,
author = {Bell, S and Bala, K and Snavely, N},
journal = {tog},
title = {{Intrinsic images in the wild}},
year = {2014}
}
@inproceedings{Meister2018,
address = {New Orleans, Louisiana},
author = {Meister, Simon and Hur, Junhwa and Roth, Stefan},
booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
month = {feb},
title = {{UnFlow: Unsupervised Learning of Optical Flow with a Bidirectional Census Loss}},
year = {2018}
}
@incollection{Skocaj2012,
address = {Boston, MA},
author = {Skocaj, Danijel and Leonardis, Ales and Kruijff, Geert-Jan M},
booktitle = {Encyclopedia of the Sciences of Learning},
doi = {10.1007/978-1-4419-1428-6_239},
editor = {Seel, Norbert M},
isbn = {978-1-4419-1428-6},
keywords = {background,modalityQ},
mendeley-tags = {background,modalityQ},
pages = {861--864},
publisher = {Springer US},
title = {{Cross-Modal Learning}},
url = {https://doi.org/10.1007/978-1-4419-1428-6{\_}239},
year = {2012}
}
@inproceedings{hu2016,
author = {Hu, Yinlin and Song, Rui and Li, Yunsong},
booktitle = {cvpr},
pages = {5704--5712},
title = {{Efficient coarse-to-fine patchmatch for large displacement optical flow}},
year = {2016}
}
@article{wang2008,
address = {Chichester, UK},
author = {Wang, Yanzhen and Xu, Kai and Xiong, Yueshan and Cheng, Zhi-Quan},
doi = {10.1002/cav.v19:3/4},
file = {:home/hale/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang et al. - 2008 - 2D Shape Deformation Based on Rigid Square Matching.pdf:pdf},
issn = {1546-4261},
journal = {Comput. Animat. Virtual Worlds},
keywords = {character animation,deformation,rigid transformation,shape deformation,shape matching,skeletal deformation},
mendeley-tags = {deformation},
month = {sep},
number = {3-4},
pages = {411--420},
publisher = {John Wiley and Sons Ltd.},
title = {{2D Shape Deformation Based on Rigid Square Matching}},
url = {http://dx.doi.org/10.1002/cav.v19:3/4},
volume = {19},
year = {2008}
}
@article{Girshick2014,
abstract = {â€”Object detection performance, as measured on the canonical PASCAL VOC Challenge datasets, plateaued in the final years of the competition. The best-performing methods were complex ensemble systems that typically combined multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 50{\%} relative to the previous best result on VOC 2012â€”achieving a mAP of 62.4{\%}. Our approach combines two ideas: (1) one can apply high-capacity convolutional networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data are scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, boosts performance significantly. Since we combine region proposals with CNNs, we call the resulting model an R-CNN or Region-based Convolutional Network. Source code for the complete system is available at},
archivePrefix = {arXiv},
arxivId = {1311.2524},
author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
doi = {10.1109/TPAMI.2015.2437384},
eprint = {1311.2524},
file = {:home/hale/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Girshick et al. - 2014 - Region-based Convolutional Networks for Accurate Object Detection and Segmentation.pdf:pdf},
isbn = {978-1-4799-5118-5},
issn = {0162-8828},
journal = {tpami},
keywords = {CNN,Convolutional Networks,Deep Learning,Detection,Index Termsâ€”Object Recognition,RCNN,Semantic Segmentation,Transfer Learning !},
mendeley-tags = {CNN,RCNN},
number = {1},
pages = {1--16},
pmid = {26656583},
title = {{Region-based Convolutional Networks for Accurate Object Detection and Segmentation}},
url = {http://www.cs.berkeley.edu/},
volume = {38},
year = {2014}
}
@article{Fortun2015,
abstract = {L'archive ouverte pluridisciplinaire HAL, est destin{\'{e}}e au d{\'{e}}p{\^{o}}t et {\`{a}} la diffusion de documents scientifiques de niveau recherche, publi{\'{e}}s ou non, {\'{e}}manant des {\'{e}}tablissements d'enseignement et de recherche fran{\c{c}}ais ou {\'{e}}trangers, des laboratoires publics ou priv{\'{e}}s. Abstract Optical flow estimation is one of the oldest and still most active research domains in computer vision. In 35 years, many methodological concepts have been introduced and have progressively improved performances, while opening the way to new challenges. In the last decade, the growing interest in evaluation benchmarks has stimulated a great amount of work. In this paper, we propose a survey of optical flow estimation classifying the main principles elaborated during this evolution, with a particular concern given to recent developments. It is conceived as a tutorial organizing in a comprehensive framework current approaches and practices. We give insights on the motivations, interests and limitations of modeling and optimization techniques, and we highlight similarities between methods to allow for a clear understanding of their behavior.},
author = {Fortun, Denis and Bouthemy, Patrick and Kervrann, Charles and Fortun, Denis and Bouthemy, Patrick and Kervrann, Charles},
journal = {Computer Vision and Image Understanding (CVIU)},
keywords = {feature matching,motion estimation,occlusions,optical flow,optimization,parametric models,regularization,survey},
mendeley-tags = {optical flow,survey},
pages = {1--21},
publisher = {Elsevier},
title = {{Optical flow modeling and computation : a survey}},
url = {https://hal.inria.fr/hal-01104081/file/CVIU{\_}survey.pdf},
volume = {134},
year = {2015}
}
@inproceedings{Flynn2016,
author = {Flynn, J and Neulander, I and Philbin, J and Snavely, N},
booktitle = {cvpr},
pages = {5515--5524},
title = {{Deep Stereo: Learning to Predict New Views from the World's Imagery}},
year = {2016}
}
@inproceedings{narihia1,
author = {Narihira, T and Maire, M and Yu, S {\~{}}X.},
booktitle = {iccv},
title = {{Direct Intrinsics: Learning Albedo-Shading Decomposition by Convolutional Regression}},
year = {2015}
}
@inproceedings{Yamaguchi_cvp2013,
abstract = {We consider the problem of computing optical flow in monocular video taken from a moving vehicle. In this setting, the vast majority of image flow is due to the vehicle's ego-motion. We propose to take advantage of this fact and estimate flow along the epipolar lines of the egomotion. Towards this goal, we derive a slanted-plane MRF model which explicitly reasons about the ordering of planes and their physical validity at junctions. Furthermore, we present a bottom-up grouping algorithm which produces over-segmentations that respect flow boundaries. We demonstrate the effectiveness of our approach in the challenging KITTI flow benchmark [11] achieving half the error of the best competing general flow algorithm and one third of the error of the best epipolar flow algorithm.},
address = {Washington, DC, USA},
author = {Yamaguchi, Koichiro and McAllester, David and Urtasun, Raquel},
booktitle = {cvpr},
doi = {10.1109/CVPR.2013.243},
isbn = {978-0-7695-4989-7},
issn = {10636919},
keywords = {Autonomous driving,Optical flow},
pages = {1862--1869},
publisher = {IEEE Computer Society},
series = {CVPR '13},
title = {{Robust monocular epipolar flow estimation}},
url = {http://dx.doi.org/10.1109/CVPR.2013.243},
year = {2013}
}
@inproceedings{Eigen2014,
address = {Cambridge, MA, USA},
author = {Eigen, David and Puhrsch, Christian and Fergus, Rob},
booktitle = {nips},
pages = {2366--2374},
publisher = {MIT Press},
series = {NIPS'14},
title = {{Depth Map Prediction from a Single Image Using a Multi-scale Deep Network}},
url = {http://dl.acm.org/citation.cfm?id=2969033.2969091},
year = {2014}
}
@inproceedings{farenzena,
author = {Farenzena, M and Fusiello, A},
booktitle = {Proceedings of the IEEE International Conference on Image Processing (ICIP)},
title = {{Recovering Intrinsic Images using an Illumination Invariant Image}},
year = {2007}
}
@article{Leyva2019,
author = {Leyva-Vallina, M and Strisciuglio, N and {L{\'{o}}pez Antequera}, M and Tylecek, R and Blaich, M and Petkov, N},
journal = {IEEE Access},
keywords = {TB,dataset,garden,real},
mendeley-tags = {TB,dataset,garden,real},
pages = {52277--52287},
title = {{TB-Places: A Data Set for Visual Place Recognition in Garden Environments}},
volume = {7},
year = {2019}
}
@article{Barth2018,
annote = {en ook TKI-nummer: EU-2015-03, 1409-035 EU.},
author = {Barth, R and IJsselmuiden, J and Hemming, J and van Henten, E J},
doi = {10.1016/j.compag.2017.12.001},
issn = {0168-1699},
journal = {Computers and Electronics in Agriculture},
keywords = {3D modelling,Agriculture,Robotics,Semantic segmentation,Synthetic dataset},
pages = {284--296},
publisher = {Elsevier},
title = {{Data synthesis methods for semantic segmentation in agriculture: A Capsicum annuum dataset}},
volume = {144},
year = {2018}
}
@article{Bruhn2005,
author = {Bruhn, Andr{\'{e}}s and Weickert, Joachim and Schn{\"{o}}rr, Christoph},
journal = {ijcv},
number = {3},
pages = {211--231},
publisher = {Springer},
title = {{Lucas/Kanade meets Horn/Schunck: Combining local and global optic flow methods}},
volume = {61},
year = {2005}
}
@inproceedings{Han2016,
abstract = {Video object detection is challenging because objects that are easily detected in one frame may be difficult to detect in another frame within the same clip. Recently, there have been major advances for doing object detection in a single image. These methods typically contain three phases: (i) object proposal generation (ii) object classification and (iii) post-processing. We propose a modification of the post-processing phase that uses high-scoring object detections from nearby frames to boost scores of weaker detections within the same clip. We show that our method obtains superior results to state-of-the-art single image object detection techniques. Our method placed 3rd in the video object detection (VID) task of the ImageNet Large Scale Visual Recognition Challenge 2015 (ILSVRC2015).},
archivePrefix = {arXiv},
arxivId = {1602.08465},
author = {Han, Wei and Khorrami, Pooya and Paine, Tom Le and Ramachandran, Prajit and Babaeizadeh, Mohammad and Shi, Honghui and Li, Jianan and Yan, Shuicheng and Huang, Thomas S.},
booktitle = {arXiv preprint arXiv:1602.08465},
eprint = {1602.08465},
pages = {1--9},
title = {{Seq-NMS for Video Object Detection}},
url = {http://arxiv.org/abs/1602.08465},
year = {2016}
}
@inproceedings{Zhou2018stereo,
author = {Zhou, Tinghui and Tucker, Richard and Flynn, John and Fyffe, Graham and Snavely, Noah},
booktitle = {tog},
title = {{Stereo Magnification: Learning View Synthesis using Multiplane Images}},
year = {2018}
}
@misc{Blender,
address = {Blender Institute, Amsterdam},
author = {{Blender Foundation}},
institution = {Blender Foundation},
publisher = {Blender Institute, Amsterdam},
title = {{Blender - a 3D modelling and rendering package, release 2.78c}},
url = {http://www.blender.org},
year = {2017}
}
@inproceedings{Isola2017,
abstract = {We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Moreover, since the release of the pix2pix software associated with this paper, hundreds of twitter users have posted their own artistic experiments using our system. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.},
archivePrefix = {arXiv},
arxivId = {1611.07004v3},
author = {Isola, Phillip and Zhu, Jun-Yan Yan and Zhou, Tinghui and Efros, Alexei A and Research, Berkeley Ai},
booktitle = {cvpr},
doi = {10.1109/CVPR.2017.632},
eprint = {1611.07004v3},
file = {:home/hale/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Isola et al. - 2017 - Image-to-image translation with conditional adversarial networks.pdf:pdf},
isbn = {9781538604571},
keywords = {conditional,gan,pix2pix},
mendeley-tags = {conditional,gan,pix2pix},
pages = {5967--5976},
title = {{Image-to-Image Translation with Conditional Adversarial Networks}},
url = {https://github.com/phillipi/pix2pix.},
volume = {2017-Janua},
year = {2017}
}
@article{Uijlings2013,
abstract = {For object recognition, the current state-of-the-art is based on exhaustive search. However, to enable the use of more expensive features and classifiers and thereby progress beyond the state-of-the-art, a selective search strategy is needed. Therefore, we adapt segmentation as a selective search by reconsidering segmentation: We propose to generate many approximate locations over few and precise object delineations because (1) an object whose location is never generated can not be recognised and (2) appearance and immediate nearby context are most effective for object recognition. Our method is class-independent and is shown to cover 96.7{\%} of all objects in the Pascal VOC 2007 test set using only 1,536 locations per image. Our selective search enables the use of the more expensive bag-of-words method which we use to substantially improve the state-of-the-art by up to 8.5{\%} for 8 out of 20 classes on the Pascal VOC 2010 detection challenge.},
archivePrefix = {arXiv},
arxivId = {1409.4842},
author = {Uijlings, J. R R and {Van De Sande}, K. E A and Gevers, T. and Smeulders, A. W M},
doi = {10.1007/s11263-013-0620-5},
eprint = {1409.4842},
file = {:home/hale/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Uijlings et al. - 2013 - Selective search for object recognition.pdf:pdf},
isbn = {9781457711015},
issn = {09205691},
journal = {ijcv},
number = {2},
pages = {154--171},
title = {{Selective search for object recognition}},
volume = {104},
year = {2013}
}
@inproceedings{Barron2013,
abstract = {In this paper we extend the " shape, illumination and re-flectance from shading " (SIRFS) model [3, 4], which recov-ers intrinsic scene properties from a single image. Though SIRFS performs well on images of segmented objects, it per-forms poorly on images of natural scenes, which contain occlusion and spatially-varying illumination. We therefore present Scene-SIRFS, a generalization of SIRFS in which we have a mixture of shapes and a mixture of illuminations, and those mixture components are embedded in a " soft " segmentation of the input image. We additionally use the noisy depth maps provided by RGB-D sensors (in this case, the Kinect) to improve shape estimation. Our model takes as input a single RGB-D image and produces as output an improved depth map, a set of surface normals, a reflectance image, a shading image, and a spatially varying model of il-lumination. The output of our model can be used for graph-ics applications, or for any application involving RGB-D images.},
author = {Barron, Jonathan T. and Malik, Jitendra},
booktitle = {cvpr},
doi = {10.1109/CVPR.2013.10},
keywords = {depth,intrinsic,surface normal},
mendeley-tags = {depth,intrinsic,surface normal},
pages = {17--24},
title = {{Intrinsic scene properties from a single RGB-D image}},
url = {file:///home/hale/Downloads/06618854.pdf},
volume = {2},
year = {2013}
}
@inproceedings{Godard2017,
author = {Godard, Cl{\'{e}}ment and {Mac Aodha}, Oisin and Brostow, Gabriel J},
booktitle = {cvpr},
title = {{Unsupervised Monocular Depth Estimation with Left-Right Consistency}},
year = {2017}
}
@article{Ye,
author = {Ye, G and Garces, E and Liu, Y and Dai, Q and Gutierrez, D},
journal = {tog},
title = {{Intrinsic video and applications}},
year = {2014}
}
@inproceedings{demon,
author = {Ummenhofer, B and Zhou, H and Uhrig, J and Mayer, N and Ilg, E and Dosovitskiy, A and Brox, T},
booktitle = {cvpr},
title = {{DeMoN: Depth and Motion Network for Learning Monocular Stereo}},
year = {2017}
}
@article{Beauchemin1995,
address = {New York, NY, USA},
author = {Beauchemin, S S and Barron, J L},
doi = {10.1145/212094.212141},
issn = {0360-0300},
journal = {ACM Computing Surveys},
month = {sep},
number = {3},
pages = {433--466},
publisher = {ACM},
title = {{The Computation of Optical Flow}},
url = {http://doi.acm.org/10.1145/212094.212141},
volume = {27},
year = {1995}
}
@article{Hirose2019,
author = {Hirose, N and Sadeghian, A and Xia, F and Mart{\'{i}}n-Mart{\'{i}}n, R and Savarese, S},
doi = {10.1109/LRA.2019.2894869},
journal = {IEEE Robotics and Automation Letters},
keywords = {Cameras,Dynamics,Estimation,GONet,Navigation,RGB camera,RGB images,Robot safety,Robot vision systems,VUNet,accurate future images,cameras,collision avoidance,computer vision for other robotic applications,current time steps,dynamic environments,dynamic obstacles,dynamic scene view synthesis,future traversability,image changes,image colour analysis,intelligent robots,learning (artificial intelligence),mobile robots,novel view synthesis method,path planning,presented method GONet,previous time steps,robot vision,semisupervised deep learning approach,telerobotics,traversable areas,view synthesis-based traversability estimation met,virtual images,virtual robot velocity},
month = {apr},
number = {2},
pages = {2062--2069},
title = {{VUNet: Dynamic Scene View Synthesis for Traversability Estimation Using an RGB Camera}},
volume = {4},
year = {2019}
}
@inproceedings{Newcombe2015,
abstract = {Figure 1: Real-time reconstructions of a moving scene with DynamicFusion; both the person and the camera are moving. The initially noisy and incomplete model is progressively denoised and completed over time (left to right). Abstract We present the first dense SLAM system capable of re-constructing non-rigidly deforming scenes in real-time, by fusing together RGBD scans captured from commodity sen-sors. Our DynamicFusion approach reconstructs scene ge-ometry whilst simultaneously estimating a dense volumet-ric 6D motion field that warps the estimated geometry into a live frame. Like KinectFusion, our system produces in-creasingly denoised, detailed, and complete reconstructions as more measurements are fused, and displays the updated model in real time. Because we do not require a template or other prior scene model, the approach is applicable to a wide range of moving objects and scenes. 3D scanning traditionally involves separate capture and off-line processing phases, requiring very careful planning of the capture to make sure that every surface is cov-ered. In practice, it's very difficult to avoid holes, requir-ing several iterations of capture, reconstruction, identifying holes, and recapturing missing regions to ensure a complete model. Real-time 3D reconstruction systems like KinectFu-sion [18, 10] represent a major advance, by providing users the ability to instantly see the reconstruction and identify regions that remain to be scanned. KinectFusion spurred a flurry of follow up research aimed at robustifying the track-ing [9, 32] and expanding its spatial mapping capabilities to larger environments [22, 19, 34, 31, 9]. However, as with all traditional SLAM and dense re-construction systems, the most basic assumption behind KinectFusion is that the observed scene is largely static. The core question we tackle in this paper is: How can we generalise KinectFusion to reconstruct and track dynamic,},
author = {Newcombe, Richard a and Fox, Dieter and Seitz, Steven M},
booktitle = {cvpr},
doi = {10.1109/CVPR.2015.7298631},
isbn = {9781467369640},
issn = {10636919},
pages = {343--352},
title = {{DynamicFusion: Reconstruction and Tracking of Non-rigid Scenes in Real-Time}},
year = {2015}
}
@inproceedings{Janai2017,
abstract = {Existing optical flow datasets are limited in size and variability due to the difficulty of capturing dense ground truth. In this paper, we tackle this problem by tracking pix-els through densely sampled space-time volumes recorded with a high-speed video camera. Our model exploits the lin-earity of small motions and reasons about occlusions from multiple frames. Using our technique, we are able to es-tablish accurate reference flow fields outside the laboratory in natural environments. Besides, we show how our predic-tions can be used to augment the input images with realistic motion blur. We demonstrate the quality of the produced flow fields on synthetic and real-world datasets. Finally, we collect a novel challenging optical flow dataset by applying our technique on data from a high-speed camera and ana-lyze the performance of the state-of-the-art in optical flow under various levels of motion blur.},
author = {Janai, Joel and G{\"{u}}ney, Fatma and Wulff, Jonas and Black, Michael J. and Geiger, Andreas},
booktitle = {cvpr},
doi = {10.1109/CVPR.2017.154},
isbn = {9781538604571},
pages = {1406--1416},
title = {{Slow flow: Exploiting high-speed cameras for accurate and diverse optical flow reference data}},
url = {http://www.cvlibs.net/publications/Janai2017CVPR.pdf},
volume = {2017-Janua},
year = {2017}
}
@inproceedings{freiburgforest2016,
author = {Valada, Abhinav and Oliveira, Gabriel and Brox, Thomas and Burgard, Wolfram},
booktitle = {International Symposium on Experimental Robotics (ISER)},
title = {{Deep Multispectral Semantic Scene Understanding of Forested Environments using Multimodal Fusion}},
year = {2016}
}
@inproceedings{Xu2019_ICCV,
author = {Xu, Xiaogang and Chen, Ying-Cong and Jia, Jiaya},
booktitle = {iccv},
month = {oct},
title = {{View Independent Generative Adversarial Network for Novel View Synthesis}},
year = {2019}
}
@inproceedings{Pepik2012,
author = {Pepik, B and Stark, M and Gehler, P and Schiele, B},
booktitle = {cvpr},
keywords = {dataset},
mendeley-tags = {dataset},
pages = {3362--3369},
title = {{Teaching 3D geometry to deformable part models}},
year = {2012}
}
@article{Kendall2015a,
abstract = {We present a robust and real-time monocular six degree of freedom relocalization system. Our system trains a convolutional neural network to regress the 6-DOF camera pose from a single RGB image in an end-to-end manner with no need of additional engineering or graph optimisation. The algorithm can operate indoors and outdoors in real time, taking 5ms per frame to compute. It obtains approximately 2m and 3 degree accuracy for large scale outdoor scenes and 0.5m and 5 degree accuracy indoors. This is achieved using an efficient 23 layer deep convnet, demonstrating that convnets can be used to solve complicated out of image plane regression problems. This was made possible by leveraging transfer learning from large scale classification data. We show the convnet localizes from high level features and is robust to difficult lighting, motion blur and different camera intrinsics where point based SIFT registration fails. Furthermore we show how the pose feature that is produced generalizes to other scenes allowing us to regress pose with only a few dozen training examples.},
archivePrefix = {arXiv},
arxivId = {1505.07427},
author = {Kendall, Alex and Grimes, Matthew and Cipolla, Roberto},
doi = {10.1109/ICCV.2015.336},
eprint = {1505.07427},
file = {:home/hale/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kendall, Grimes, Cipolla - 2015 - PoseNet A Convolutional Network for Real-Time 6-DOF Camera Relocalization.pdf:pdf},
isbn = {978-1-4673-8391-2},
issn = {978-1-4673-8391-2},
journal = {iccv},
keywords = {localization},
mendeley-tags = {localization},
pages = {9},
title = {{PoseNet: A Convolutional Network for Real-Time 6-DOF Camera Relocalization}},
url = {http://arxiv.org/abs/1505.07427},
year = {2015}
}
@inproceedings{White,
author = {White, R. and Forsyth, D.-A.},
booktitle = {eccv},
title = {{Retexturing Single Views Using Texture and Shading}},
year = {2006}
}
@inproceedings{Debevec1996,
address = {New York, NY, USA},
author = {Debevec, Paul E and Taylor, Camillo J and Malik, Jitendra},
booktitle = {Proceedings of the Annual Conference on Computer Graphics and Interactive Techniques},
doi = {10.1145/237170.237191},
isbn = {0897917464},
pages = {11--20},
publisher = {Association for Computing Machinery},
series = {SIGGRAPH '96},
title = {{Modeling and Rendering Architecture from Photographs: A Hybrid Geometry- and Image-Based Approach}},
url = {https://doi.org/10.1145/237170.237191},
year = {1996}
}
@article{Liu,
author = {Liu, X and Wan, L and Qu, Y and Wong, T. T. and Lin, S and Leung, C. S. and Heng, P. A.},
journal = {tog},
title = {{Intrinsic colorization}},
year = {2008}
}
@book{Palmer1999,
address = {Cambridge, Mass.},
annote = {Stephen E. Palmer.; "A Bradford book."; Bibliography: Includes bibliographical references (p. [737]-769) and indexes.},
author = {Palmer, Stephen E},
isbn = {0262161834 Thanks for using Barton, the MIT Libraries' catalog http},
keywords = {Cognitive Vision,Qrcognitive,Visual perception,background,kk,perception,science,vision},
mendeley-tags = {Qrcognitive,background,kk,perception,vision},
pages = {810},
publisher = {MIT Press},
title = {{Vision science : photons to phenomenology}},
year = {1999}
}
@inproceedings{msra,
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
booktitle = {iccv},
title = {{Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification}},
year = {2015}
}
@article{Barrow,
author = {Barrow, Harry G. and Tenenbaum, Joan M.},
journal = {Computer Vision Systems},
pages = {3--26},
title = {{Recovering intrinsic scene characteristics from images}},
year = {1978}
}
@inproceedings{Liu2019DDFlow,
author = {Liu, Pengpeng and King, Irwin and Lyu, Michael R and Xu, Jia},
booktitle = {AAAI},
title = {{DDFlow: Learning Optical Flow with Unlabeled Data Distillation}},
year = {2019}
}
@book{Hartley2003,
abstract = {From the Publisher:A basic problem in computer vision is to understand the structure of a real world scene given several images of it. Recent major developments in the theory and practice of scene reconstruction are described in detail in a unified framework. The book covers the geometric principles and how to represent objects algebraically so they can be computed and applied. The authors provide comprehensive background material and explain how to apply the methods and implement the algorithms directly.},
address = {USA},
author = {Hartley, Richard and Zisserman, Andrew},
edition = {2},
isbn = {0521540518},
publisher = {Cambridge University Press},
title = {{Multiple View Geometry in Computer Vision}},
year = {2003}
}
@inproceedings{zhu17dff,
abstract = {Deep convolutional neutral networks have achieved great success on image recognition tasks. Yet, it is non-trivial to transfer the state-of-the-art image recognition net-works to videos as per-frame evaluation is too slow and un-affordable. We present deep feature flow, a fast and accu-rate framework for video recognition. It runs the expensive convolutional sub-network only on sparse key frames and propagates their deep feature maps to other frames via a flow field. It achieves significant speedup as flow computa-tion is relatively fast. The end-to-end training of the whole architecture significantly boosts the recognition accuracy. Deep feature flow is flexible and general. It is validated on two recent large scale video datasets. It makes a large step towards practical video recognition.},
archivePrefix = {arXiv},
arxivId = {1611.07715},
author = {Zhu, Xizhou and Xiong, Yuwen and Dai, Jifeng and Yuan, Lu and Wei, Yichen},
booktitle = {cvpr},
doi = {10.1109/CVPR.2017.441},
eprint = {1611.07715},
title = {{Deep Feature Flow for Video Recognition}},
url = {https://arxiv.org/pdf/1611.07715.pdf},
year = {2017}
}
@article{Schwab2017,
author = {Schwab, I R},
doi = {10.1038/eye.2017.226},
journal = {Eye},
month = {oct},
number = {2},
pages = {302--313},
publisher = {Springer Science and Business Media {\{}LLC{\}}},
title = {{The evolution of eyes: major steps. The Keeler lecture 2017: centenary of Keeler Ltd}},
url = {https://doi.org/10.1038/eye.2017.226},
volume = {32},
year = {2017}
}
@inproceedings{Wang2015,
archivePrefix = {arXiv},
arxivId = {1411.4958},
author = {Wang, Xiaolong and Fouhey, David F and Gupta, Abhinav},
booktitle = {cvpr},
doi = {10.1109/CVPR.2015.7298652},
eprint = {1411.4958},
isbn = {9781467369640},
issn = {10636919},
title = {{Designing Deep Networks for Surface Normal Estimation}},
url = {http://www.cs.cmu.edu/{~}xiaolonw/papers/deep3d.pdf},
year = {2015}
}
@inproceedings{kitti2015,
abstract = {This paper proposes a novel model and dataset for 3D scene flow estimation with an application to autonomous driving. Taking advantage of the fact that outdoor scenes often decompose into a small number of independently mov- ing objects, we represent each element in the scene by its rigid motion parameters and each superpixel by a 3D plane as well as an index to the corresponding object. This minimal representation increases robustness and leads to a discrete-continuous CRF where the data term decomposes into pairwise potentials between superpixels and objects. Moreover, our model intrinsically segments the scene into its constituting dynamic components. We demonstrate the performance of our model on existing benchmarks as well as a novel realistic dataset with scene flow ground truth. We obtain this dataset by annotating 400 dynamic scenes from the KITTI raw data collection using detailed 3D CAD models for all vehicles in motion. Our experiments also re- veal novel challenges which cannot be handled by existing methods. 1.},
archivePrefix = {arXiv},
arxivId = {1512.02134},
author = {Menze, Moritz and Geiger, Andreas},
booktitle = {cvpr},
doi = {10.1109/CVPR.2015.7298925},
eprint = {1512.02134},
file = {:home/hale/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Menze, Geiger - 2015 - Object scene flow for autonomous vehicles.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
keywords = {dataset,flow,optical flow,real,scene flow,stereo},
mendeley-tags = {dataset,flow,optical flow,real,scene flow,stereo},
pages = {3061--3070},
pmid = {15747803},
title = {{Object scene flow for autonomous vehicles}},
url = {http://www.cvlibs.net/publications/Menze2015CVPR.pdf},
volume = {07-12-June},
year = {2015}
}
@article{Aoki2019,
abstract = {PointNet has revolutionized how we think about representing point clouds. For classification and segmentation tasks, the approach and its subsequent extensions are state-of-the-art. To date, the successful application of PointNet to point cloud registration has remained elusive. In this paper we argue that PointNet itself can be thought of as a learnable "imaging" function. As a consequence, classical vision algorithms for image alignment can be applied on the problem - namely the Lucas {\&} Kanade (LK) algorithm. Our central innovations stem from: (i) how to modify the LK algorithm to accommodate the PointNet imaging function, and (ii) unrolling PointNet and the LK algorithm into a single trainable recurrent deep neural network. We describe the architecture, and compare its performance against state-of-the-art in common registration scenarios. The architecture offers some remarkable properties including: generalization across shape categories and computational efficiency - opening up new paths of exploration for the application of deep learning to point cloud registration. Code and videos are available at https://github.com/hmgoforth/PointNetLK.},
archivePrefix = {arXiv},
arxivId = {1903.05711},
author = {Aoki, Yasuhiro and Goforth, Hunter and Srivatsan, Rangaprasad Arun and Lucey, Simon},
eprint = {1903.05711},
file = {:home/hale/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Aoki et al. - 2019 - PointNetLK Robust {\&} Efficient Point Cloud Registration using PointNet.pdf:pdf},
keywords = {deep learning,pc matching,point cloud,point net},
mendeley-tags = {deep learning,pc matching,point cloud,point net},
month = {mar},
title = {{PointNetLK: Robust {\&} Efficient Point Cloud Registration using PointNet}},
url = {http://arxiv.org/abs/1903.05711},
year = {2019}
}
@inproceedings{Choi2019,
author = {Choi, Inchang and Gallo, Orazio and Troccoli, Alejandro and Kim, Min H and Kautz, Jan},
booktitle = {iccv},
month = {oct},
title = {{Extreme View Synthesis}},
year = {2019}
}
@inproceedings{resnet,
author = {He, K and Zhang, X and Ren, S and Sun, J},
booktitle = {cvpr},
doi = {10.1109/CVPR.2016.90},
month = {jun},
pages = {770--778},
title = {{Deep Residual Learning for Image Recognition}},
year = {2016}
}
@article{Rock1968,
author = {Rock, Irvin and Hill, A Lewis and Fineman, Mark},
doi = {10.3758/bf03210444},
journal = {Perception {\&} Psychophysics},
month = {jan},
number = {1},
pages = {37--40},
publisher = {Springer Science and Business Media {\{}LLC{\}}},
title = {{Speed constancy as a function of size constancy}},
url = {https://doi.org/10.3758/bf03210444},
volume = {4},
year = {1968}
}
@inproceedings{Baslamisli18eccv,
abstract = {Semantic segmentation of outdoor scenes is problematic when there are variations in imaging conditions. It is known that albedo (reflectance) is invariant to all kinds of illumination effects. Thus, using reflectance images for semantic segmentation task can be favorable. Additionally, not only segmentation may benefit from reflectance, but also segmentation may be useful for reflectance computation. Therefore, in this paper, the tasks of semantic segmentation and intrinsic image decomposition are considered as a combined process by exploring their mutual relationship in a joint fashion. To that end, we propose a supervised end-to-end CNN architecture to jointly learn intrinsic image decomposition and semantic segmentation. We analyze the gains of addressing those two problems jointly. Moreover, new cascade CNN architectures for intrinsic-for-segmentation and segmentation-for-intrinsic are proposed as single tasks. Furthermore, a dataset of 35K synthetic images of natural environments is created with corresponding albedo and shading (intrinsics), as well as semantic labels (segmentation) assigned to each object/scene. The experiments show that joint learning of intrinsic image decomposition and semantic segmentation is beneficial for both tasks for natural scenes. Dataset and models are available at: https://ivi.fnwi.uva.nl/cv/intrinseg},
archivePrefix = {arXiv},
arxivId = {1807.11857},
author = {Baslamisli, Anil S. and Groenestege, Thomas T. and Das, Partha and Le, Hoang-An and Karaoglu, Sezer and Gevers, Theo},
booktitle = {eccv},
eprint = {1807.11857},
file = {:home/hale/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Baslamisli et al. - 2018 - Joint Learning of Intrinsic Images and Semantic Segmentation.pdf:pdf},
month = {jul},
title = {{Joint Learning of Intrinsic Images and Semantic Segmentation}},
url = {http://arxiv.org/abs/1807.11857},
year = {2018}
}
@article{Poggio1981,
author = {Poggio, T},
doi = {10.1016/0166-2236(81)90081-3},
journal = {Trends in Neurosciences},
month = {jan},
pages = {258--262},
publisher = {Elsevier {\{}BV{\}}},
title = {{Marr{\{}$\backslash$textquotesingle{\}}s computational approach to vision}},
url = {https://doi.org/10.1016/0166-2236(81)90081-3},
volume = {4},
year = {1981}
}
@article{Shen3,
author = {Shen, L and Yang, X and Li, X and Jia, Y},
journal = {IEEE Transactions on Cybernetics},
pages = {425--436},
title = {{Intrinsic Image Decomposition Using Optimization and User Scribbles}},
year = {2013}
}
@inproceedings{Girshick2014a,
abstract = {Can a large convolutional neural network trained for whole-image classification on ImageNet be coaxed into detecting objects in PASCAL? We show that the answer is yes, and that the resulting system is simple, scalable, and boosts mean average precision, relative to the venerable deformable part model, by more than 40{\%} (achieving a final mAP of 48{\%} on VOC 2007). Our framework combines powerful computer vision techniques for generating bottom-up region proposals with recent advances in learning high-capacity convolutional neural networks. We call the resulting system R-CNN: Regions with CNN features. The same framework is also competitive with state-of-the-art semantic segmentation methods, demonstrating its flexibility. Beyond these results, we execute a battery of experiments that provide insight into what the network learns to represent, revealing a rich hierarchy of discriminative and often semantically meaningful features.},
archivePrefix = {arXiv},
arxivId = {1311.2524},
author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
booktitle = {cvpr},
doi = {10.1109/CVPR.2014.81},
eprint = {1311.2524},
file = {:home/hale/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Girshick et al. - Unknown - Rich feature hierarchies for accurate object detection and semantic segmentation.pdf:pdf},
isbn = {9781479951178},
issn = {10636919},
pages = {580--587},
pmid = {26656583},
title = {{Rich feature hierarchies for accurate object detection and semantic segmentation}},
url = {http://www.cs.berkeley.edu http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6909475 file:///Files/80/802045A7-7E41-430C-903D-8FFB98A6BADA.pdf papers3://publication/doi/10.1109/CVPR.2014.81},
year = {2014}
}
@inproceedings{Bartell1981,
author = {Bartell, F O and Dereniak, E L and Wolfe, W L},
booktitle = {Radiation Scattering in Optical Systems},
doi = {10.1117/12.959611},
editor = {Hunt, Gary H},
month = {mar},
publisher = {SPIE},
title = {{The Theory and Measurement of Bidirectional Reflectance Distribution Function (BRDF) and Bidirectional Transmittance Distribution Function (BTDF)}},
url = {https://doi.org/10.1117/12.959611},
year = {1981}
}
@inproceedings{Zhou2017,
author = {Zhou, Tinghui and Brown, Matthew and Snavely, Noah and Lowe, David G},
booktitle = {cvpr},
title = {{Unsupervised Learning of Depth and Ego-Motion from Video}},
url = {https://people.eecs.berkeley.edu/{~}tinghuiz/projects/SfMLearner/ https://people.eecs.berkeley.edu/{~}tinghuiz/projects/SfMLearner/cvpr17{\_}sfm{\_}final.pdf https://arxiv.org/pdf/1704.07813.pdf},
year = {2017}
}
@article{Gupta,
abstract = {In this paper we study the problem of object detection for RGB-D images using semantically rich image and depth features. We propose a new geocentric embedding for depth images that encodes height above ground and angle with gravity for each pixel in addition to the horizontal disparity. We demonstrate that this geocentric embedding works better than using raw depth images for learning feature representations with convolutional neural networks. Our final object detection system achieves an average precision of 37.3{\%}, which is a 56{\%} relative improvement over existing methods. We then focus on the task of instance segmentation where we label pixels belonging to object instances found by our detector. For this task, we propose a decision forest approach that classifies pixels in the detection window as foreground or background using a family of unary and binary tests that query shape and geocentric pose features. Finally, we use the output from our object detectors in an existing superpixel classification framework for semantic scene segmentation and achieve a 24{\%} relative improvement over current state-of-the-art for the object categories that we study. We believe advances such as those represented in this paper will facilitate the use of perception in fields like robotics.},
archivePrefix = {arXiv},
arxivId = {arXiv:1407.5736v1},
author = {Gupta, Saurabh and Girshick, Ross and Arbelaez, Pablo and Malik, Jitendra and Arbel{\'{a}}ez, Pablo Andr{\'{e}}s and Malik, Jitendra},
doi = {10.1007/978-3-319-10584-0_23},
eprint = {arXiv:1407.5736v1},
file = {:home/hale/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gupta et al. - 2014 - Learning Rich Features from {\{}RGB-D{\}} Images for Object Detection and Segmentation.pdf:pdf},
isbn = {9783319105833},
issn = {16113349},
journal = {eccv},
keywords = {RGB-D perception,RGBD,caffe,cnn,object detection,object detectionDiffindo,object segmentation,region proposal,rgbd,se,segmentation,semantic segmentation},
mendeley-tags = {RGBD,caffe,cnn,object detection,object detectionDiffindo,rgbd,se,segmentation,semantic segmentation},
number = {PART 7},
pages = {345--360},
title = {{Learning Rich Features from {\{}RGB-D{\}} Images for Object Detection and Segmentation}},
url = {http://dx.doi.org/10.1007/978-3-319-10584-0{\_}23},
volume = {8695},
year = {2014}
}
@article{smpl2015,
address = {New York, NY, USA},
author = {Loper, Matthew and Mahmood, Naureen and Romero, Javier and Pons-Moll, Gerard and Black, Michael J},
doi = {10.1145/2816795.2818013},
issn = {0730-0301},
journal = {tog},
keywords = {blendshapes,body shape,skinning,soft-tissue},
month = {oct},
number = {6},
publisher = {Association for Computing Machinery},
title = {{SMPL: A Skinned Multi-Person Linear Model}},
url = {https://doi.org/10.1145/2816795.2818013},
volume = {34},
year = {2015}
}
@inproceedings{liu2016ssd,
abstract = {We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines pre-dictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. SSD is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stages and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into sys-tems that require a detection component. Experimental results on the PASCAL VOC, COCO, and ILSVRC datasets confirm that SSD has competitive accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. For 300 Ã— 300 in-put, SSD achieves 74.3{\%} mAP 1 on VOC2007 test at 59 FPS on a Nvidia Titan X and for 512 Ã— 512 input, SSD achieves 76.9{\%} mAP, outperforming a compa-rable state-of-the-art Faster R-CNN model. Compared to other single stage meth-ods, SSD has much better accuracy even with a smaller input image size. Code is available at: https://github.com/weiliu89/caffe/tree/ssd .},
archivePrefix = {arXiv},
arxivId = {arXiv:1512.02325v1},
author = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C.},
booktitle = {eccv},
doi = {10.1016/j.nima.2015.05.028},
eprint = {arXiv:1512.02325v1},
file = {:home/hale/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu et al. - 2015 - SSD Single Shot MultiBox Detector.pdf:pdf},
isbn = {9781479951178},
issn = {01689002},
keywords = {Convolutional Neural Network,Real-time Object Detection},
pmid = {23739795},
title = {{SSD : Single Shot MultiBox Detector}},
url = {http://www.cs.unc.edu/{~}wliu/papers/ssd.pdf https://arxiv.org/pdf/1512.02325.pdf},
year = {2016}
}
@inproceedings{3drms2017,
abstract = {Part of the workshop is a challenge on combining 3D and semantic information in complex scenes. To this end, a challenging outdoor dataset, captured by a robot driving through a semantically-rich garden that contains fine geo-metric details, was released. A multi-camera rig is mounted on top of the robot, enabling the use of both stereo and mo-tion stereo information. Precise ground truth for the 3D structure of the garden has been obtained with a laser scan-ner and accurate pose estimates for the robot are available as well. Ground truth semantic labels and ground truth depth from a laser scan are used for benchmarking the qual-ity of the 3D reconstructions.},
author = {Sattler, Torsten and Tylecek, Radim and Brox, Thomas and Pollefeys, Marc and Fisher, Robert B},
booktitle = {ccvwl},
file = {:home/hale/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sattler et al. - 2017 - 3D Reconstruction meets Semantics â€“ Reconstruction Challenge.pdf:pdf},
keywords = {trimbot},
mendeley-tags = {trimbot},
month = {oct},
organization = {ICCV Workshops},
pages = {1--7},
title = {{3D Reconstruction meets Semantics â€“ Reconstruction Challenge}},
url = {http://trimbot2020.webhosting.rug.nl/events/3drms/challenge/ http://trimbot2020.webhosting.rug.nl/wp-content/uploads/2017/11/rms{\_}challenge.pdf},
year = {2017}
}
@inproceedings{Fouhey2013,
abstract = {What primitives should we use to infer the rich 3D world behind an image? We argue that these primitives should be both visually discriminative and geometrically informa-tive and we present a technique for discovering such primi-tives. We demonstrate the utility of our primitives by using them to infer 3D surface normals given a single image. Our technique substantially outperforms the state-of-the-art and shows improved cross-dataset performance.},
author = {Fouhey, David F and Gupta, Abhinav and Hebert, Martial},
booktitle = {iccv},
keywords = {metric,surface normal},
mendeley-tags = {metric,surface normal},
pages = {3392--3399},
title = {{Data-Driven 3D Primitives for Single Image Understanding}},
url = {http://www.cs.cmu.edu/{~}dfouhey/3DP/dfouhey{\_}primitives.pdf},
year = {2013}
}
@techreport{shapenet2015,
author = {Chang, Angel X and Funkhouser, Thomas and Guibas, Leonidas and Hanrahan, Pat and Huang, Qixing and Li, Zimo and Savarese, Silvio and Savva, Manolis and Song, Shuran and Su, Hao and Xiao, Jianxiong and Yi, Li and Yu, Fisher},
institution = {Stanford University --- Princeton University --- Toyota Technological Institute at Chicago},
number = {arXiv:1512.03012 [cs.GR]},
title = {{ShapeNet: An Information-Rich 3D Model Repository}},
year = {2015}
}
@inproceedings{Yuan2019,
abstract = {Shape completion, the problem of estimating the complete geometry of objects from partial observations, lies at the core of many vision and robotics applications. In this work, we propose Point Completion Network (PCN), a novel learning-based approach for shape completion. Unlike existing shape completion methods, PCN directly operates on raw point clouds without any structural assumption (e.g. symmetry) or annotation (e.g. semantic class) about the underlying shape. It features a decoder design that enables the generation of fine-grained completions while maintaining a small number of parameters. Our experiments show that PCN produces dense, complete point clouds with realistic structures in the missing regions on inputs with various levels of incompleteness and noise, including cars from LiDAR scans in the KITTI dataset.},
archivePrefix = {arXiv},
arxivId = {1808.00671},
author = {Yuan, Wentao and Khot, Tejas and Held, David and Mertz, Christoph and Hebert, Martial},
booktitle = {Proceedings - 2018 International Conference on 3D Vision, 3DV 2018},
doi = {10.1109/3DV.2018.00088},
eprint = {1808.00671},
file = {:home/hale/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yuan et al. - 2018 - PCN Point completion network.pdf:pdf},
isbn = {9781538684252},
keywords = {3D reconstruction,Learning on point clouds,Point cloud registration,Shape completion},
pages = {728--737},
title = {{PCN: Point completion network}},
url = {https://wentaoyuan.github.io/pcn.},
year = {2018}
}
@inproceedings{Meka1,
author = {Meka, A and Fox, G and Zollh{\"{o}}fer, M and Richardt, C and Theobalt, C},
booktitle = {IEEE International Symposium on Mixed and Augmented Reality},
title = {{Live User-Guided Intrinsic Video For Static Scenes}},
year = {2017}
}
@inproceedings{Song2014,
abstract = {The depth information of RGB-D sensors has greatly simplified some common challenges in computer vision and enabled breakthroughs for several tasks. In this paper, we propose to use depth maps for object detection and de-sign a 3D detector to overcome the major difficulties for recognition, namely the variations of texture, illumination, shape, viewpoint, clutter, occlusion, self-occlusion and sensor noises. We take a collection of 3D CAD models and render each CAD model from hundreds of viewpoints to obtain synthetic depth maps. For each depth rendering, we extract features from the 3D point cloud and train an Exemplar-SVM classifier. During testing and hard-negative mining, we slide a 3D detection window in 3D space. Experiment results show that our 3D detector significantly outperforms the state-of-the-art algorithms for both RGB and RGB-D images, and achieves about Ã—1.7 improvement on average precision compared to DPM and R-CNN. All source code and data are available online.},
archivePrefix = {arXiv},
arxivId = {1511.02300},
author = {Song, Shuran and Xiao, Jianxiong},
booktitle = {eccv},
doi = {10.1007/978-3-319-10599-4},
eprint = {1511.02300},
file = {:home/hale/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Song, Xiao - 2014 - Sliding Shapes for 3D Object Detection in Depth Images.pdf:pdf},
isbn = {978-3-319-10598-7},
keywords = {3D,RGBD,detection,labelling,object detection,sliding windows},
mendeley-tags = {3D,RGBD,detection,labelling,object detection,sliding windows},
pages = {1--8},
title = {{Sliding Shapes for 3D Object Detection in Depth Images}},
url = {http://slidingshapes.cs.princeton.edu},
year = {2014}
}
@article{McCane2001,
abstract = {Evaluating the performance of optical flow algorithms has been difficult because of the lack of ground truth data sets for complex scenes. We present a new method for generating motion fields from real sequences containing polyhedral objects and present a test suite for benchmarking optical flow algorithms consisting of complex synthetic sequences and real scenes with ground truth. We provide a preliminary quantitative evaluation of seven optical flow algorithms using these synthetic and real sequences. Ultimately, we feel that researchers should benchmark their own algorithms using a standard suite. To that end, we offer our Web site as a repository for standard sequences and results.},
author = {McCane, B and Novins, K and Crannitch, D and Galvin, B},
doi = {https://doi.org/10.1006/cviu.2001.0930},
issn = {1077-3142},
journal = {Computer Vision and Image Understanding (CVIU)},
keywords = {dataset,optical flow},
mendeley-tags = {dataset,optical flow},
number = {1},
pages = {126--143},
title = {{On Benchmarking Optical Flow}},
url = {http://www.sciencedirect.com/science/article/pii/S1077314201909300},
volume = {84},
year = {2001}
}
@inproceedings{Krizhevsky2012,
address = {USA},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
booktitle = {nips},
pages = {1097--1105},
publisher = {Curran Associates Inc.},
series = {NIPS'12},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
url = {http://dl.acm.org/citation.cfm?id=2999134.2999257},
year = {2012}
}
@inproceedings{Jaderberg2015,
abstract = {Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner. In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in stateoftheart performance on several benchmarks, and for a number of classes of transformations.},
author = {Jaderberg, Max and Simonyan, Karen and Zisserman, Andrew and Kavukcuoglu, Koray},
booktitle = {NIPS},
title = {{Spatial Transformer Networks}}
}
@article{Vazquez2014,
author = {V{\'{a}}zquez, D and L{\'{o}}pez, A M and Mar{\'{i}}n, J and Ponsa, D and Ger{\'{o}}nimo, D},
journal = {tpami},
keywords = {adaptation,synthetic},
mendeley-tags = {adaptation,synthetic},
number = {4},
pages = {797--809},
title = {{Virtual and Real World Adaptation for Pedestrian Detection}},
volume = {36},
year = {2014}
}
@inproceedings{Fu2018,
author = {Fu, H and Gong, M and Wang, C and Batmanghelich, K and Tao, D},
booktitle = {cvpr},
pages = {2002--2011},
title = {{Deep Ordinal Regression Network for Monocular Depth Estimation}},
year = {2018}
}
@inproceedings{Peng2015,
author = {Peng, Xingchao and Sun, Baochen and Ali, Karim and Saenko, Kate},
booktitle = {iccv},
month = {dec},
title = {{Learning Deep Object Detectors From 3D Models}},
year = {2015}
}
@inproceedings{Sun2018_actrec,
author = {Sun, Shuyang and Kuang, Zhanghui and Sheng, Lu and Ouyang, Wanli and Zhang, Wei},
booktitle = {cvpr},
month = {jun},
title = {{Optical Flow Guided Feature: A Fast and Robust Motion Representation for Video Action Recognition}},
year = {2018}
}
@book{Gevers2012,
author = {Gevers, Theo and Gijsenij, Arjan and van de Weijer, Joost and Geusebroek, Jan-Mark},
doi = {10.1002/9781118350089},
month = {aug},
publisher = {John Wiley $\backslash${\&} Sons, Inc.},
title = {{Color in Computer Vision}},
url = {https://doi.org/10.1002/9781118350089},
year = {2012}
}
@article{Warren2009,
abstract = {We have recently suggested that the brain uses its sensitivity to optic flow in order to parse retinal motion into components arising due to self and object movement (e.g. Rushton, S. K., {\&} Warren, P. A. (2005). Moving observers, 3D relative motion and the detection of object movement. Current Biology, 15, R542-R543). Here, we explore whether stereo disparity is necessary for flow parsing or whether other sources of depth information, which could theoretically constrain flow-field interpretation, are sufficient. Stationary observers viewed large field of view stimuli containing textured cubes, moving in a manner that was consistent with a complex observer movement through a stationary scene. Observers made speeded responses to report the perceived direction of movement of a probe object presented at different depths in the scene. Across conditions we varied the presence or absence of different binocular and monocular cues to depth order. In line with previous studies, results consistent with flow parsing (in terms of both perceived direction and response time) were found in the condition in which motion parallax and stereoscopic disparity were present. Observers were poorer at judging object movement when depth order was specified by parallax alone. However, as more monocular depth cues were added to the stimulus the results approached those found when the scene contained stereoscopic cues. We conclude that both monocular and binocular static depth information contribute to flow parsing. These findings are discussed in the context of potential architectures for a model of the flow parsing mechanism. {\textcopyright} 2009 Elsevier Ltd. All rights reserved.},
author = {Warren, Paul A and Rushton, Simon K},
doi = {10.1016/j.visres.2009.01.016},
isbn = {0042-6989},
issn = {00426989},
journal = {Vision Research},
keywords = {Motion processing,Object movement,Optic flow,Psychophysics,Self-movement},
number = {11},
pages = {1406--1419},
pmid = {19480063},
title = {{Perception of scene-relative object movement: Optic flow parsing and the contribution of monocular depth cues}},
volume = {49},
year = {2009}
}
@article{DAVIS2017,
author = {Pont-Tuset, Jordi and Perazzi, Federico and Caelles, Sergi and Arbel{\'{a}}ez, Pablo and Sorkine-Hornung, Alexander and {Van Gool}, Luc},
journal = {arXiv:1704.00675},
title = {{The 2017 DAVIS Challenge on Video Object Segmentation}},
year = {2017}
}
@inproceedings{Shi2017,
author = {Shi, J and Dong, Y and Su, H and Yu, S {\~{}}X.},
booktitle = {cvpr},
title = {{Learning Non-Lambertian Object Intrinsics across ShapeNet Categories}},
year = {2017}
}
@inproceedings{Liu2018,
author = {Liu, Miaomiao and He, Xuming and Salzmann, Mathieu},
booktitle = {cvpr},
month = {jun},
title = {{Geometry-Aware Deep Network for Single-Image Novel View Synthesis}},
year = {2018}
}
@article{kitti2013,
abstract = {Abstract We present a novel dataset captured from a VW station wagon for use in mobile robotics and autonomous driving research. In total, we recorded 6 hours of traffic scenarios at 10â€“100 Hz using a variety of sensor modalities such as high-resolution color and ...},
author = {Geiger, A. and Lenz, P. and Stiller, C. and Urtasun, R.},
doi = {10.1177/0278364913491297},
file = {:home/hale/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Geiger et al. - 2013 - Vision meets robotics The KITTI dataset.pdf:pdf},
issn = {0278-3649},
journal = {The International Journal of Robotics Research},
keywords = {dataset,real},
mendeley-tags = {dataset,real},
number = {11},
pages = {1231--1237},
title = {{Vision meets robotics: The KITTI dataset}},
url = {http://ijr.sagepub.com/cgi/doi/10.1177/0278364913491297},
volume = {32},
year = {2013}
}
@inproceedings{Kaneva2011,
author = {Kaneva, B and Torralba, A and Freeman, W T},
booktitle = {iccv},
keywords = {simulator},
mendeley-tags = {simulator},
pages = {2282--2289},
title = {{Evaluation of image features using a photorealistic virtual world}},
year = {2011}
}
@article{Galama2019,
abstract = {We are interested in learning visual representations which allow for 3D manipulations of visual objects based on a single 2D image. We cast this into an image-to-image transformation task, and propose Iterative Generative Adversarial Networks (IterGANs) which iteratively transform an input image into an output image. Our models learn a visual representation that can be used for objects seen in training, but also for never seen objects. Since object manipulation requires a full understanding of the geometry and appearance of the object, our IterGANs learn an implicit 3D model and a full appearance model of the object, which are both inferred from a single (test) image. Two advantages of IterGANs are that the intermediate generated images can be used for an additional supervision signal, even in an unsupervised fashion, and that the number of iterations can be used as a control signal to steer the transformation. Experiments on rotated objects and scenes show how IterGANs help with the generation process.},
archivePrefix = {arXiv},
arxivId = {1804.05651},
author = {Galama, Ysbrand and Mensink, Thomas},
doi = {10.1016/j.cviu.2019.102803},
eprint = {1804.05651},
isbn = {2019.102803},
issn = {10773142},
journal = {Computer Vision and Image Understanding (CVIU)},
keywords = {Adversarial,Estimation,Generative,Networks,Novel,Object,Transoformation,Viewpoint},
pages = {102803},
title = {{IterGANs: Iterative GANs to learn and control 3D object transformation}},
url = {https://doi.org/10.1016/j.cviu.2019.102803},
volume = {189},
year = {2019}
}
@article{Barth2020,
abstract = {In this paper we report on improving part segmentation performance for robotic vision using convolutional neural networks by optimising the visual realism of synthetic agricultural images. In Part I, a cycle consistent generative adversarial network was applied to synthetic and empirical images with the objective to generate more realistic synthetic images by translating them to the empirical domain. We hypothesise that plant part image features (e.g. color, texture) become more similar to the empirical domain after translation of the synthetic images. Results confirm this with an improved mean color distribution correlation with the empirical data prior of 0.62 and post translation of 0.90. Furthermore, the mean image features of contrast, homogeneity, energy and entropy moved closer to the empirical mean, post translation. In Part II, 7 experiments were performed using convolutional neural networks with different combinations of synthetic, synthetic translated to empirical and empirical images. We hypothesise that the translated images can be used for (i) improved learning of empirical images, and (ii) that learning without any fine-tuning with empirical images is improved by bootstrapping with translated images over bootstrapping with synthetic images. Results confirm our hypotheses in Part II. First a maximum intersection-over-union performance was achieved of 0.52 when bootstrapping with translated images and fine-tuning with empirical images; an 8{\%} increase compared to only using synthetic images. Second, training without any empirical fine-tuning resulted in an average IOU of 0.31; a 55{\%} performance increase over previous methods that only used synthetic images. The key contribution of this paper to robotic vision is to provide supporting evidence that domain adaptation can be successfully used to translate and improve synthetic data to the real empirical domain that results in improved segmentation learning whilst lowering the dependency on manually annotated data.},
author = {Barth, R and Hemming, J and {Van Henten}, E J},
doi = {10.1016/j.compag.2020.105378},
issn = {0168-1699},
journal = {Computers and Electronics in Agriculture},
keywords = {3D modelling,Agriculture,Robotics,Semantic segmentation,Synthetic dataset,agriculture,dataset,semantic segmentation,synthetic},
mendeley-tags = {agriculture,dataset,semantic segmentation,synthetic},
publisher = {Elsevier},
title = {{Optimising realism of synthetic images using cycle generative adversarial networks for improved part segmentation}},
volume = {173},
year = {2020}
}
@article{Kholgade2014,
address = {New York, NY, USA},
author = {Kholgade, Natasha and Simon, Tomas and Efros, Alexei and Sheikh, Yaser},
doi = {10.1145/2601097.2601209},
issn = {0730-0301},
journal = {tog},
keywords = {3D models,photo-editing,three-dimensional},
month = {jul},
number = {4},
pages = {127:1----127:12},
publisher = {ACM},
title = {{3D Object Manipulation in a Single Photograph Using Stock 3D Models}},
url = {http://doi.acm.org/10.1145/2601097.2601209},
volume = {33},
year = {2014}
}
@inproceedings{zhou,
author = {Zhou, T and Kr{\"{a}}henb{\"{u}}hl, P and Efros, A {\~{}}A.},
booktitle = {iccv},
title = {{Learning Data-driven Reflectance Priors for Intrinsic Image Decomposition}},
year = {2015}
}
@inproceedings{runia2018,
author = {Runia, Tom F H and Snoek, Cees G M and Smeulders, Arnold W M},
booktitle = {cvpr},
month = {jun},
title = {{Real-World Repetition Estimation by Div, Grad and Curl}},
year = {2018}
}
@inproceedings{Zhao2017,
author = {Zhao, Hengshuang and Shi, Jianping and Qi, Xiaojuan and Wang, Xiaogang and Jia, Jiaya},
booktitle = {cvpr},
keywords = {cnn,segmentation,semantic},
mendeley-tags = {cnn,segmentation,semantic},
pages = {310--313},
title = {{Pyramid Scene Parsing Network}},
year = {2017}
}
@inproceedings{batchnorm,
abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it no-toriously hard to train models with saturating nonlineari-ties. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer in-puts. Our method draws its strength from making normal-ization a part of the model architecture and performing the normalization for each training mini-batch. Batch Nor-malization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regu-larizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9{\%} top-5 validation error (and 4.8{\%} test error), exceeding the ac-curacy of human raters.},
archivePrefix = {arXiv},
arxivId = {arXiv:1502.03167v3},
author = {Ioffe, S and Szegedy, C},
booktitle = {Proceedings of the International Conference on Machine Learning (ICML)},
eprint = {arXiv:1502.03167v3},
title = {{Batch normalization: Accelerating deep network training by reducing internal covariate shift}},
year = {2015}
}
@inproceedings{Hinton2015,
author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
keywords = {compression,deep{\_}learning{\_}architectures,deep{\_}learning{\_}theory,hinton,multilayer{\_}networks,networks,neural},
title = {{Distilling the Knowledge in a Neural Network}},
url = {http://arxiv.org/pdf/1503.02531v1.pdf},
year = {2015}
}
@article{matterport3d2017,
author = {Chang, Angel and Dai, Angela and Funkhouser, Thomas and Halber, Maciej and Niessner, Matthias and Savva, Manolis and Song, Shuran and Zeng, Andy and Zhang, Yinda},
journal = {International Conference on 3D Vision (3DV)},
keywords = {dataset,scanned},
mendeley-tags = {dataset,scanned},
title = {{Matterport3D: Learning from RGB-D Data in Indoor Environments}},
year = {2017}
}
@inproceedings{Yang2015,
address = {Cambridge, MA, USA},
author = {Yang, Jimei and Reed, Scott and Yang, Ming-Hsuan and Lee, Honglak},
booktitle = {nips},
keywords = {3d,nvs,weak},
mendeley-tags = {3d,nvs,weak},
pages = {1099--1107},
publisher = {MIT Press},
series = {NIPS'15},
title = {{Weakly-Supervised Disentangling with Recurrent Transformations for 3D View Synthesis}},
year = {2015}
}
@inproceedings{zoran,
author = {Zoran, D and Isola, P and Krishnan, D and Freeman, William T.},
booktitle = {iccv},
title = {{Learning Ordinal Relationships for Mid-Level Vision}},
year = {2015}
}
@article{Laffont0,
author = {Laffont, P. Y. and Bousseau, A and Paris, S and Durand, F and Drettakis, G},
journal = {tog},
title = {{Coherent intrinsic images from photo collections}},
year = {2012}
}
@inproceedings{Rock2015,
abstract = {single depth imageë¡œ 3D modelì„ reconstructí•˜ëŠ” ë°©ë²• ì†Œê°œ},
author = {Rock, Jason and Thorsen, Justin and Hoiem, Derek},
booktitle = {cvpr},
file = {:home/hale/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rock, Thorsen, Hoiem - 2015 - Completing 3D Object Shape from One Depth Image.pdf:pdf},
isbn = {9781467369640},
keywords = {3D Objects,3D deformation,mesh analysis,shape matching},
mendeley-tags = {3D Objects,3D deformation,mesh analysis,shape matching},
pages = {3013--3021},
title = {{Completing 3D Object Shape from One Depth Image}},
year = {2015}
}
@inproceedings{Leibe2007,
author = {Leibe, B and Cornelis, N and Cornelis, K and {Van Gool}, L},
booktitle = {cvpr},
keywords = {dataset,driving,real},
mendeley-tags = {dataset,driving,real},
pages = {1--8},
title = {{Dynamic 3D Scene Analysis from a Moving Vehicle}},
year = {2007}
}
@incollection{Wedel2011,
abstract = {In this chapter we review the estimation of the two-dimensional apparent motion field of two consecutive images in an image sequence. This apparent motion field is referred to as optical flow field, a two-dimensional vector field on the image plane. Because it is nearly impossible to cover the vast amount of approaches in the literature, in this chapter we set the focus on energy minimization approaches which estimate a dense flow field. The term dense refers to the fact that a flow vector is assigned to every (non-occluded) image pixel. Most dense approaches are based on the variational formulation of the optical flow problem, firstly suggested by Horn and Schunk. Depending on the application, density might be one important property besides accuracy and robustness. In many cases computational speed and real-time capability is a crucial issue. In this chapter we therefore discuss the latest progress in accuracy, robustness and real-time capability of dense optical flow algorithms.},
address = {London},
author = {Wedel, Andreas and Cremers, Daniel},
booktitle = {Stereo Scene Flow for 3D Motion Analysis},
doi = {10.1007/978-0-85729-965-9_2},
isbn = {978-0-85729-965-9},
keywords = {lecture,optical flow},
mendeley-tags = {lecture,optical flow},
pages = {5--34},
publisher = {Springer London},
title = {{Optical Flow Estimation}},
url = {https://doi.org/10.1007/978-0-85729-965-9{\_}2},
year = {2011}
}
@inproceedings{bell,
author = {Bell, M and Freeman, William T.},
booktitle = {iccv},
title = {{Learning local evidence for shading and reflectance}},
year = {2001}
}
@book{Wandell1995,
author = {Wandell, B A},
isbn = {9780878938537},
keywords = {background,perception,visionj},
mendeley-tags = {background,perception,visionj},
publisher = {Oxford University Press, Incorporated},
series = {Europ{\"{a}}ische Hochschulschriften},
title = {{Foundations of Vision}},
url = {https://books.google.nl/books?id=dVRRAAAAMAAJ},
year = {1995}
}
@article{Bousseau,
author = {Bousseau, A and Paris, S and Durand, F},
journal = {tog},
title = {{User-assisted intrinsic images}},
year = {2009}
}
@inproceedings{Ren,
abstract = {Scene labeling research has mostly focused on outdoor scenes, leaving the harder case of indoor scenes poorly understood. Microsoft Kinect dramatically changed the landscape, showing great potentials for RGB-D perception (color+depth). Our main objective is to empirically understand the promises and challenges of scene labeling with RGB-D. We use the NYU Depth Dataset as collected and analyzed by Silberman and Fergus [30]. For RGB-D features, we adapt the framework of kernel descriptors that converts local similarities (kernels) to patch descriptors. For contextual modeling, we combine two lines of approaches, one using a superpixel MRF, and the other using a segmentation tree. We find that (1) kernel descriptors are very effective in capturing appearance (RGB) and shape (D) similarities; (2) both superpixel MRF and segmentation tree are useful in modeling context; and (3) the key to labeling accuracy is the ability to efficiently train and test with large-scale data. We improve labeling accuracy on the NYU Dataset from 56.6{\%} to 76.1{\%}. We also apply our approach to image-only scene labeling and improve the accuracy on the Stanford Background Dataset from 79.4{\%} to 82.9{\%}.},
author = {Ren, Xiaofeng and Bo, Liefeng and Fox, Dieter},
booktitle = {cvpr},
doi = {10.1109/CVPR.2012.6247999},
file = {:home/hale/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ren, Bo, Fox - 2012 - RGB-(D) scene labeling Features and algorithms.pdf:pdf},
isbn = {9781467312264},
issn = {10636919},
keywords = {MRF,labelling,rgbd,scene labelling,segmentation tree,superpixel},
mendeley-tags = {MRF,labelling,rgbd,scene labelling,segmentation tree,superpixel},
pages = {2759--2766},
title = {{RGB-(D) scene labeling: Features and algorithms}},
year = {2012}
}
@article{Han2018arxiv,
annote = {From Duplicate 1 (Improving Face Detection Performance with 3D-Rendered Synthetic Data - Han, Jian; Karaoglu, Sezer; Le, Hoang-An; Gevers, Theo)

Withdrawn.

From Duplicate 2 (Improving Face Detection Performance with 3D-Rendered Synthetic Data - Han, Jian; Karaoglu, Sezer; Le, Hoang-An; Gevers, Theo)

From Duplicate 1 (Improving Face Detection Performance with 3D-Rendered Synthetic Data - Han, Jian; Karaoglu, Sezer; Le, Hoang-An; Gevers, Theo)

Withdrawn.},
archivePrefix = {arXiv},
arxivId = {1812.07363},
author = {Han, Jian and Karaoglu, Sezer and Le, Hoang-An and Gevers, Theo},
eprint = {1812.07363},
journal = {ArXiv e-prints},
keywords = {dataset,synthetic},
mendeley-tags = {dataset,synthetic},
month = {dec},
title = {{Improving Face Detection Performance with 3D-Rendered Synthetic Data}},
url = {http://arxiv.org/abs/1812.07363},
year = {2018}
}
@article{Kendall2015,
abstract = {We present a novel deep learning framework for probabilistic pixel-wise semantic segmentation, which we term Bayesian SegNet. Pixel-wise semantic segmentation is an important step for visual scene understanding. It is a complex task requiring knowledge of support relationships and contextual information, as well as visual appearance. Our contribution is a practical system which is able to predict pixel-wise class labels with a measure of model uncertainty. We achieve this by Monte Carlo sampling with dropout at test time to generate a posterior distribution of pixel class labels. We show this Bayesian neural network provides a significant performance improvement in segmentation, with no additional parameterisation. We set a new benchmark with state-of-the-art performance on both the indoor SUN Scene Understanding and outdoor CamVid driving scenes datasets. Bayesian SegNet also performs competitively on Pascal VOC 2012 object segmentation challenge. For our web demo and source code, see http://mi.eng.cam.ac.uk/projects/segnet/},
archivePrefix = {arXiv},
arxivId = {1511.02680},
author = {Kendall, Alex and Badrinarayanan, Vijay and Cipolla, Roberto},
eprint = {1511.02680},
file = {:home/hale/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kendall, Badrinarayanan, Cipolla - 2015 - Bayesian SegNet model uncertainty in deep convolutional encoder-decoder architectures for scen.pdf:pdf},
journal = {arXiv:1511.02680v1 [cs.CV]},
keywords = {cnn,deep,segmentation},
mendeley-tags = {cnn,deep,segmentation},
title = {{Bayesian SegNet: model uncertainty in deep convolutional encoder-decoder architectures for scene understanding}},
url = {http://arxiv.org/abs/1511.02680},
year = {2015}
}
@inproceedings{HALe2018,
abstract = {Optical flow, semantic segmentation, and surface normals represent different information modalities, yet together they bring better cues for scene understanding problems. In this paper, we study the influence between the three modalities: how one impacts on the others and their efficiency in combination. We employ a modular approach using a convolutional refinement network which is trained supervised but isolated from RGB images to enforce joint modality features. To assist the training process, we create a large-scale synthetic outdoor dataset that supports dense annotation of semantic segmentation, optical flow, and surface normals. The experimental results show positive influence among the three modalities, especially for objects' boundaries, region consistency, and scene structures.},
archivePrefix = {arXiv},
arxivId = {1807.07473},
author = {Le, Hoang-An and Baslamisli, Anil S. and Mensink, Thomas and Gevers, Theo},
booktitle = {bmvc},
doi = {arXiv:1807.07473v1},
eprint = {1807.07473},
file = {:home/hale/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Le et al. - 2018 - Three for one and one for three Flow, Segmentation, and Surface Normals.pdf:pdf},
month = {jul},
title = {{Three for one and one for three: Flow, Segmentation, and Surface Normals}},
url = {http://arxiv.org/abs/1807.07473},
year = {2018}
}
@article{Yuan2018,
abstract = {3D point cloud is an efficient and flexible representation of 3D structures. Recently, neural networks operating on point clouds have shown superior performance on 3D understanding tasks such as shape classification and part segmentation. However, performance on such tasks is evaluated on complete shapes aligned in a canonical frame, while real world 3D data are partial and unaligned. A key challenge in learning from partial, unaligned point cloud data is to learn features that are invariant or equivariant with respect to geometric transformations. To address this challenge, we propose the Iterative Transformer Network (IT-Net), a network module that canonicalizes the pose of a partial object with a series of 3D rigid transformations predicted in an iterative fashion. We demonstrate the efficacy of IT-Net as an anytime pose estimator from partial point clouds without using complete object models. Further, we show that IT-Net achieves superior performance over alternative 3D transformer networks on various tasks, such as partial shape classification and object part segmentation.},
archivePrefix = {arXiv},
arxivId = {1811.11209},
author = {Yuan, Wentao and Held, David and Mertz, Christoph and Hebert, Martial},
eprint = {1811.11209},
file = {:home/hale/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yuan et al. - 2018 - Iterative Transformer Network for 3D Point Cloud.pdf:pdf},
keywords = {deep learning,pc matching,point cloud,point net,pointnet},
mendeley-tags = {deep learning,pc matching,point cloud,point net,pointnet},
month = {nov},
title = {{Iterative Transformer Network for 3D Point Cloud}},
url = {http://arxiv.org/abs/1811.11209},
year = {2018}
}
@inproceedings{Hoiem2005,
address = {New York, NY, USA},
author = {Hoiem, Derek and Efros, Alexei A and Hebert, Martial},
booktitle = {tog},
doi = {10.1145/1186822.1073232},
isbn = {9781450378253},
keywords = {image segmentation,image-based rendering,machine learning,single-view reconstruction},
pages = {577--584},
publisher = {Association for Computing Machinery},
series = {SIGGRAPH '05},
title = {{Automatic Photo Pop-Up}},
url = {https://doi.org/10.1145/1186822.1073232},
year = {2005}
}
@inproceedings{Weiss,
author = {Weiss, Y},
booktitle = {iccv},
title = {{Deriving intrinsic images from image sequences}},
year = {2001}
}
@inproceedings{skipconnection,
author = {Mao, X and Shen, C and Yang, Y},
booktitle = {nips},
title = {{Image restoration using very deep fully convolutional encoder-decoder networks with symmetric skip connections}},
year = {2016}
}
@article{devito2017opt,
author = {DeVito, Zachary and Mara, Michael and Zoll{\"{o}}fer, Michael and Bernstein, Gilbert and Theobalt, Christian and Hanrahan, Pat and Fisher, Matthew and Nie{\ss}ner, Matthias},
journal = {tog},
title = {{Opt: A Domain Specific Language for Non-linear Least Squares Optimization in Graphics and Imaging}},
year = {2017}
}
@inproceedings{EpicFlow,
author = {Revaud, J and Weinzaepfel, P and Harchaoui, Z and Schmid, C},
booktitle = {cvpr},
doi = {10.1109/CVPR.2015.7298720},
issn = {1063-6919},
month = {jun},
pages = {1164--1172},
title = {{EpicFlow: Edge-preserving interpolation of correspondences for optical flow}},
year = {2015}
}
@inproceedings{SegFlow_ICCV17,
abstract = {This paper proposes an end-to-end trainable network, SegFlow, for simultaneously predicting pixel-wise object segmentation and optical flow in videos. The proposed SegFlow has two branches where useful information of object segmentation and optical flow is propagated bidirectionally in a unified framework. The segmentation branch is based on a fully convolutional network, which has been proved effective in image segmentation task, and the optical flow branch takes advantage of the FlowNet model. The unified framework is trained iteratively offline to learn a generic notion, and fine-tuned online for specific objects. Extensive experiments on both the video object segmentation and optical flow datasets demonstrate that introducing optical flow improves the performance of segmentation and vice versa, against the state-of-the-art algorithms.},
archivePrefix = {arXiv},
arxivId = {1709.06750},
author = {Cheng, Jingchun and Tsai, Yi-hsuan and Wang, Shengjin and Yang, Ming-Hsuan and Yang, Shengjin Wang Ming-hsuan},
booktitle = {iccv},
doi = {10.1109/ICCV.2017.81},
eprint = {1709.06750},
file = {:home/hale/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cheng et al. - Unknown - SegFlow Joint Learning for Video Object Segmentation and Optical Flow.pdf:pdf},
title = {{SegFlow : Joint Learning for Video Object Segmentation and Optical Flow}},
url = {http://openaccess.thecvf.com/content{\_}ICCV{\_}2017/papers/Cheng{\_}SegFlow{\_}Joint{\_}Learning{\_}ICCV{\_}2017{\_}paper.pdf http://arxiv.org/abs/1709.06750},
year = {2017}
}
@article{Aodha2013,
address = {Washington, DC, USA},
author = {{Mac Aodha}, Oisin and Humayun, Ahmad and Pollefeys, Marc and Brostow, Gabriel J},
doi = {10.1109/TPAMI.2012.171},
issn = {0162-8828},
journal = {tpami},
keywords = {Accuracy,Adaptive optics,Optical flow,Optical imaging,Optical variables measurement,Prediction algorithms,Random Forest,Supervised learning,Vectors,algorithm selection,confidence measure,dataset,optical flow,synthetic data},
mendeley-tags = {dataset,optical flow},
month = {may},
number = {5},
pages = {1107--1120},
publisher = {IEEE Computer Society},
title = {{Learning a Confidence Measure for Optical Flow}},
url = {http://dx.doi.org/10.1109/TPAMI.2012.171},
volume = {35},
year = {2013}
}
@article{Snoek2012,
abstract = {Machine learning algorithms frequently require careful tuning of model hyperparameters, regularization terms, and optimization parameters. Unfortunately, this tuning is often a "black art" that requires expert experience, unwritten rules of thumb, or sometimes brute-force search. Much more appealing is the idea of developing automatic approaches which can optimize the performance of a given learning algorithm to the task at hand. In this work, we consider the automatic tuning problem within the framework of Bayesian optimization, in which a learning algorithm's generalization performance is modeled as a sample from a Gaussian process (GP). The tractable posterior distribution induced by the GP leads to efficient use of the information gathered by previous experiments, enabling optimal choices about what parameters to try next. Here we show how the effects of the Gaussian process prior and the associated inference procedure can have a large impact on the success or failure of Bayesian optimization. We show that thoughtful choices can lead to results that exceed expert-level performance in tuning machine learning algorithms. We also describe new algorithms that take into account the variable cost (duration) of learning experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization on a diverse set of contemporary algorithms including latent Dirichlet allocation, structured SVMs and convolutional neural networks.},
archivePrefix = {arXiv},
arxivId = {arXiv:1206.2944v2},
author = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P. Rp},
doi = {2012arXiv1206.2944S},
eprint = {arXiv:1206.2944v2},
file = {:home/hale/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Snoek, Larochelle, Adams - 2012 - Practical Bayesian optimization of machine learning algorithms.pdf:pdf},
isbn = {9781627480031},
issn = {10495258},
journal = {arXiv preprint arXiv:1206.2944},
pages = {1--9},
pmid = {9377276},
title = {{Practical Bayesian optimization of machine learning algorithms}},
url = {http://arxiv.org/abs/1206.2944},
year = {2012}
}
@inproceedings{Meshry2019,
author = {Meshry, M and Goldman, D B and Khamis, S and Hoppe, H and Pandey, R and Snavely, N and Martin-Brualla, R},
booktitle = {cvpr},
pages = {6871--6880},
title = {{Neural Rerendering in the Wild}},
year = {2019}
}
@inproceedings{bailer2017,
author = {Bailer, Christian and Varanasi, Kiran and Stricker, Didier},
booktitle = {cvpr},
number = {3},
pages = {7},
title = {{CNN-based patch matching for optical flow with thresholded hinge embedding loss}},
volume = {2},
year = {2017}
}
@article{Handa,
abstract = {Scene understanding is a prerequisite to many high level tasks for any automated intelligent machine operating in real world environments. Recent attempts with supervised learning have shown promise in this direction but also highlighted the need for enormous quantity of supervised data --- performance increases in proportion to the amount of data used. However, this quickly becomes prohibitive when considering the manual labour needed to collect such data. In this work, we focus our attention on depth based semantic per-pixel labelling as a scene understanding problem and show the potential of computer graphics to generate virtually unlimited labelled data from synthetic 3D scenes. By carefully synthesizing training data with appropriate noise models we show comparable performance to state-of-the-art RGBD systems on NYUv2 dataset despite using only depth data as input and set a benchmark on depth-based segmentation on SUN RGB-D dataset. Additionally, we offer a route to generating synthesized frame or video data, and understanding of different factors influencing performance gains.},
archivePrefix = {arXiv},
arxivId = {1511.07041},
author = {Handa, Ankur and Patraucean, Viorica and Badrinarayanan, Vijay and Stent, Simon and Cipolla, Roberto},
eprint = {1511.07041},
file = {:home/hale/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Handa et al. - Unknown - SceneNet Understanding Real World Indoor Scenes With Synthetic Data.pdf:pdf},
keywords = {scene,segmentation,understanding},
mendeley-tags = {scene,segmentation,understanding},
title = {{SceneNet: Understanding Real World Indoor Scenes With Synthetic Data}},
url = {http://arxiv.org/abs/1511.07041},
year = {2015}
}
@inproceedings{Zhu2018_tracking,
author = {Zhu, Zheng and Wu, Wei and Zou, Wei and Yan, Junjie},
booktitle = {cvpr},
month = {jun},
title = {{End-to-End Flow Correlation Tracking With Spatial-Temporal Attention}},
year = {2018}
}
@inproceedings{Eigen2015,
abstract = {In this paper we address three different computer vision tasks using a single multiscale convolutional network archi-tecture: depth prediction, surface normal estimation, and semantic labeling. The network that we develop is able to adapt naturally to each task using only small modifica-tions, regressing from the input image to the output map di-rectly. Our method progressively refines predictions using a sequence of scales, and captures many image details with-out any superpixels or low-level segmentation. We achieve state-of-the-art performance on benchmarks for all three tasks.},
author = {Eigen, David and Fergus, Rob},
booktitle = {iccv},
keywords = {CNN,depth,semantic,surface normal},
mendeley-tags = {CNN,depth,semantic,surface normal},
pages = {2650--2658},
title = {{Predicting Depth, Surface Normals and Semantic Labels with a Common Multi-Scale Convolutional Architecture}},
url = {https://arxiv.org/pdf/1411.4734v4.pdf},
year = {2015}
}
@article{ImgNet,
address = {Hingham, MA, USA},
author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C and Fei-Fei, Li},
doi = {10.1007/s11263-015-0816-y},
issn = {0920-5691},
journal = {ijcv},
keywords = {Benchmark,Dataset,Large-scale,Object detection,Object recognition},
month = {dec},
number = {3},
pages = {211--252},
publisher = {Kluwer Academic Publishers},
title = {{ImageNet Large Scale Visual Recognition Challenge}},
url = {http://dx.doi.org/10.1007/s11263-015-0816-y},
volume = {115},
year = {2015}
}
@inproceedings{Sun2018,
abstract = {We present a compact but effective CNN model for optical flow, called PWC-Net. PWC-Net has been designed according to simple and well-established principles: pyra-midal processing, warping, and the use of a cost volume. Cast in a learnable feature pyramid, PWC-Net uses the current optical flow estimate to warp the CNN features of the second image. It then uses the warped features and features of the first image to construct a cost volume, which is processed by a CNN to estimate the optical flow. PWC-Net is 17 times smaller in size and easier to train than the recent FlowNet2 model. Moreover, it outperforms all published optical flow methods on the MPI Sintel final pass and KITTI 2015 benchmarks, running at about 35 fps on Sintel resolution (1024Ã—436) images. Our models are available on our project website.},
author = {Sun, Deqing and Yang, Xiaodong and Liu, Ming-Yu and Kautz, Jan and Nvidia, Jan Kautz},
booktitle = {cvpr},
file = {:home/hale/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sun et al. - Unknown - PWC-Net CNNs for Optical Flow Using Pyramid, Warping, and Cost Volume.pdf:pdf},
keywords = {deep learning,optical flow},
mendeley-tags = {deep learning,optical flow},
title = {{PWC-Net: CNNs for Optical Flow Using Pyramid, Warping, and Cost Volume}},
url = {http://openaccess.thecvf.com/content{\_}cvpr{\_}2018/papers/Sun{\_}PWC-Net{\_}CNNs{\_}for{\_}CVPR{\_}2018{\_}paper.pdf},
year = {2018}
}
@inproceedings{densenet,
author = {Huang, G and Liu, Z and van der Maaten, L and Weinberger, K {\~{}}Q.},
booktitle = {cvpr},
title = {{Densely Connected Convolutional Networks}},
year = {2017}
}
@inproceedings{Wu2015,
abstract = {3D shape is a crucial but heavily underutilized cue in today's computer vision systems, mostly due to the lack of a good generic shape representation. With the recent availability of inexpensive 2.5D depth sensors (e.g. Microsoft Kinect), it is becoming increasingly important to have a powerful 3D shape representation in the loop. Apart from category recognition, recovering full 3D shapes from viewbased 2.5D depth maps is also a critical part of visual understanding. To this end, we propose to represent a geometric 3D shape as a probability distribution of binary variables on a 3D voxel grid, using a Convolutional Deep Belief Network. Our model, 3D ShapeNets, learns the distribution of complex 3D shapes across different object categories and arbitrary poses from raw CAD data, and discovers hierarchical compositional part representations automatically. It naturally supports joint object recognition and shape completion from 2.5D depth maps, and it enables active object recognition through view planning. To train our 3D deep learning model, we construct ModelNet â€“ a large-scale 3D CAD model dataset. Extensive experiments show that our 3D deep representation enables significant performance improvement over the-state-of-the-arts in a variety of tasks.},
archivePrefix = {arXiv},
arxivId = {arXiv:1406.5670v3},
author = {Wu, Zhirong and Song, Shuran and Khosla, Aditya and Yu, Fisher and Zhang, Linguang and Tang, Xiaoou and Xiao, Jianxiong},
booktitle = {cvpr},
doi = {10.1109/CVPR.2015.7298801},
eprint = {arXiv:1406.5670v3},
file = {:home/hale/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wu et al. - 2015 - 3D ShapeNets A Deep Representation for Volumetric Shapes.pdf:pdf},
isbn = {9781467369640},
keywords = {2.5D,3d,3dcnn,object,recogntion},
mendeley-tags = {2.5D,3d,3dcnn,object,recogntion},
pages = {1--9},
title = {{3D ShapeNets : A Deep Representation for Volumetric Shapes}},
year = {2015}
}
@article{Land1971,
author = {Land, E. H. and McCann, J. J.},
journal = {Journal of Optical Society of America},
pages = {1--11},
title = {{Lightness and retinex theory}},
year = {1971}
}
@inproceedings{Chen,
abstract = {Deep Convolutional Neural Networks (DCNNs) have recently shown state of the art performance in high level vision tasks, such as image classification and ob-ject detection. This work brings together methods from DCNNs and probabilistic graphical models for addressing the task of pixel-level classification (also called " semantic image segmentation "). We show that responses at the final layer of DCNNs are not sufficiently localized for accurate object segmentation. This is due to the very invariance properties that make DCNNs good for high level tasks. We overcome this poor localization property of deep networks by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF). Qualitatively, our " DeepLab " system is able to localize segment boundaries at a level of accuracy which is beyond previous methods. Quantita-tively, our method sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 71.6{\%} IOU accuracy in the test set. We show how these results can be obtained efficiently: Careful network re-purposing and a novel application of the 'hole' algorithm from the wavelet community allow dense computation of neural net responses at 8 frames per second on a modern GPU.},
archivePrefix = {arXiv},
arxivId = {1412.7062},
author = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L},
booktitle = {iclr},
eprint = {1412.7062},
file = {:home/hale/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen et al. - Unknown - SEMANTIC IMAGE SEGMENTATION WITH DEEP CON- VOLUTIONAL NETS AND FULLY CONNECTED CRFS.pdf:pdf},
isbn = {9783901608353},
keywords = {cnn,crf,semantic segmentation},
mendeley-tags = {cnn,crf,semantic segmentation},
title = {{Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs}},
year = {2015}
}
@incollection{Kar2017,
author = {Kar, Abhishek and H{\"{a}}ne, Christian and Malik, Jitendra},
title = {{Learning a Multi-View Stereo Machine}},
year = {2017}
}
@article{Olszewski2019,
author = {Olszewski, Kyle and Tulyakov, Sergey and Woodford, Oliver and Li, Hao and Luo, Linjie},
journal = {iccv},
month = {nov},
title = {{Transformable Bottleneck Networks}},
year = {2019}
}
@inproceedings{Gupta2010,
address = {Berlin, Heidelberg},
author = {Gupta, Abhinav and Efros, Alexei A and Hebert, Martial},
booktitle = {eccv},
isbn = {3-642-15560-X, 978-3-642-15560-4},
pages = {482--496},
publisher = {Springer-Verlag},
series = {ECCV'10},
title = {{Blocks World Revisited: Image Understanding Using Qualitative Geometry and Mechanics}},
url = {http://dl.acm.org/citation.cfm?id=1888089.1888126},
year = {2010}
}
@inproceedings{Marin2010,
author = {Mar{\'{i}}n, J and V{\'{a}}zquez, D and Ger{\'{o}}nimo, D and L{\'{o}}pez, A M},
booktitle = {cvpr},
pages = {137--144},
title = {{Learning appearance in virtual scenarios for pedestrian detection}},
year = {2010}
}
@inproceedings{Hattori2015,
author = {Hattori, H and Boddeti, V N and Kitani, K and Kanade, T},
booktitle = {cvpr},
pages = {3819--3827},
title = {{Learning scene-specific pedestrian detectors without real data}},
year = {2015}
}
@article{Chen2016Attention,
abstract = {Incorporating multi-scale features to deep convolutional neural networks (DCNNs) has been a key element to achieve state-of-art performance on semantic image segmentation benchmarks. One way to extract multi-scale features is by feeding several resized input images to a shared deep network and then merge the resulting multi-scale features for pixel-wise classification. In this work, we adapt a state-of-art semantic image segmentation model with multi-scale input images. We jointly train the network and an attention model which learns to softly weight the multi-scale features, and show that it outperforms average- or max-pooling over scales. The proposed attention model allows us to diagnostically visualize the importance of features at different positions and scales. Moreover, we show that adding extra supervision to the output of DCNN for each scale is essential to achieve excellent performance when merging multi-scale features. We demonstrate the effectiveness of our model with exhaustive experiments on three challenging datasets, including PASCAL-Person-Part, PASCAL VOC 2012 and a subset of MS-COCO 2014.},
archivePrefix = {arXiv},
arxivId = {1511.03339},
author = {Chen, Liang-Chieh and Yang, Yi and Wang, Jiang and Xu, Wei and Yuille, Alan L},
doi = {10.1109/CVPR.2016.396},
eprint = {1511.03339},
file = {:home/hale/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen et al. - 2016 - Attention to Scale Scale-aware Semantic Image Segmentation.pdf:pdf},
isbn = {9781467388511},
issn = {10636919},
journal = {cvpr},
pages = {13},
title = {{Attention to Scale: Scale-aware Semantic Image Segmentation}},
url = {http://arxiv.org/abs/1511.03339},
year = {2016}
}
